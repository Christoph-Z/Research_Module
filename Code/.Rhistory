beta.vec <- c(beta.pos[1:p.True],rep(0,p-p.True))
X<- X.pos[,1:p]
##Genereta dependent variables acording to the Above statet Model
y <- X%*%beta.vec + eps    #True Model
return(cbind(y,X))
}
#Spits out a vector of Integers, i.e, alpha
CV <- function(n_v, y, X, Alpha = NULL, MonteCarlo = NULL, Replacement = FALSE ){  #n_v = #leaved out data points
#n_v          Number of leaved out data points
#y,X          Data for Regression
#Alpha        Set of possible Modelvaritaions for which the CV should be calculated
#             (By Default use all possible Models)
#MonteCarlo   Number of Subsets of {1,...,n} which is randomly drawn for a Monte Carlo CV
#             (By Default do K-Fold CV)
#Replacement  Replacement for Monte Carlo (Default False)
##Change some Variable names to keep the code shorter
A <- Alpha
b <- MonteCarlo
##Number of Possible Regressors
p <- length(X[1,])
##Set of possible Models
if(is.null(A)){
#Creats the set {1,...,p} from which we want to generate the Powerset
Index <- seq(1,p,1)
#Denotes the number of Possible Models out of {1,...,p}
col.A <- 2^p-1
#Denote A as Powerset of {1,...,p}
A <- matrix(0L,nrow = p, ncol = col.A)
k <- choose(p,1)
l <- 1
for (i in 1:p) {
#combn spits out all combinations of i elements in Index
A[1:i,l:k] <- combn(Index,i)
k <-k + choose(p,i+1)
l <- l + choose(p,i)
}
}
##Number of Observations
n <- length(y)
##Combinations of Sample partions for fitting the model
##For the Mone Carlo CV with b subsets of {1,...,n}
if(!is.null(b)){
train <- matrix(ncol = b, nrow = n-n_v)
for (i in 1:b) {
train[,i] <- sample(seq(1,n,1),n-n_v,replace = Replacement)
}
}else{
##For the general case with all subsets of {1,...,n}
train <- combn(seq(1,n,1),n-n_v)
}
#Compute Prediction Errors for all sets in A
MeanPred.Error <- c()
for (i in 1:length(A[1,])) {
Pred.Error <- c()
for (j in 1:length(train[1,])) {
#To make the Code a bit shorter
train.j <- train[,j]
X.train <- X[train.j,A[,i]]
#Prediction Error for a given alpha and a given subset
Pred.Error[j] <- norm(as.matrix(y[-train.j]-X[-train.j,A[,i]]%*%solve(t(X.train)%*%X.train)%*%t(X.train)%*%y[train.j]),"2")^2
}
MeanPred.Error[i] <- 1/length(train[1,])*sum(Pred.Error)
}
TheChosenOne <- which.min(MeanPred.Error)
return(A[,TheChosenOne])
}
##Vector of different Sample sizes
N <- seq(15,25,2)
##Number of True regressors
##We suppose this number to be fixed. We do this for simplicity since we have limited computational power
p.True<- 2
##Vector of different Modeldimensons
P <- seq(p.True+1,5)
##Number of Iteration of Modelfitting
m <- 2
##Criterion Function
##Is minimal iff CV chooses the true model and prefers less conservative Choices
L <- function(model){
+ sum( model > p.True) -sum( (model <= p.True & model > 0) )
}
##Matrix of optimal n_v given n and p
M <- matrix(0L,ncol = length(N), nrow = length(P))
View(M)
##Simulation
for (i in 1:length(N)) {
n <- N[i]
##Define the number b of Samplepartions as function of n
##Shao 93 claimed that b=O(n)
b <- n*50
for (j in 1:length(P)) {
p <- P[j]
##Grid of possible n_v values for a given Samplezize n in N
N_v <- seq(2,n-p,2)
##Vector of all Criterionfunction values fiven n_v
N_v.CritValue <- rep(0,length(N_v))
for (k in 1:length(N_v)) {
n_v <- N_v[k]
##For statistical Influence fit the Model m times
for (l in 1:m) {
##Generate Data
Data <- DataGen(n,p,p.True)
y <- Data[,1]
X <- Data[,-1]
##Select a Model
model <- CV(n_v,y,X, MonteCarlo = b)
N_v.CritValue[k] <-  N_v.CritValue[k] + L(model)
}
}
TheChoosenOne <- which.min(N_v.CritValue)
M[j,i] <- N_v[TheChoosenOne]
}
}
M
ggplot(d1,aes(x=Samplesize,y=value,colour=key)) +
geom_line(size=1.2) +
coord_cartesian(ylim = c(0.6,1), xlim = c(13, 113)) +
ylab("Prob of choosing a Cat. II Model") + ggtitle("Probability of Cat. II Model") +
theme_bw() +
theme(text = element_text(size=15),
plot.title = element_text(size = 20,hjust = 0.5,face="bold")) +
theme(axis.title.x = element_text(size = 15)) +
theme(axis.title.y = element_text(size = 15)) +
theme(axis.title.y = element_text(margin = margin(t = 0, r = 20, b = 0, l = 0))) +
theme(axis.title.x = element_text(margin = margin(t = 20, r = 0, b = 0, l = 0))) +
theme(plot.title = element_text(margin = margin(t = 0, r = 0 , b = 20, l = 0))) +
theme(legend.title=element_blank(),legend.text = element_text(size = 13)) +
theme(legend.position = c(0.1,0.8))
library(dplyr)
library(tidyr)
MCV <- MCV[1:50]
AIC <- AIC[1:50]
CV1 <- CV1[1:50]
BIC <- BIC[1:50]
N <- N[1:50]
load("/Users/christophzilligen/Documents/Studium/Master/3. Semester/Research_Module/Code/Daten Simulationen.RData")
library(dplyr)
library(tidyr)
MCV <- MCV[1:50]
AIC <- AIC[1:50]
CV1 <- CV1[1:50]
BIC <- BIC[1:50]
N <- N[1:50]
d <- data.frame("Samplesize"=N,"MCCV"=MCV,"AIC"=AIC,"CV1"=CV1,"BIC"=BIC)
d1 <-d %>% gather(key,value,MCCV,AIC,BIC,CV1)
ggplot(d1,aes(x=Samplesize,y=value,colour=key)) +
geom_line(size=1.2) +
coord_cartesian(ylim = c(0.6,1), xlim = c(13, 113)) +
ylab("Prob of choosing a Cat. II Model") + ggtitle("Probability of Cat. II Model") +
theme_bw() +
theme(text = element_text(size=15),
plot.title = element_text(size = 20,hjust = 0.5,face="bold")) +
theme(axis.title.x = element_text(size = 15)) +
theme(axis.title.y = element_text(size = 15)) +
theme(axis.title.y = element_text(margin = margin(t = 0, r = 20, b = 0, l = 0))) +
theme(axis.title.x = element_text(margin = margin(t = 20, r = 0, b = 0, l = 0))) +
theme(plot.title = element_text(margin = margin(t = 0, r = 0 , b = 20, l = 0))) +
theme(legend.title=element_blank(),legend.text = element_text(size = 13)) +
theme(legend.position = c(0.1,0.8))
load("/Users/christophzilligen/Documents/Studium/Master/3. Semester/Research_Module/Code/Daten Simulationen.RData")
library(dplyr)
library(tidyr)
MCV <- MCV[1:50]
AIC <- AIC[1:50]
CV1 <- CV1[1:50]
BIC <- BIC[1:50]
N <- N[1:50]
d <- data.frame("Samplesize"=N,"MCCV"=MCV,"AIC"=AIC,"CV1"=CV1,"BIC"=BIC)
d1 <-d %>% gather(key,value,MCCV,AIC,BIC,CV1)
ggplot(d1,aes(x=Samplesize,y=value,colour=key)) +
geom_line(size=1.2) +
coord_cartesian(ylim = c(0.6,1), xlim = c(13, 113)) +
ylab("Prob of choosing a Cat. II Model") + ggtitle("Probability of Cat. II Model") +
theme_bw() +
theme(text = element_text(size=15),
plot.title = element_text(size = 20,hjust = 0.5,face="bold")) +
theme(axis.title.x = element_text(size = 15)) +
theme(axis.title.y = element_text(size = 15)) +
theme(axis.title.y = element_text(margin = margin(t = 0, r = 20, b = 0, l = 0))) +
theme(axis.title.x = element_text(margin = margin(t = 20, r = 0, b = 0, l = 0))) +
theme(plot.title = element_text(margin = margin(t = 0, r = 0 , b = 20, l = 0))) +
theme(legend.title=element_blank(),legend.text = element_text(size = 13)) +
theme(legend.position = c(0.1,0.8))
ggplot(d1,aes(x=Samplesize,y=value,colour=key)) +
geom_line(size=1.2) +
coord_cartesian(ylim = c(0.6,1), xlim = c(13, 113)) +
ylab("Prob of choosing a Cat. II Model") + ggtitle("Probability of Cat. II Model") +
theme_bw() +
theme(text = element_text(size=15),
plot.title = element_text(size = 20,hjust = 0.5,face="bold")) +
theme(axis.title.x = element_text(size = 15)) +
theme(axis.title.y = element_text(size = 15)) +
theme(axis.title.y = element_text(margin = margin(t = 0, r = 20, b = 0, l = 0))) +
theme(axis.title.x = element_text(margin = margin(t = 20, r = 0, b = 0, l = 0))) +
theme(plot.title = element_text(margin = margin(t = 0, r = 0 , b = 20, l = 0))) +
theme(legend.title=element_blank(),legend.text = element_text(size = 13)) +
theme(legend.position = c(0.1,0.8))
library(ggplot2)
ggplot(d1,aes(x=Samplesize,y=value,colour=key)) +
geom_line(size=1.2) +
coord_cartesian(ylim = c(0.6,1), xlim = c(13, 113)) +
ylab("Prob of choosing a Cat. II Model") + ggtitle("Probability of Cat. II Model") +
theme_bw() +
theme(text = element_text(size=15),
plot.title = element_text(size = 20,hjust = 0.5,face="bold")) +
theme(axis.title.x = element_text(size = 15)) +
theme(axis.title.y = element_text(size = 15)) +
theme(axis.title.y = element_text(margin = margin(t = 0, r = 20, b = 0, l = 0))) +
theme(axis.title.x = element_text(margin = margin(t = 20, r = 0, b = 0, l = 0))) +
theme(plot.title = element_text(margin = margin(t = 0, r = 0 , b = 20, l = 0))) +
theme(legend.title=element_blank(),legend.text = element_text(size = 13)) +
theme(legend.position = c(0.1,0.8))
##Sample size
n <- 5000
##Number of regressors
p <- 5
##Number of regressors unequal to zero
p.True <-2
##True Beta Vector
beta <- c(1.5,3,0,0,0)
##Generate data for X
x_1 <- rep(1,n)
x_2 <- rnorm(n,2,1)
x_3 <- rnorm(n,0,1)
x_4 <- rnorm(n,5,2)
x_5 <- rnorm(n,4,1)
X.all <- cbind(x_1,x_2,x_3,x_4,x_5)
y.mean <- X.all%*%beta
y <- X.all%*%beta+rnorm(n,0,1)
Models <- function(p.True,p){
##For the CV we need to compute the set A of all possible models
#Creats the set {1,...,p} from which we want to generate the Powerset
Index <- seq(1,p,1)
#Denotes the number of Possible Models out of {1,...,p} Regressors
col.A <- 2^p-1
#Denote A as Powerset of {1,...,p}
A <- matrix(0L,nrow = p, ncol = col.A)
k <- choose(p,1)
l <- 1
for (i in 1:p) {
#combn spits out all combinations of i elements in Index
A[1:i,l:k] <- combn(Index,i)
k <-k + choose(p,i+1)
l <- l + choose(p,i)
}
return(A)
}
A <- Models(p.True,5)
##AIC/BIC
InfoCrit <- function(y,X,I,A,Criterion = "AIC"){
##Calculate Information Criterion
##For AIC
if(Criterion == "AIC"){
j <- 0
InfoCriterion <- c()
for (i in I) {
j <- j+1
##Number of regressors
k <- sum(A[,j] != 0)
##Expectation observation j
mu <- i%*%y
##Varianz
var <- 1/n*sum((y-mu)^2)
##Calculate Log Likelihood value
LogL <- -n/2*(log(2*pi)+log(var)+1)
InfoCriterion[j] <- 2*k-2*LogL
}
}
##For BIC
if(Criterion == "BIC"){
j <- 0
InfoCriterion <- c()
for (i in I) {
j <- j+1
##Number of regressors
k <- sum(A[,j] != 0)
##Expectation observation j
mu <- i%*%y
##Varianz
var <- 1/n*sum((y-mu)^2)
##Calculate Log Likelihood value
LogL <- -n/2*(log(2*pi)+log(var)+1)
InfoCriterion[j] <- log(n)*k - 2*LogL
}
}
#Choose the Model which minimze the Information Criterion
TheChosenOne <- which.min(InfoCriterion)
return(A[,TheChosenOne])
}
CV1 <- function(n,y,X,I,A,train){
#Compute Prediction Errors for all sets in A
MeanPred.Error <- c()
k <- 0
for (i in 1:length(A[1,])) {
Pred.Error <- c()
for (j in 1:n) {
k <- k+1
#To make the Code a bit shorter
train.j <- train[,j]
#Prediction Error for a given alpha and a given subset
for(v in I[k]){
Error.temp <- y[-train.j]-v%*%y[train.j]
Pred.Error[j] <- t(Error.temp)%*%Error.temp
}
}
MeanPred.Error[i] <- 1/length(n)*sum(Pred.Error)
}
TheChosenOne <- which.min(MeanPred.Error)
return(A[,TheChosenOne])
}
MCCV.v2 <- function(n,n_v, y, X,I, A, b,train, Replacement = FALSE){  #n_v = #leaved out data points
#Compute Prediction Errors for all sets in A
MeanPred.Error <- c()
k <- 0
for (i in 1:length(A[1,])) {
Pred.Error <- c()
for (j in 1:length(train[1,])) {
k <- k+1
#To make the Code a bit shorter
train.j <- train[,j]
#Prediction Error for a given alpha and a given subset
for(v in I[k]){
Error.temp <- y[-train.j]-v%*%y[train.j]
Pred.Error[j] <- t(Error.temp)%*%Error.temp
}
}
MeanPred.Error[i] <- 1/length(n_v)*sum(Pred.Error)
}
TheChosenOne <- which.min(MeanPred.Error)
return(A[,TheChosenOne])
}
##Vector of diffrent Sample sizes
N <- seq(15,300,2)
##Number of Repetitions of the Experiment
m <- 2000
##Set of posssible models
A <- Models(2,5)
##Vector of probabilties of frequencys of choosing a Cat II Model for each samplezize by m repetations
BIC <- rep(0,length(N))
AIC <- rep(0,length(N))
CV <- rep(0,length(N))
MCV <- rep(0,length(N))
for (q in 1:length(N)) {
n <- N[q]
n_v <- floor(n-n^(3/4))
##Number of Samplepartions
b <- 5*n
X <- X.all[1:n,]
##Pmatrix AIC/BIC
Pmatrix_INF <- list()
for (i in 1:length(A[1,])) {
X.train <- X[,A[,i]]
X.train.transpos <- t(X.train)
Pmatrix_INF[[paste0("element",i)]] <- X.train%*%(solve(X.train.transpos%*%X.train)%*%X.train.transpos)
}
##Pmatrix CV1
train <- combn(seq(1,n,1),n-1)
Pmatrix_CV <- list()
k <- 0
for (i in 1:length(A[1,])) {
for (j in 1:length(train[1,])) {
k <- k+1
train.j <- train[,j]
X.train <- X[train.j,A[,i]]
Pmatrix_CV[[paste0("element",k)]] <- X[-train.j,A[,i]]%*%solve(t(X.train)%*%X.train)%*%t(X.train)
}
}
##Combination of Samplesplits
train.MCCV <- matrix(ncol = b, nrow = n-n_v)
for (i in 1:b) {
train.MCCV[,i] <- sample(seq(1,n,1),n-n_v,replace = FALSE)
}
##Pmatrix
Pmatrix_MCCV <- list()
k <- 0
for (i in 1:length(A[1,])) {
A.i <- A[,i]
for (j in 1:length(train.MCCV[1,])) {
k <- k+1
train.j <- train.MCCV[,j]
X.train <- X[train.j,A.i]
Pmatrix_MCCV[[paste0("element",k)]] <- X[-train.j,A.i]%*%(solve(t(X.train)%*%X.train)%*%t(X.train))
}
}
for (j in 1:m) {
##Generating Data
y <- y.mean[1:n]+rnorm(n,0,1)
#Define some temporary Variables, i.e, define for a given Method the in iteration i choosed Model
temp.BIC <- InfoCrit(y,X,Pmatrix_INF,A,Criterion = "BIC")
temp.AIC <- InfoCrit(y,X,Pmatrix_INF,A)
temp.CV1 <- CV1(n,y,X,Pmatrix_CV,A,train)
temp.MCV <- MCCV.v2(n,n_v,y,X,Pmatrix_MCCV,A,b,train.MCCV)
##Counting how many times a Cat II Model is picked for n= N[i] in m interations
BIC[q] <- BIC[q] + (sum( temp.BIC < (p.True+1) & temp.BIC >0) == p.True)
AIC[q] <- AIC[q] + (sum( temp.AIC < (p.True+1) & temp.AIC > 0) == p.True)
CV[q] <- CV[q] + (sum( temp.CV1 < (p.True+1) & temp.CV1 > 0) == p.True)
MCV[q] <- MCV[q] + (sum( temp.MCV < (p.True+1) & temp.MCV > 0) == p.True)
}
}
n <- 500
p <- 5
p.True <- 2
beta <- c(4,7,0,0,0)
DataGen_X <- function(n){
x_1 <- rep(1,n)
x_2 <- rnorm(n,0,1)
x_3 <- rnorm(n,0,1)
x_4 <- rnorm(n,0,1)
x_5 <- rnorm(n,4,1)
return(X<- cbind(x_1,x_2,x_3,x_4,x_5))
}
X <- DataGen_X(n)
sigma <- 1
y <- X%*%beta+rnorm(n,0,sigma)
##--------------------------------------------------------------------
##ESPE for CV(1) and CV(n_v) for Models in Category II
ESPE_CV1 <- function(d,n){
return((1+d/n)*sigma^2)
}
ESP_CVn.v <- function(d,n){
return(1+d/n^(3/4))
}
N.grid <-seq(20,100,1)
CV <- function(n_v, y, X, Alpha = NULL, MonteCarlo = NULL, Replacement = FALSE, BICV = NULL ){  #n_v = #leaved out data points
#n_v          Number of leaved out data points
#y,X          Data for Regression
#Alpha        Set of possible Modelvaritaions for which the CV should be calculated
#             (By Default use all possible Models)
#MonteCarlo   Number of Subsets of {1,...,n} which is randomly drawn for a Monte Carlo CV
#             (By Default do K-Fold CV)
#Replacement  Replacement for Monte Carlo (Default False)
#
##BICV        Incidence Matrix for a BICV
##Change some Variable names to keep the code shorter
A <- Alpha
b <- MonteCarlo
##Number of Possible Regressors
p <- length(X[1,])
##Set of possible Models
if(is.null(A)){
#Creats the set {1,...,p} from which we want to generate the Powerset
Index <- seq(1,p,1)
#Denotes the number of Possible Models out of {1,...,p}
col.A <- 2^p-1
#Denote A as Powerset of {1,...,p}
A <- matrix(0L,nrow = p, ncol = col.A)
k <- choose(p,1)
l <- 1
for (i in 1:p) {
#combn spits out all combinations of i elements in Index
A[1:i,l:k] <- combn(Index,i)
k <-k + choose(p,i+1)
l <- l + choose(p,i)
}
}
##Number of Observations
n <- length(y)
##Combinations of Sample partions for fitting the model
##For the Mone Carlo CV with b subsets of {1,...,n}
if(!is.null(b)){
train <- matrix(ncol = b, nrow = n-n_v)
for (i in 1:b) {
train[,i] <- sample(seq(1,n,1),n-n_v,replace = Replacement)
}
}else if(!is.null(BICV)){
#Convert the Incedence matrix of BIBD in our Notation
train <- BICV * seq(1,n,1)
}else{
##For the general case with all subsets of {1,...,n}
train <- combn(seq(1,n,1),n-n_v)
}
#Compute Prediction Errors for all sets in A
MeanPred.Error <- c()
for (i in 1:length(A[1,])) {
Pred.Error <- c()
for (j in 1:length(train[1,])) {
#To make the Code a bit shorter
train.j <- train[,j]
X.train <- X[train.j,A[,i]]
#Prediction Error for a given alpha and a given subset
Pred.Error[j] <- norm(as.matrix(y[-train.j]-X[-train.j,A[,i]]%*%solve(t(X.train)%*%X.train)%*%t(X.train)%*%y[train.j]),"2")^2
}
MeanPred.Error[i] <- 1/length(train[1,])*sum(Pred.Error)
}
TheChosenOne <- which.min(MeanPred.Error)
return(A[,TheChosenOne])
}
MCCV1_d2 <- c()
for (i in 1:length(N.grid)) {
CV1_d2[i] <- CV(N.grid[i]-floor(N.grid[i]^{3/4}),y[1:N.grid[i]],X[1:N.grid[i],1:2], MonteCarlo = 5*N.grid[i])
}
MCCV1_d3 <- c()
for (i in 1:length(N.grid)) {
CV1_d3[i] <- CV(N.grid[i]-floor(N.grid[i]^{3/4}),y[1:N.grid[i]],X[1:N.grid[i],1:3],MonteCarlo = 5*N.grid[i])
}
d <- data.frame("Samplesize"=N.grid,"ESPE1_2"=ESPE_CVn.v(2,N.grid),"ESPE1_3"=ESPE_CVn.v(3,N.grid),"ASPE1_2"=MCCV1_d2,"ASPE1_3"=MCCV1_d3)
MCCV1_d2 <- c()
for (i in 1:length(N.grid)) {
MCCV1_d2[i] <- CV(N.grid[i]-floor(N.grid[i]^{3/4}),y[1:N.grid[i]],X[1:N.grid[i],1:2], MonteCarlo = 5*N.grid[i])
}
MCCV1_d3 <- c()
for (i in 1:length(N.grid)) {
MCCV1_d3[i] <- CV(N.grid[i]-floor(N.grid[i]^{3/4}),y[1:N.grid[i]],X[1:N.grid[i],1:3],MonteCarlo = 5*N.grid[i])
}
N.grid <-seq(1,50,1)
MCCV1_d2 <- c()
for (i in 1:length(N.grid)) {
MCCV1_d2[i] <- CV(N.grid[i]-floor(N.grid[i]^{3/4}),y[1:N.grid[i]],X[1:N.grid[i],1:2], MonteCarlo = 5*N.grid[i])
}
MCCV1_d3 <- c()
for (i in 1:length(N.grid)) {
MCCV1_d3[i] <- CV(N.grid[i]-floor(N.grid[i]^{3/4}),y[1:N.grid[i]],X[1:N.grid[i],1:3],MonteCarlo = 5*N.grid[i])
}
