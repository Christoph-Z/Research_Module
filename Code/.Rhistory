return(C1)
}
##Calculate the set of Category II Models
C2 <- Partion(p.True,5)
View(C2)
View(C2)
View(C2)
set.seed(100)
##Generate some data for regression
n <- 50
##Function that Generates Data
#The only purpose of this function is to save some lines of code. Hence that the function
#is not very general
DataGen <- function(n){
##Data for True Regression Model
#Intercept
x_1 <- rep(1,n)
x_2 <- rnorm(n,2,1)
#x_3 <- rnorm(n,0,1)
#x_4 <- rnorm(n,5,2)
eps <- rnorm(n,0,1)     #Errorterm
##Generate some unecessary extra data
x_5 <- rnorm(n,4,1)
x_6 <- rnorm(n,2,2)
x_7 <- rnorm(n,1,3)
#x_8 <- rnorm(n,3,7)
#x_9 <- rnorm(n,0,1)
#x_10 <- rnorm(n,5,5)
beta.vec <- c(1.5,3,rep(0,3))
X <- cbind(x_1,x_2,x_3,x_4,x_5,x_6,x_7,x_8,x_9,x_10)
##Genereta dependent variables acording to the Above statet Model
y <- X%*%beta.vec + eps    #True Model
return(cbind(y,X))
}
#Data
Data <- DataGen(n)
#Data
Data <- DataGen(n)
##Function that Generates Data
#The only purpose of this function is to save some lines of code. Hence that the function
#is not very general
DataGen <- function(n){
##Data for True Regression Model
#Intercept
x_1 <- rep(1,n)
x_2 <- rnorm(n,2,1)
#x_3 <- rnorm(n,0,1)
#x_4 <- rnorm(n,5,2)
eps <- rnorm(n,0,1)     #Errorterm
##Generate some unecessary extra data
x_5 <- rnorm(n,4,1)
x_6 <- rnorm(n,2,2)
x_7 <- rnorm(n,1,3)
#x_8 <- rnorm(n,3,7)
#x_9 <- rnorm(n,0,1)
#x_10 <- rnorm(n,5,5)
beta.vec <- c(1.5,3,rep(0,3))
X <- cbind(x_1,x_2,x_5,x_6,x_7)
##Genereta dependent variables acording to the Above statet Model
y <- X%*%beta.vec + eps    #True Model
return(cbind(y,X))
}
#Data
Data <- DataGen(n)
y <- Data[,1]
X <- Data[,-1]
#Number of True regressors
p.True<- 2
##Number of Regressors
p <- length(X[1,])
Partion <- function(p.True,p){
##For the CV we need to compute the set A of all possible models
#Creats the set {1,...,p} from which we want to generate the Powerset
Index <- seq(1,p,1)
#Denotes the number of Possible Models out of {1,...,p} Regressors
col.A <- 0
for (i in 1:p) {
col.A <- col.A + choose(p,i)
}
#Denote A as Powerset of {1,...,p}
A <- matrix(0L,nrow = p, ncol = col.A)
k <- choose(p,1)
l <- 1
for (i in 1:p) {
#combn spits out all combinations of i elements in Index
A[1:i,l:k] <- combn(Index,i)
k <-k + choose(p,i+1)
l <- l + choose(p,i)
}
##Split A into two dijoint subsets, ie, the set of Category I Models and Category II Models
##
##To Compute the Set of CI and CII we assume a certain strurcture on the Data. We need X to be s.t all
##"TRUE" regrossers are in the first column of the X Matrix, i.e, if we have P "true" regressors, they
##are given throght X[1],...,X[P]. And evrey X[P+] is trash...
##
##We may need this sets later n for Simulation study's
coln <- c()
for (i in p.True:col.A) {
if(all(seq(1,p.True,1) %in% A[,i])){
coln <- c(coln,i)
}
}
C2 <- A[,coln]           #Set of all Category II Models
C1 <-A[,-coln]           #Set of all Category I Models
return(C2)
}
CV <- function(n_v, y, X, Alpha = NULL, MonteCarlo = NULL, Replacement = FALSE ){  #n_v = #leaved out data points
#n_v          Number of leaved out data points
#y,X          Data for Regression
#Alpha        Set of possible Modelvaritaions for which the CV should be calculated
#             (By Default use all possible Models)
#MonteCarlo   Number of Subsets of {1,...,n} which is randomly drawn for a Monte Carlo CV
#             (By Default do K-Fold CV)
#Replacement  Replacement for Monte Carlo (Default False)
##Change some Variable names to keep the code shorter
A <- Alpha
b <- MonteCarlo
##Number of Possible Regressors
p <- length(X[1,])
##Set of possible Models
if(is.null(A)){
#Creats the set {1,...,p} from which we want to generate the Powerset
Index <- seq(1,p,1)
#Denotes the number of Possible Models out of {1,...,p}
col.A <- 0
for (i in 1:p) {
col.A <- col.A + choose(p,i)
}
#Denote A as Powerset of {1,...,p}
A <- matrix(0L,nrow = p, ncol = col.A)
k <- choose(p,1)
l <- 1
for (i in 1:p) {
#combn spits out all combinations of i elements in Index
A[1:i,l:k] <- combn(Index,i)
k <-k + choose(p,i+1)
l <- l + choose(p,i)
}
}
##Number of Observations
n <- length(y)
##Combinations of Sample partions for fitting the model
##For the Mone Carlo CV with b subsets of {1,...,n}
if(!is.null(b)){
train <- matrix(ncol = b, nrow = n-n_v)
for (i in 1:b) {
train[,i] <- sample(seq(1,n,1),n-n_v,replace = Replacement)
}
}else{
##For the general case with all subsets of {1,...,n}
train <- combn(seq(1,n,1),n-n_v)
}
#Compute Prediction Errors for all sets in A
MeanPred.Error <- c()
for (i in 1:length(A[1,])) {
Pred.Error <- c()
for (j in 1:length(train[1,])) {
#To make the Code a bit shorter
train.j <- train[,j]
X.train <- X[train.j,A[,i]]
#Prediction Error for a given alpha and a given subset
Pred.Error[j] <- norm(as.matrix(y[-train.j]-X[-train.j,A[,i]]%*%solve(t(X.train)%*%X.train)%*%t(X.train)%*%y[train.j]),"2")^2
}
MeanPred.Error[i] <- 1/length(train[1,])*sum(Pred.Error)
}
TheChosenOne <- which.min(MeanPred.Error)
return(A[,TheChosenOne])
}
CV(1,y,X,MonteCarlo = 50)
##Akaike and Schwarz Information Criterion
InfoCrit <- function(y,X,Alpha = NULL, Criterion = "AIC"){
#Alpha        Set of possible Modelvaritaions for which the CV should be calculated
#             (By Default use all possible Models)
#Criterion    Decides if we want to use the Akaike or Bayesian information criterion
##Change some Variable names to keep the code shorter
A <- Alpha
##Number of Observations
n <- length(y)
##Set of possible Models
if(is.null(A)){
##Number of Possible Regressors
p <- length(X[1,])
Index <- seq(1,p,1)    #Creats the set {1,...,p} from which we want to generate the Powerset
col.A <- 0                                  #Denotes the number of Possible Models out of {1,...,p}
for (i in 1:p) {
col.A <- col.A + choose(p,i)
}
A <- matrix(0L,nrow = p, ncol = col.A)      #Denote A as Powerset of {1,...,p}
k <- choose(p,1)
l <- 1
for (i in 1:p) {
A[1:i,l:k] <- combn(Index,i)              #combn spits out all combinations of i elements in Index
k <-k + choose(p,i+1)
l <- l + choose(p,i)
}
}
##Vector of Likelihood values for different Models
InfoCriterion <- c()
for (i in 1:length(A[1,])) {
##Number of regressors
k <- sum(A[,i] != 0)
##Data for the choosen Model
X.model <- X[,A[,i]]
##Expectation observation i
mu <- X.model%*%solve(t(X.model)%*%X.model)%*%t(X.model)%*%y
error <- sum((y-mu)^2)
##Varianz
var <- 1/n*error
##Calculate Log Likelihood value
LogL <- -n/2*log(2*pi)-n/2*log(var)-1/(2*var)*error
##Calculate Information Criterion
##For AIC
if(Criterion == "AIC"){
InfoCriterion[i] <- 2*k-2*LogL
}
##For BIC
if(Criterion == "BIC"){
InfoCriterion[i] <- log(n)*k - 2*LogL
}
}
#Choose the Model which minimze the Information Criterion
TheChosenOne <- which.min(InfoCriterion)
return(A[,TheChosenOne])
}
InfoCrit(y,X)
c(1,1,0,0) != 0
sum(c(1,1,0,0) != 0)
matrix(c(1,1,1,1),2,2)
##Function that Generates Data
#The only purpose of this function is to save some lines of code. Hence that the function
#is not very general
DataGen <- function(n,p,p.True){
##Data for True Regression Model
#Intercept
x_1 <- rep(1,n)
x_2 <- rnorm(n,2,1)
x_3 <- rnorm(n,0,1)
x_4 <- rnorm(n,5,2)
eps <- rnorm(n,0,1)     #Errorterm
##Generate some unecessary extra data
x_5 <- rnorm(n,4,1)
x_6 <- rnorm(n,2,2)
x_7 <- rnorm(n,1,3)
x_8 <- rnorm(n,3,7)
x_9 <- rnorm(n,0,1)
x_10 <- rnorm(n,5,5)
##Possible values for Beta and X
beta.pos <- c(1.5,3,2,5,3,7,4,6,5,7)
X.pos <- cbind(x_1,x_2,x_3,x_4,x_5,x_6,x_7,x_8,x_9,x_10)
beta.vec <- c(beta.pos[1:p.True],rep(0,p-p.True))
X<- X.pos[,1:p]
##Genereta dependent variables acording to the Above statet Model
y <- X%*%beta.vec + eps    #True Model
return(cbind(y,X))
}
#Number of True regressors
p.True<- 2
##Number of Regressors
p <- 5
#The Number of Times a Criterion Chooses the Optimal model is given by the number of all entries
#which are equal to p.True.
matrix(0L,2,p-p.True)
#The Number of Times a Criterion Chooses the Optimal model is given by the number of all entries
#which are equal to p.True.
matrix(0L,3,p-p.True)
p - p.True
0:2
set.seed(100)
##Function that Generates Data
#The only purpose of this function is to save some lines of code. Hence that the function
#is not very general
DataGen <- function(n,p,p.True){
##Data for True Regression Model
#Intercept
x_1 <- rep(1,n)
x_2 <- rnorm(n,2,1)
x_3 <- rnorm(n,0,1)
x_4 <- rnorm(n,5,2)
eps <- rnorm(n,0,1)     #Errorterm
##Generate some unecessary extra data
x_5 <- rnorm(n,4,1)
x_6 <- rnorm(n,2,2)
x_7 <- rnorm(n,1,3)
x_8 <- rnorm(n,3,7)
x_9 <- rnorm(n,0,1)
x_10 <- rnorm(n,5,5)
##Possible values for Beta and X
beta.pos <- c(1.5,3,2,5,3,7,4,6,5,7)
X.pos <- cbind(x_1,x_2,x_3,x_4,x_5,x_6,x_7,x_8,x_9,x_10)
beta.vec <- c(beta.pos[1:p.True],rep(0,p-p.True))
X<- X.pos[,1:p]
##Genereta dependent variables acording to the Above statet Model
y <- X%*%beta.vec + eps    #True Model
return(cbind(y,X))
}
Partion <- function(p.True,p){
##For the CV we need to compute the set A of all possible models
#Creats the set {1,...,p} from which we want to generate the Powerset
Index <- seq(1,p,1)
#Denotes the number of Possible Models out of {1,...,p} Regressors
col.A <- 0
for (i in 1:p) {
col.A <- col.A + choose(p,i)
}
#Denote A as Powerset of {1,...,p}
A <- matrix(0L,nrow = p, ncol = col.A)
k <- choose(p,1)
l <- 1
for (i in 1:p) {
#combn spits out all combinations of i elements in Index
A[1:i,l:k] <- combn(Index,i)
k <-k + choose(p,i+1)
l <- l + choose(p,i)
}
##Split A into two dijoint subsets, ie, the set of Category I Models and Category II Models
##
##To Compute the Set of CI and CII we assume a certain strurcture on the Data. We need X to be s.t all
##"TRUE" regrossers are in the first column of the X Matrix, i.e, if we have P "true" regressors, they
##are given throght X[1],...,X[P]. And evrey X[P+] is trash...
##
##We may need this sets later n for Simulation study's
coln <- c()
for (i in p.True:col.A) {
if(all(seq(1,p.True,1) %in% A[,i])){
coln <- c(coln,i)
}
}
C2 <- A[,coln]           #Set of all Category II Models
C1 <-A[,-coln]           #Set of all Category I Models
return(C2)
}
CV <- function(n_v, y, X, Alpha = NULL, MonteCarlo = NULL, Replacement = FALSE ){  #n_v = #leaved out data points
#n_v          Number of leaved out data points
#y,X          Data for Regression
#Alpha        Set of possible Modelvaritaions for which the CV should be calculated
#             (By Default use all possible Models)
#MonteCarlo   Number of Subsets of {1,...,n} which is randomly drawn for a Monte Carlo CV
#             (By Default do K-Fold CV)
#Replacement  Replacement for Monte Carlo (Default False)
##Change some Variable names to keep the code shorter
A <- Alpha
b <- MonteCarlo
##Number of Possible Regressors
p <- length(X[1,])
##Set of possible Models
if(is.null(A)){
#Creats the set {1,...,p} from which we want to generate the Powerset
Index <- seq(1,p,1)
#Denotes the number of Possible Models out of {1,...,p}
col.A <- 0
for (i in 1:p) {
col.A <- col.A + choose(p,i)
}
#Denote A as Powerset of {1,...,p}
A <- matrix(0L,nrow = p, ncol = col.A)
k <- choose(p,1)
l <- 1
for (i in 1:p) {
#combn spits out all combinations of i elements in Index
A[1:i,l:k] <- combn(Index,i)
k <-k + choose(p,i+1)
l <- l + choose(p,i)
}
}
##Number of Observations
n <- length(y)
##Combinations of Sample partions for fitting the model
##For the Mone Carlo CV with b subsets of {1,...,n}
if(!is.null(b)){
train <- matrix(ncol = b, nrow = n-n_v)
for (i in 1:b) {
train[,i] <- sample(seq(1,n,1),n-n_v,replace = Replacement)
}
}else{
##For the general case with all subsets of {1,...,n}
train <- combn(seq(1,n,1),n-n_v)
}
#Compute Prediction Errors for all sets in A
MeanPred.Error <- c()
for (i in 1:length(A[1,])) {
Pred.Error <- c()
for (j in 1:length(train[1,])) {
#To make the Code a bit shorter
train.j <- train[,j]
X.train <- X[train.j,A[,i]]
#Prediction Error for a given alpha and a given subset
Pred.Error[j] <- norm(as.matrix(y[-train.j]-X[-train.j,A[,i]]%*%solve(t(X.train)%*%X.train)%*%t(X.train)%*%y[train.j]),"2")^2
}
MeanPred.Error[i] <- 1/length(train[1,])*sum(Pred.Error)
}
TheChosenOne <- which.min(MeanPred.Error)
return(A[,TheChosenOne])
}
##Akaike and Schwarz Information Criterion
InfoCrit <- function(y,X,Alpha = NULL, Criterion = "AIC"){
#Alpha        Set of possible Modelvaritaions for which the CV should be calculated
#             (By Default use all possible Models)
#Criterion    Decides if we want to use the Akaike or Bayesian information criterion
##Change some Variable names to keep the code shorter
A <- Alpha
##Number of Observations
n <- length(y)
##Set of possible Models
if(is.null(A)){
##Number of Possible Regressors
p <- length(X[1,])
Index <- seq(1,p,1)    #Creats the set {1,...,p} from which we want to generate the Powerset
col.A <- 0                                  #Denotes the number of Possible Models out of {1,...,p}
for (i in 1:p) {
col.A <- col.A + choose(p,i)
}
A <- matrix(0L,nrow = p, ncol = col.A)      #Denote A as Powerset of {1,...,p}
k <- choose(p,1)
l <- 1
for (i in 1:p) {
A[1:i,l:k] <- combn(Index,i)              #combn spits out all combinations of i elements in Index
k <-k + choose(p,i+1)
l <- l + choose(p,i)
}
}
##Vector of Likelihood values for different Models
InfoCriterion <- c()
for (i in 1:length(A[1,])) {
##Number of regressors
k <- sum(A[,i] != 0)
##Data for the choosen Model
X.model <- X[,A[,i]]
##Expectation observation i
mu <- X.model%*%solve(t(X.model)%*%X.model)%*%t(X.model)%*%y
error <- sum((y-mu)^2)
##Varianz
var <- 1/n*error
##Calculate Log Likelihood value
LogL <- -n/2*log(2*pi)-n/2*log(var)-1/(2*var)*error
##Calculate Information Criterion
##For AIC
if(Criterion == "AIC"){
InfoCriterion[i] <- 2*k-2*LogL
}
##For BIC
if(Criterion == "BIC"){
InfoCriterion[i] <- log(n)*k - 2*LogL
}
}
#Choose the Model which minimze the Information Criterion
TheChosenOne <- which.min(InfoCriterion)
return(A[,TheChosenOne])
}
#Samplesize
n <- 500
#Number of Repetitions of the Experiment
m <- 1000
#Number of True regressors
p.True<- 2
##Number of Regressors
p <- 5
##Calculate the set of Category II Models
C2 <- Partion(p.True,5)
##Calculate the set of Category II Models
C2 <- Partion(p.True,p)
#Gives the Dimenson of the choosen Model for each iteration
BIC <- c()
AIC <- c()
CV1 <- c()
for (i in 1:m) {
#Generating Data
Data <- DataGen(n,p,p.True)
y <- Data[,1]
X <- Data[,-1]
BIC[i] <- sum( InfoCrit(y,X,C2,Criterion = "BIC") != 0)
AIC[i] <- sum( InfoCrit(y,X,C2,Criterion = "AIC") != 0)
CV1[i] <- sum( CV(1,y,X,C2) != 0)
}
##Computes the probability for a Criterion to Chooses a Cat II Model of a given size.
Probabilities <- matrix(0L,3,p-p.True)
for(i in 0:(p-p.True)){
Probabilities[,i+1] <- c(sum( BIC == (p.True + i)), sum( AIC == (p.True + i)),  sum( CV1 == (p.True + i)) )/m
}
##Computes the probability for a Criterion to Chooses a Cat II Model of a given size.
Probabilities <- matrix(0L,3,p-p.True+1)
for(i in 0:(p-p.True)){
Probabilities[,i+1] <- c(sum( BIC == (p.True + i)), sum( AIC == (p.True + i)),  sum( CV1 == (p.True + i)) )/m
}
return(Probabilities)
Probabilities
#Number of Repetitions of the Experiment
m <- 2000
#Number of True regressors
p.True<- 2
##Number of Regressors
p <- 5
##Calculate the set of Category II Models
C2 <- Partion(p.True,p)
#Gives the Dimenson of the choosen Model for each iteration
BIC <- c()
AIC <- c()
CV1 <- c()
for (i in 1:m) {
#Generating Data
Data <- DataGen(n,p,p.True)
y <- Data[,1]
X <- Data[,-1]
BIC[i] <- sum( InfoCrit(y,X,C2,Criterion = "BIC") != 0)
AIC[i] <- sum( InfoCrit(y,X,C2,Criterion = "AIC") != 0)
CV1[i] <- sum( CV(1,y,X,C2) != 0)
}
##Computes the probability for a Criterion to Chooses a Cat II Model of a given size.
Probabilities <- matrix(0L,3,p-p.True+1)
for(i in 0:(p-p.True)){
Probabilities[,i+1] <- c(sum( BIC == (p.True + i)), sum( AIC == (p.True + i)),  sum( CV1 == (p.True + i)) )/m
}
Probabilities
