var <- 1/n*error
##Calculate Log Likelihood value
LogL <- -n/2*log(2*pi)-n/2*log(var)-1/(2*var)*error
##Calculate Information Criterion
##For AIC
if(Criterion == "AIC"){
InfoCriterion[i] <- 2*k-2*LogL
}
##For BIC
if(Criterion == "BIC"){
InfoCriterion[i] <- log(n)*k - 2*LogL
}
}
#Choose the Model which minimze the Information Criterion
TheChosenOne <- which.min(InfoCriterion)
return(A[,TheChosenOne])
}
#Samplesize
n <- 500
#Number of Repetitions of the Experiment
m <- 1000
#Number of True regressors
p.True<- 2
##Number of Regressors
p <- 5
##Calculate the set of Category II Models
C2 <- Partion(p.True,5)
##Calculate the set of Category II Models
C2 <- Partion(p.True,p)
#Gives the Dimenson of the choosen Model for each iteration
BIC <- c()
AIC <- c()
CV1 <- c()
for (i in 1:m) {
#Generating Data
Data <- DataGen(n,p,p.True)
y <- Data[,1]
X <- Data[,-1]
BIC[i] <- sum( InfoCrit(y,X,C2,Criterion = "BIC") != 0)
AIC[i] <- sum( InfoCrit(y,X,C2,Criterion = "AIC") != 0)
CV1[i] <- sum( CV(1,y,X,C2) != 0)
}
##Computes the probability for a Criterion to Chooses a Cat II Model of a given size.
Probabilities <- matrix(0L,3,p-p.True)
for(i in 0:(p-p.True)){
Probabilities[,i+1] <- c(sum( BIC == (p.True + i)), sum( AIC == (p.True + i)),  sum( CV1 == (p.True + i)) )/m
}
##Computes the probability for a Criterion to Chooses a Cat II Model of a given size.
Probabilities <- matrix(0L,3,p-p.True+1)
for(i in 0:(p-p.True)){
Probabilities[,i+1] <- c(sum( BIC == (p.True + i)), sum( AIC == (p.True + i)),  sum( CV1 == (p.True + i)) )/m
}
return(Probabilities)
Probabilities
#Number of Repetitions of the Experiment
m <- 2000
#Number of True regressors
p.True<- 2
##Number of Regressors
p <- 5
##Calculate the set of Category II Models
C2 <- Partion(p.True,p)
#Gives the Dimenson of the choosen Model for each iteration
BIC <- c()
AIC <- c()
CV1 <- c()
for (i in 1:m) {
#Generating Data
Data <- DataGen(n,p,p.True)
y <- Data[,1]
X <- Data[,-1]
BIC[i] <- sum( InfoCrit(y,X,C2,Criterion = "BIC") != 0)
AIC[i] <- sum( InfoCrit(y,X,C2,Criterion = "AIC") != 0)
CV1[i] <- sum( CV(1,y,X,C2) != 0)
}
##Computes the probability for a Criterion to Chooses a Cat II Model of a given size.
Probabilities <- matrix(0L,3,p-p.True+1)
for(i in 0:(p-p.True)){
Probabilities[,i+1] <- c(sum( BIC == (p.True + i)), sum( AIC == (p.True + i)),  sum( CV1 == (p.True + i)) )/m
}
Probabilities
matrix(0L, ncol=5,nrow = 2)
N_v <- N_v(2)
##Grid of possible n_v values for a given Samplezize n in N
N_v <- function(n){
return(seq(2,n-2,2))
}
N_v <- N_v(2)
N_v(2)
N_v(4)
N_v(8)
N_v <- N_v(8)
View(CV)
matrix(0L,2,2)
View(CV)
set.seed(100)
##Function that Generates Data
#The only purpose of this function is to save some lines of code. Hence that the function
#is not very general
DataGen <- function(n,p,p.True){
##Data for True Regression Model
#Intercept
x_1 <- rep(1,n)
x_2 <- rnorm(n,2,1)
x_3 <- rnorm(n,0,1)
x_4 <- rnorm(n,5,2)
eps <- rnorm(n,0,1)     #Errorterm
##Generate some unecessary extra data
x_5 <- rnorm(n,4,1)
x_6 <- rnorm(n,2,2)
x_7 <- rnorm(n,1,3)
x_8 <- rnorm(n,3,7)
x_9 <- rnorm(n,0,1)
x_10 <- rnorm(n,5,5)
##Possible values for Beta and X
beta.pos <- c(1.5,3,2,5,3,7,4,6,5,7)
X.pos <- cbind(x_1,x_2,x_3,x_4,x_5,x_6,x_7,x_8,x_9,x_10)
beta.vec <- c(beta.pos[1:p.True],rep(0,p-p.True))
X<- X.pos[,1:p]
##Genereta dependent variables acording to the Above statet Model
y <- X%*%beta.vec + eps    #True Model
return(cbind(y,X))
}
#Spits out a vector of Integers, i.e, alpha
CV <- function(n_v, y, X, Alpha = NULL, MonteCarlo = NULL, Replacement = FALSE ){  #n_v = #leaved out data points
#n_v          Number of leaved out data points
#y,X          Data for Regression
#Alpha        Set of possible Modelvaritaions for which the CV should be calculated
#             (By Default use all possible Models)
#MonteCarlo   Number of Subsets of {1,...,n} which is randomly drawn for a Monte Carlo CV
#             (By Default do K-Fold CV)
#Replacement  Replacement for Monte Carlo (Default False)
##Change some Variable names to keep the code shorter
A <- Alpha
b <- MonteCarlo
##Number of Possible Regressors
p <- length(X[1,])
##Set of possible Models
if(is.null(A)){
#Creats the set {1,...,p} from which we want to generate the Powerset
Index <- seq(1,p,1)
#Denotes the number of Possible Models out of {1,...,p}
col.A <- 2^p-1
#Denote A as Powerset of {1,...,p}
A <- matrix(0L,nrow = p, ncol = col.A)
k <- choose(p,1)
l <- 1
for (i in 1:p) {
#combn spits out all combinations of i elements in Index
A[1:i,l:k] <- combn(Index,i)
k <-k + choose(p,i+1)
l <- l + choose(p,i)
}
}
##Number of Observations
n <- length(y)
##Combinations of Sample partions for fitting the model
##For the Mone Carlo CV with b subsets of {1,...,n}
if(!is.null(b)){
train <- matrix(ncol = b, nrow = n-n_v)
for (i in 1:b) {
train[,i] <- sample(seq(1,n,1),n-n_v,replace = Replacement)
}
}else{
##For the general case with all subsets of {1,...,n}
train <- combn(seq(1,n,1),n-n_v)
}
#Compute Prediction Errors for all sets in A
MeanPred.Error <- c()
for (i in 1:length(A[1,])) {
Pred.Error <- c()
for (j in 1:length(train[1,])) {
#To make the Code a bit shorter
train.j <- train[,j]
X.train <- X[train.j,A[,i]]
#Prediction Error for a given alpha and a given subset
Pred.Error[j] <- norm(as.matrix(y[-train.j]-X[-train.j,A[,i]]%*%solve(t(X.train)%*%X.train)%*%t(X.train)%*%y[train.j]),"2")^2
}
MeanPred.Error[i] <- 1/length(train[1,])*sum(Pred.Error)
}
TheChosenOne <- which.min(MeanPred.Error)
return(A[,TheChosenOne])
}
##Vector of different Sample sizes
N <- seq(15,30,2)
##Number of True regressors
##We suppose this number to be fixed. We do this for simplicity since we have limited computational power
p.True<- 2
##Vector of different Modeldimensons
P <- seq(p.True+1,6)
##Number of Iteration of Modelfitting
m <- 5
##Criterion Function
##Is minimal iff CV chooses the true model and prefers less conservative Choices
L <- function(model){
+ sum( model > p.True) -sum( (model <= p.True & model > 0) )
}
##Matrix of optimal n_v given n and p
M <- matrix(0L,ncol = length(N), nrow = length(P))
##Simulation
for (i in 1:length(N)) {
##Define the number b of Samplepartions as function of n
##Shao 93 claimed that b=O(n)
b <- n*50
for (j in 1:length(P)) {
n <- N[i]
p <- P[j]
##Grid of possible n_v values for a given Samplezize n in N
N_v <- seq(2,n-p,2)
##Vector of all Criterionfunction values fiven n_v
N_v.CritValue <- rep(0,length(N_v))
for (k in 1:length(N_v)) {
n_v <- N_v[k]
##For statistical Influence fit the Model m times
for (l in 1:m) {
##Generate Data
Data <- DataGen(n,p,p.True)
y <- Data[,1]
X <- Data[,-1]
##Select a Model
model <- CV(n_v,y,X, MonteCarlo = b)
N_v.CritValue[k] <-  N_v.CritValue[k] + L(model)
}
}
TheChoosenOne <- which.min(N_v.CritValue)
M[j,i] <- N_v[TheChoosenOne]
}
}
##Simulation
for (i in 1:length(N)) {
n <- N[i]
##Define the number b of Samplepartions as function of n
##Shao 93 claimed that b=O(n)
b <- n*50
for (j in 1:length(P)) {
p <- P[j]
##Grid of possible n_v values for a given Samplezize n in N
N_v <- seq(2,n-p,2)
##Vector of all Criterionfunction values fiven n_v
N_v.CritValue <- rep(0,length(N_v))
for (k in 1:length(N_v)) {
n_v <- N_v[k]
##For statistical Influence fit the Model m times
for (l in 1:m) {
##Generate Data
Data <- DataGen(n,p,p.True)
y <- Data[,1]
X <- Data[,-1]
##Select a Model
model <- CV(n_v,y,X, MonteCarlo = b)
N_v.CritValue[k] <-  N_v.CritValue[k] + L(model)
}
}
TheChoosenOne <- which.min(N_v.CritValue)
M[j,i] <- N_v[TheChoosenOne]
}
}
View(M)
##Function that Generates Data
#The only purpose of this function is to save some lines of code. Hence that the function
#is not very general
DataGen <- function(n,p,p.True){
##Data for True Regression Model
#Intercept
x_1 <- rep(1,n)
x_2 <- rnorm(n,2,1)
x_3 <- rnorm(n,0,1)
x_4 <- rnorm(n,5,2)
eps <- rnorm(n,0,1)     #Errorterm
##Generate some unecessary extra data
x_5 <- rnorm(n,4,1)
x_6 <- rnorm(n,2,2)
x_7 <- rnorm(n,1,3)
x_8 <- rnorm(n,3,7)
x_9 <- rnorm(n,0,1)
x_10 <- rnorm(n,5,5)
##Possible values for Beta and X
beta.pos <- c(1.5,3,2,5,3,7,4,6,5,7)
X.pos <- cbind(x_1,x_2,x_3,x_4,x_5,x_6,x_7,x_8,x_9,x_10)
beta.vec <- c(beta.pos[1:p.True],rep(0,p-p.True))
X<- X.pos[,1:p]
##Genereta dependent variables acording to the Above statet Model
y <- X%*%beta.vec + eps    #True Model
return(cbind(y,X))
}
Partion <- function(p.True,p){
##For the CV we need to compute the set A of all possible models
#Creats the set {1,...,p} from which we want to generate the Powerset
Index <- seq(1,p,1)
#Denotes the number of Possible Models out of {1,...,p} Regressors
col.A <- 2^p-1
#Denote A as Powerset of {1,...,p}
A <- matrix(0L,nrow = p, ncol = col.A)
k <- choose(p,1)
l <- 1
for (i in 1:p) {
#combn spits out all combinations of i elements in Index
A[1:i,l:k] <- combn(Index,i)
k <-k + choose(p,i+1)
l <- l + choose(p,i)
}
##Split A into two dijoint subsets, ie, the set of Category I Models and Category II Models
##
##To Compute the Set of CI and CII we assume a certain strurcture on the Data. We need X to be s.t all
##"TRUE" regrossers are in the first column of the X Matrix, i.e, if we have P "true" regressors, they
##are given throght X[1],...,X[P]. And evrey X[P+] is trash...
##
##We may need this sets later n for Simulation study's
coln <- c()
for (i in p.True:col.A) {
if(all(seq(1,p.True,1) %in% A[,i])){
coln <- c(coln,i)
}
}
C2 <- A[,coln]           #Set of all Category II Models
C1 <-A[,-coln]           #Set of all Category I Models
return(C2)
}
#Spits out a vector of Integers, i.e, alpha
CV <- function(n_v, y, X, Alpha = NULL, MonteCarlo = NULL, Replacement = FALSE ){  #n_v = #leaved out data points
#n_v          Number of leaved out data points
#y,X          Data for Regression
#Alpha        Set of possible Modelvaritaions for which the CV should be calculated
#             (By Default use all possible Models)
#MonteCarlo   Number of Subsets of {1,...,n} which is randomly drawn for a Monte Carlo CV
#             (By Default do K-Fold CV)
#Replacement  Replacement for Monte Carlo (Default False)
##Change some Variable names to keep the code shorter
A <- Alpha
b <- MonteCarlo
##Number of Possible Regressors
p <- length(X[1,])
##Set of possible Models
if(is.null(A)){
#Creats the set {1,...,p} from which we want to generate the Powerset
Index <- seq(1,p,1)
#Denotes the number of Possible Models out of {1,...,p}
col.A <- 2^p-1
#Denote A as Powerset of {1,...,p}
A <- matrix(0L,nrow = p, ncol = col.A)
k <- choose(p,1)
l <- 1
for (i in 1:p) {
#combn spits out all combinations of i elements in Index
A[1:i,l:k] <- combn(Index,i)
k <-k + choose(p,i+1)
l <- l + choose(p,i)
}
}
##Number of Observations
n <- length(y)
##Combinations of Sample partions for fitting the model
##For the Mone Carlo CV with b subsets of {1,...,n}
if(!is.null(b)){
train <- matrix(ncol = b, nrow = n-n_v)
for (i in 1:b) {
train[,i] <- sample(seq(1,n,1),n-n_v,replace = Replacement)
}
}else{
##For the general case with all subsets of {1,...,n}
train <- combn(seq(1,n,1),n-n_v)
}
#Compute Prediction Errors for all sets in A
MeanPred.Error <- c()
for (i in 1:length(A[1,])) {
Pred.Error <- c()
for (j in 1:length(train[1,])) {
#To make the Code a bit shorter
train.j <- train[,j]
X.train <- X[train.j,A[,i]]
#Prediction Error for a given alpha and a given subset
Pred.Error[j] <- norm(as.matrix(y[-train.j]-X[-train.j,A[,i]]%*%solve(t(X.train)%*%X.train)%*%t(X.train)%*%y[train.j]),"2")^2
}
MeanPred.Error[i] <- 1/length(train[1,])*sum(Pred.Error)
}
TheChosenOne <- which.min(MeanPred.Error)
return(A[,TheChosenOne])
}
##Function that Generates Data
#The only purpose of this function is to save some lines of code. Hence that the function
#is not very general
DataGen <- function(n,p,p.True){
##Data for True Regression Model
#Intercept
x_1 <- rep(1,n)
x_2 <- rnorm(n,2,1)
x_3 <- rnorm(n,0,1)
x_4 <- rnorm(n,5,2)
eps <- rnorm(n,0,1)     #Errorterm
##Generate some unecessary extra data
x_5 <- rnorm(n,4,1)
x_6 <- rnorm(n,2,2)
x_7 <- rnorm(n,1,3)
x_8 <- rnorm(n,3,7)
x_9 <- rnorm(n,0,1)
x_10 <- rnorm(n,5,5)
##Possible values for Beta and X
beta.pos <- c(1.5,3,2,5,3,7,4,6,5,7)
X.pos <- cbind(x_1,x_2,x_3,x_4,x_5,x_6,x_7,x_8,x_9,x_10)
beta.vec <- c(beta.pos[1:p.True],rep(0,p-p.True))
X<- X.pos[,1:p]
##Genereta dependent variables acording to the Above statet Model
y <- X%*%beta.vec + eps    #True Model
return(cbind(y,X))
}
#Spits out a vector of Integers, i.e, alpha
CV <- function(n_v, y, X, Alpha = NULL, MonteCarlo = NULL, Replacement = FALSE ){  #n_v = #leaved out data points
#n_v          Number of leaved out data points
#y,X          Data for Regression
#Alpha        Set of possible Modelvaritaions for which the CV should be calculated
#             (By Default use all possible Models)
#MonteCarlo   Number of Subsets of {1,...,n} which is randomly drawn for a Monte Carlo CV
#             (By Default do K-Fold CV)
#Replacement  Replacement for Monte Carlo (Default False)
##Change some Variable names to keep the code shorter
A <- Alpha
b <- MonteCarlo
##Number of Possible Regressors
p <- length(X[1,])
##Set of possible Models
if(is.null(A)){
#Creats the set {1,...,p} from which we want to generate the Powerset
Index <- seq(1,p,1)
#Denotes the number of Possible Models out of {1,...,p}
col.A <- 2^p-1
#Denote A as Powerset of {1,...,p}
A <- matrix(0L,nrow = p, ncol = col.A)
k <- choose(p,1)
l <- 1
for (i in 1:p) {
#combn spits out all combinations of i elements in Index
A[1:i,l:k] <- combn(Index,i)
k <-k + choose(p,i+1)
l <- l + choose(p,i)
}
}
##Number of Observations
n <- length(y)
##Combinations of Sample partions for fitting the model
##For the Mone Carlo CV with b subsets of {1,...,n}
if(!is.null(b)){
train <- matrix(ncol = b, nrow = n-n_v)
for (i in 1:b) {
train[,i] <- sample(seq(1,n,1),n-n_v,replace = Replacement)
}
}else{
##For the general case with all subsets of {1,...,n}
train <- combn(seq(1,n,1),n-n_v)
}
#Compute Prediction Errors for all sets in A
MeanPred.Error <- c()
for (i in 1:length(A[1,])) {
Pred.Error <- c()
for (j in 1:length(train[1,])) {
#To make the Code a bit shorter
train.j <- train[,j]
X.train <- X[train.j,A[,i]]
#Prediction Error for a given alpha and a given subset
Pred.Error[j] <- norm(as.matrix(y[-train.j]-X[-train.j,A[,i]]%*%solve(t(X.train)%*%X.train)%*%t(X.train)%*%y[train.j]),"2")^2
}
MeanPred.Error[i] <- 1/length(train[1,])*sum(Pred.Error)
}
TheChosenOne <- which.min(MeanPred.Error)
return(A[,TheChosenOne])
}
##Vector of different Sample sizes
N <- seq(15,25,2)
##Number of True regressors
##We suppose this number to be fixed. We do this for simplicity since we have limited computational power
p.True<- 2
##Vector of different Modeldimensons
P <- seq(p.True+1,5)
##Number of Iteration of Modelfitting
m <- 2
##Criterion Function
##Is minimal iff CV chooses the true model and prefers less conservative Choices
L <- function(model){
+ sum( model > p.True) -sum( (model <= p.True & model > 0) )
}
##Matrix of optimal n_v given n and p
M <- matrix(0L,ncol = length(N), nrow = length(P))
View(M)
##Simulation
for (i in 1:length(N)) {
n <- N[i]
##Define the number b of Samplepartions as function of n
##Shao 93 claimed that b=O(n)
b <- n*50
for (j in 1:length(P)) {
p <- P[j]
##Grid of possible n_v values for a given Samplezize n in N
N_v <- seq(2,n-p,2)
##Vector of all Criterionfunction values fiven n_v
N_v.CritValue <- rep(0,length(N_v))
for (k in 1:length(N_v)) {
n_v <- N_v[k]
##For statistical Influence fit the Model m times
for (l in 1:m) {
##Generate Data
Data <- DataGen(n,p,p.True)
y <- Data[,1]
X <- Data[,-1]
##Select a Model
model <- CV(n_v,y,X, MonteCarlo = b)
N_v.CritValue[k] <-  N_v.CritValue[k] + L(model)
}
}
TheChoosenOne <- which.min(N_v.CritValue)
M[j,i] <- N_v[TheChoosenOne]
}
}
M
