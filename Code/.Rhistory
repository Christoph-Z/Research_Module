#is not very general
DataGen <- function(n,p,p.True){
##Data for True Regression Model
#Intercept
x_1 <- rep(1,n)
x_2 <- rnorm(n,2,1)
x_3 <- rnorm(n,0,1)
x_4 <- rnorm(n,5,2)
eps <- rnorm(n,0,1)     #Errorterm
##Generate some unecessary extra data
x_5 <- rnorm(n,4,1)
x_6 <- rnorm(n,2,2)
x_7 <- rnorm(n,1,3)
x_8 <- rnorm(n,3,7)
x_9 <- rnorm(n,0,1)
x_10 <- rnorm(n,5,5)
##Possible values for Beta and X
beta.pos <- c(1.5,3,2,5,3,7,4,6,5,7)
X.pos <- cbind(x_1,x_2,x_3,x_4,x_5,x_6,x_7,x_8,x_9,x_10)
beta.vec <- c(beta.pos[1:p.True],rep(0,p-p.True))
X<- X.pos[,1:p]
##Genereta dependent variables acording to the Above statet Model
y <- X%*%beta.vec + eps    #True Model
return(cbind(y,X))
}
#Spits out a vector of Integers, i.e, alpha
CV <- function(n_v, y, X, Alpha = NULL, MonteCarlo = NULL, Replacement = FALSE ){  #n_v = #leaved out data points
#n_v          Number of leaved out data points
#y,X          Data for Regression
#Alpha        Set of possible Modelvaritaions for which the CV should be calculated
#             (By Default use all possible Models)
#MonteCarlo   Number of Subsets of {1,...,n} which is randomly drawn for a Monte Carlo CV
#             (By Default do K-Fold CV)
#Replacement  Replacement for Monte Carlo (Default False)
##Change some Variable names to keep the code shorter
A <- Alpha
b <- MonteCarlo
##Number of Possible Regressors
p <- length(X[1,])
##Set of possible Models
if(is.null(A)){
#Creats the set {1,...,p} from which we want to generate the Powerset
Index <- seq(1,p,1)
#Denotes the number of Possible Models out of {1,...,p}
col.A <- 2^p-1
#Denote A as Powerset of {1,...,p}
A <- matrix(0L,nrow = p, ncol = col.A)
k <- choose(p,1)
l <- 1
for (i in 1:p) {
#combn spits out all combinations of i elements in Index
A[1:i,l:k] <- combn(Index,i)
k <-k + choose(p,i+1)
l <- l + choose(p,i)
}
}
##Number of Observations
n <- length(y)
##Combinations of Sample partions for fitting the model
##For the Mone Carlo CV with b subsets of {1,...,n}
if(!is.null(b)){
train <- matrix(ncol = b, nrow = n-n_v)
for (i in 1:b) {
train[,i] <- sample(seq(1,n,1),n-n_v,replace = Replacement)
}
}else{
##For the general case with all subsets of {1,...,n}
train <- combn(seq(1,n,1),n-n_v)
}
#Compute Prediction Errors for all sets in A
MeanPred.Error <- c()
for (i in 1:length(A[1,])) {
Pred.Error <- c()
for (j in 1:length(train[1,])) {
#To make the Code a bit shorter
train.j <- train[,j]
X.train <- X[train.j,A[,i]]
#Prediction Error for a given alpha and a given subset
Pred.Error[j] <- norm(as.matrix(y[-train.j]-X[-train.j,A[,i]]%*%solve(t(X.train)%*%X.train)%*%t(X.train)%*%y[train.j]),"2")^2
}
MeanPred.Error[i] <- 1/length(train[1,])*sum(Pred.Error)
}
TheChosenOne <- which.min(MeanPred.Error)
return(A[,TheChosenOne])
}
##Vector of different Sample sizes
N <- seq(15,30,2)
##Number of True regressors
##We suppose this number to be fixed. We do this for simplicity since we have limited computational power
p.True<- 2
##Vector of different Modeldimensons
P <- seq(p.True+1,6)
##Number of Iteration of Modelfitting
m <- 5
##Criterion Function
##Is minimal iff CV chooses the true model and prefers less conservative Choices
L <- function(model){
+ sum( model > p.True) -sum( (model <= p.True & model > 0) )
}
##Matrix of optimal n_v given n and p
M <- matrix(0L,ncol = length(N), nrow = length(P))
##Simulation
for (i in 1:length(N)) {
##Define the number b of Samplepartions as function of n
##Shao 93 claimed that b=O(n)
b <- n*50
for (j in 1:length(P)) {
n <- N[i]
p <- P[j]
##Grid of possible n_v values for a given Samplezize n in N
N_v <- seq(2,n-p,2)
##Vector of all Criterionfunction values fiven n_v
N_v.CritValue <- rep(0,length(N_v))
for (k in 1:length(N_v)) {
n_v <- N_v[k]
##For statistical Influence fit the Model m times
for (l in 1:m) {
##Generate Data
Data <- DataGen(n,p,p.True)
y <- Data[,1]
X <- Data[,-1]
##Select a Model
model <- CV(n_v,y,X, MonteCarlo = b)
N_v.CritValue[k] <-  N_v.CritValue[k] + L(model)
}
}
TheChoosenOne <- which.min(N_v.CritValue)
M[j,i] <- N_v[TheChoosenOne]
}
}
##Simulation
for (i in 1:length(N)) {
n <- N[i]
##Define the number b of Samplepartions as function of n
##Shao 93 claimed that b=O(n)
b <- n*50
for (j in 1:length(P)) {
p <- P[j]
##Grid of possible n_v values for a given Samplezize n in N
N_v <- seq(2,n-p,2)
##Vector of all Criterionfunction values fiven n_v
N_v.CritValue <- rep(0,length(N_v))
for (k in 1:length(N_v)) {
n_v <- N_v[k]
##For statistical Influence fit the Model m times
for (l in 1:m) {
##Generate Data
Data <- DataGen(n,p,p.True)
y <- Data[,1]
X <- Data[,-1]
##Select a Model
model <- CV(n_v,y,X, MonteCarlo = b)
N_v.CritValue[k] <-  N_v.CritValue[k] + L(model)
}
}
TheChoosenOne <- which.min(N_v.CritValue)
M[j,i] <- N_v[TheChoosenOne]
}
}
View(M)
##Function that Generates Data
#The only purpose of this function is to save some lines of code. Hence that the function
#is not very general
DataGen <- function(n,p,p.True){
##Data for True Regression Model
#Intercept
x_1 <- rep(1,n)
x_2 <- rnorm(n,2,1)
x_3 <- rnorm(n,0,1)
x_4 <- rnorm(n,5,2)
eps <- rnorm(n,0,1)     #Errorterm
##Generate some unecessary extra data
x_5 <- rnorm(n,4,1)
x_6 <- rnorm(n,2,2)
x_7 <- rnorm(n,1,3)
x_8 <- rnorm(n,3,7)
x_9 <- rnorm(n,0,1)
x_10 <- rnorm(n,5,5)
##Possible values for Beta and X
beta.pos <- c(1.5,3,2,5,3,7,4,6,5,7)
X.pos <- cbind(x_1,x_2,x_3,x_4,x_5,x_6,x_7,x_8,x_9,x_10)
beta.vec <- c(beta.pos[1:p.True],rep(0,p-p.True))
X<- X.pos[,1:p]
##Genereta dependent variables acording to the Above statet Model
y <- X%*%beta.vec + eps    #True Model
return(cbind(y,X))
}
Partion <- function(p.True,p){
##For the CV we need to compute the set A of all possible models
#Creats the set {1,...,p} from which we want to generate the Powerset
Index <- seq(1,p,1)
#Denotes the number of Possible Models out of {1,...,p} Regressors
col.A <- 2^p-1
#Denote A as Powerset of {1,...,p}
A <- matrix(0L,nrow = p, ncol = col.A)
k <- choose(p,1)
l <- 1
for (i in 1:p) {
#combn spits out all combinations of i elements in Index
A[1:i,l:k] <- combn(Index,i)
k <-k + choose(p,i+1)
l <- l + choose(p,i)
}
##Split A into two dijoint subsets, ie, the set of Category I Models and Category II Models
##
##To Compute the Set of CI and CII we assume a certain strurcture on the Data. We need X to be s.t all
##"TRUE" regrossers are in the first column of the X Matrix, i.e, if we have P "true" regressors, they
##are given throght X[1],...,X[P]. And evrey X[P+] is trash...
##
##We may need this sets later n for Simulation study's
coln <- c()
for (i in p.True:col.A) {
if(all(seq(1,p.True,1) %in% A[,i])){
coln <- c(coln,i)
}
}
C2 <- A[,coln]           #Set of all Category II Models
C1 <-A[,-coln]           #Set of all Category I Models
return(C2)
}
#Spits out a vector of Integers, i.e, alpha
CV <- function(n_v, y, X, Alpha = NULL, MonteCarlo = NULL, Replacement = FALSE ){  #n_v = #leaved out data points
#n_v          Number of leaved out data points
#y,X          Data for Regression
#Alpha        Set of possible Modelvaritaions for which the CV should be calculated
#             (By Default use all possible Models)
#MonteCarlo   Number of Subsets of {1,...,n} which is randomly drawn for a Monte Carlo CV
#             (By Default do K-Fold CV)
#Replacement  Replacement for Monte Carlo (Default False)
##Change some Variable names to keep the code shorter
A <- Alpha
b <- MonteCarlo
##Number of Possible Regressors
p <- length(X[1,])
##Set of possible Models
if(is.null(A)){
#Creats the set {1,...,p} from which we want to generate the Powerset
Index <- seq(1,p,1)
#Denotes the number of Possible Models out of {1,...,p}
col.A <- 2^p-1
#Denote A as Powerset of {1,...,p}
A <- matrix(0L,nrow = p, ncol = col.A)
k <- choose(p,1)
l <- 1
for (i in 1:p) {
#combn spits out all combinations of i elements in Index
A[1:i,l:k] <- combn(Index,i)
k <-k + choose(p,i+1)
l <- l + choose(p,i)
}
}
##Number of Observations
n <- length(y)
##Combinations of Sample partions for fitting the model
##For the Mone Carlo CV with b subsets of {1,...,n}
if(!is.null(b)){
train <- matrix(ncol = b, nrow = n-n_v)
for (i in 1:b) {
train[,i] <- sample(seq(1,n,1),n-n_v,replace = Replacement)
}
}else{
##For the general case with all subsets of {1,...,n}
train <- combn(seq(1,n,1),n-n_v)
}
#Compute Prediction Errors for all sets in A
MeanPred.Error <- c()
for (i in 1:length(A[1,])) {
Pred.Error <- c()
for (j in 1:length(train[1,])) {
#To make the Code a bit shorter
train.j <- train[,j]
X.train <- X[train.j,A[,i]]
#Prediction Error for a given alpha and a given subset
Pred.Error[j] <- norm(as.matrix(y[-train.j]-X[-train.j,A[,i]]%*%solve(t(X.train)%*%X.train)%*%t(X.train)%*%y[train.j]),"2")^2
}
MeanPred.Error[i] <- 1/length(train[1,])*sum(Pred.Error)
}
TheChosenOne <- which.min(MeanPred.Error)
return(A[,TheChosenOne])
}
##Function that Generates Data
#The only purpose of this function is to save some lines of code. Hence that the function
#is not very general
DataGen <- function(n,p,p.True){
##Data for True Regression Model
#Intercept
x_1 <- rep(1,n)
x_2 <- rnorm(n,2,1)
x_3 <- rnorm(n,0,1)
x_4 <- rnorm(n,5,2)
eps <- rnorm(n,0,1)     #Errorterm
##Generate some unecessary extra data
x_5 <- rnorm(n,4,1)
x_6 <- rnorm(n,2,2)
x_7 <- rnorm(n,1,3)
x_8 <- rnorm(n,3,7)
x_9 <- rnorm(n,0,1)
x_10 <- rnorm(n,5,5)
##Possible values for Beta and X
beta.pos <- c(1.5,3,2,5,3,7,4,6,5,7)
X.pos <- cbind(x_1,x_2,x_3,x_4,x_5,x_6,x_7,x_8,x_9,x_10)
beta.vec <- c(beta.pos[1:p.True],rep(0,p-p.True))
X<- X.pos[,1:p]
##Genereta dependent variables acording to the Above statet Model
y <- X%*%beta.vec + eps    #True Model
return(cbind(y,X))
}
#Spits out a vector of Integers, i.e, alpha
CV <- function(n_v, y, X, Alpha = NULL, MonteCarlo = NULL, Replacement = FALSE ){  #n_v = #leaved out data points
#n_v          Number of leaved out data points
#y,X          Data for Regression
#Alpha        Set of possible Modelvaritaions for which the CV should be calculated
#             (By Default use all possible Models)
#MonteCarlo   Number of Subsets of {1,...,n} which is randomly drawn for a Monte Carlo CV
#             (By Default do K-Fold CV)
#Replacement  Replacement for Monte Carlo (Default False)
##Change some Variable names to keep the code shorter
A <- Alpha
b <- MonteCarlo
##Number of Possible Regressors
p <- length(X[1,])
##Set of possible Models
if(is.null(A)){
#Creats the set {1,...,p} from which we want to generate the Powerset
Index <- seq(1,p,1)
#Denotes the number of Possible Models out of {1,...,p}
col.A <- 2^p-1
#Denote A as Powerset of {1,...,p}
A <- matrix(0L,nrow = p, ncol = col.A)
k <- choose(p,1)
l <- 1
for (i in 1:p) {
#combn spits out all combinations of i elements in Index
A[1:i,l:k] <- combn(Index,i)
k <-k + choose(p,i+1)
l <- l + choose(p,i)
}
}
##Number of Observations
n <- length(y)
##Combinations of Sample partions for fitting the model
##For the Mone Carlo CV with b subsets of {1,...,n}
if(!is.null(b)){
train <- matrix(ncol = b, nrow = n-n_v)
for (i in 1:b) {
train[,i] <- sample(seq(1,n,1),n-n_v,replace = Replacement)
}
}else{
##For the general case with all subsets of {1,...,n}
train <- combn(seq(1,n,1),n-n_v)
}
#Compute Prediction Errors for all sets in A
MeanPred.Error <- c()
for (i in 1:length(A[1,])) {
Pred.Error <- c()
for (j in 1:length(train[1,])) {
#To make the Code a bit shorter
train.j <- train[,j]
X.train <- X[train.j,A[,i]]
#Prediction Error for a given alpha and a given subset
Pred.Error[j] <- norm(as.matrix(y[-train.j]-X[-train.j,A[,i]]%*%solve(t(X.train)%*%X.train)%*%t(X.train)%*%y[train.j]),"2")^2
}
MeanPred.Error[i] <- 1/length(train[1,])*sum(Pred.Error)
}
TheChosenOne <- which.min(MeanPred.Error)
return(A[,TheChosenOne])
}
##Vector of different Sample sizes
N <- seq(15,25,2)
##Number of True regressors
##We suppose this number to be fixed. We do this for simplicity since we have limited computational power
p.True<- 2
##Vector of different Modeldimensons
P <- seq(p.True+1,5)
##Number of Iteration of Modelfitting
m <- 2
##Criterion Function
##Is minimal iff CV chooses the true model and prefers less conservative Choices
L <- function(model){
+ sum( model > p.True) -sum( (model <= p.True & model > 0) )
}
##Matrix of optimal n_v given n and p
M <- matrix(0L,ncol = length(N), nrow = length(P))
View(M)
##Simulation
for (i in 1:length(N)) {
n <- N[i]
##Define the number b of Samplepartions as function of n
##Shao 93 claimed that b=O(n)
b <- n*50
for (j in 1:length(P)) {
p <- P[j]
##Grid of possible n_v values for a given Samplezize n in N
N_v <- seq(2,n-p,2)
##Vector of all Criterionfunction values fiven n_v
N_v.CritValue <- rep(0,length(N_v))
for (k in 1:length(N_v)) {
n_v <- N_v[k]
##For statistical Influence fit the Model m times
for (l in 1:m) {
##Generate Data
Data <- DataGen(n,p,p.True)
y <- Data[,1]
X <- Data[,-1]
##Select a Model
model <- CV(n_v,y,X, MonteCarlo = b)
N_v.CritValue[k] <-  N_v.CritValue[k] + L(model)
}
}
TheChoosenOne <- which.min(N_v.CritValue)
M[j,i] <- N_v[TheChoosenOne]
}
}
M
ggplot(d1,aes(x=Samplesize,y=value,colour=key)) +
geom_line(size=1.2) +
coord_cartesian(ylim = c(0.6,1), xlim = c(13, 113)) +
ylab("Prob of choosing a Cat. II Model") + ggtitle("Probability of Cat. II Model") +
theme_bw() +
theme(text = element_text(size=15),
plot.title = element_text(size = 20,hjust = 0.5,face="bold")) +
theme(axis.title.x = element_text(size = 15)) +
theme(axis.title.y = element_text(size = 15)) +
theme(axis.title.y = element_text(margin = margin(t = 0, r = 20, b = 0, l = 0))) +
theme(axis.title.x = element_text(margin = margin(t = 20, r = 0, b = 0, l = 0))) +
theme(plot.title = element_text(margin = margin(t = 0, r = 0 , b = 20, l = 0))) +
theme(legend.title=element_blank(),legend.text = element_text(size = 13)) +
theme(legend.position = c(0.1,0.8))
library(dplyr)
library(tidyr)
MCV <- MCV[1:50]
AIC <- AIC[1:50]
CV1 <- CV1[1:50]
BIC <- BIC[1:50]
N <- N[1:50]
load("/Users/christophzilligen/Documents/Studium/Master/3. Semester/Research_Module/Code/Daten Simulationen.RData")
library(dplyr)
library(tidyr)
MCV <- MCV[1:50]
AIC <- AIC[1:50]
CV1 <- CV1[1:50]
BIC <- BIC[1:50]
N <- N[1:50]
d <- data.frame("Samplesize"=N,"MCCV"=MCV,"AIC"=AIC,"CV1"=CV1,"BIC"=BIC)
d1 <-d %>% gather(key,value,MCCV,AIC,BIC,CV1)
ggplot(d1,aes(x=Samplesize,y=value,colour=key)) +
geom_line(size=1.2) +
coord_cartesian(ylim = c(0.6,1), xlim = c(13, 113)) +
ylab("Prob of choosing a Cat. II Model") + ggtitle("Probability of Cat. II Model") +
theme_bw() +
theme(text = element_text(size=15),
plot.title = element_text(size = 20,hjust = 0.5,face="bold")) +
theme(axis.title.x = element_text(size = 15)) +
theme(axis.title.y = element_text(size = 15)) +
theme(axis.title.y = element_text(margin = margin(t = 0, r = 20, b = 0, l = 0))) +
theme(axis.title.x = element_text(margin = margin(t = 20, r = 0, b = 0, l = 0))) +
theme(plot.title = element_text(margin = margin(t = 0, r = 0 , b = 20, l = 0))) +
theme(legend.title=element_blank(),legend.text = element_text(size = 13)) +
theme(legend.position = c(0.1,0.8))
load("/Users/christophzilligen/Documents/Studium/Master/3. Semester/Research_Module/Code/Daten Simulationen.RData")
library(dplyr)
library(tidyr)
MCV <- MCV[1:50]
AIC <- AIC[1:50]
CV1 <- CV1[1:50]
BIC <- BIC[1:50]
N <- N[1:50]
d <- data.frame("Samplesize"=N,"MCCV"=MCV,"AIC"=AIC,"CV1"=CV1,"BIC"=BIC)
d1 <-d %>% gather(key,value,MCCV,AIC,BIC,CV1)
ggplot(d1,aes(x=Samplesize,y=value,colour=key)) +
geom_line(size=1.2) +
coord_cartesian(ylim = c(0.6,1), xlim = c(13, 113)) +
ylab("Prob of choosing a Cat. II Model") + ggtitle("Probability of Cat. II Model") +
theme_bw() +
theme(text = element_text(size=15),
plot.title = element_text(size = 20,hjust = 0.5,face="bold")) +
theme(axis.title.x = element_text(size = 15)) +
theme(axis.title.y = element_text(size = 15)) +
theme(axis.title.y = element_text(margin = margin(t = 0, r = 20, b = 0, l = 0))) +
theme(axis.title.x = element_text(margin = margin(t = 20, r = 0, b = 0, l = 0))) +
theme(plot.title = element_text(margin = margin(t = 0, r = 0 , b = 20, l = 0))) +
theme(legend.title=element_blank(),legend.text = element_text(size = 13)) +
theme(legend.position = c(0.1,0.8))
ggplot(d1,aes(x=Samplesize,y=value,colour=key)) +
geom_line(size=1.2) +
coord_cartesian(ylim = c(0.6,1), xlim = c(13, 113)) +
ylab("Prob of choosing a Cat. II Model") + ggtitle("Probability of Cat. II Model") +
theme_bw() +
theme(text = element_text(size=15),
plot.title = element_text(size = 20,hjust = 0.5,face="bold")) +
theme(axis.title.x = element_text(size = 15)) +
theme(axis.title.y = element_text(size = 15)) +
theme(axis.title.y = element_text(margin = margin(t = 0, r = 20, b = 0, l = 0))) +
theme(axis.title.x = element_text(margin = margin(t = 20, r = 0, b = 0, l = 0))) +
theme(plot.title = element_text(margin = margin(t = 0, r = 0 , b = 20, l = 0))) +
theme(legend.title=element_blank(),legend.text = element_text(size = 13)) +
theme(legend.position = c(0.1,0.8))
library(ggplot2)
ggplot(d1,aes(x=Samplesize,y=value,colour=key)) +
geom_line(size=1.2) +
coord_cartesian(ylim = c(0.6,1), xlim = c(13, 113)) +
ylab("Prob of choosing a Cat. II Model") + ggtitle("Probability of Cat. II Model") +
theme_bw() +
theme(text = element_text(size=15),
plot.title = element_text(size = 20,hjust = 0.5,face="bold")) +
theme(axis.title.x = element_text(size = 15)) +
theme(axis.title.y = element_text(size = 15)) +
theme(axis.title.y = element_text(margin = margin(t = 0, r = 20, b = 0, l = 0))) +
theme(axis.title.x = element_text(margin = margin(t = 20, r = 0, b = 0, l = 0))) +
theme(plot.title = element_text(margin = margin(t = 0, r = 0 , b = 20, l = 0))) +
theme(legend.title=element_blank(),legend.text = element_text(size = 13)) +
theme(legend.position = c(0.1,0.8))
