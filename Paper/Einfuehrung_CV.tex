\documentclass[Research_Module_ES.tex]{subfiles}

\begin{document}
\section{Cross-Validation: General Framework}
The general framework of \textit{Cross-Validation} was first described by \cite{stone1974cross} and  \cite{geisser1975predictive} as follows: \\

Suppose, we have data $Y=(y_1,\ldots,y_N)$ where each data point $y_i$ is associated with some $x_i$, i.e, $X=(x_1,\ldots,x_N)^\prime$. Note that $x$ and $y$ can be quite general.\\
\\
We are interested in predicting a future value of $y$ for a given value of $x$. Suppose we have a class of possible predictors 
\[
	\{\hat{y}=\hat{y}(Y,X,x,\alpha)|\alpha\in\mathscr{A}\}
\]
where $\alpha$ is some set of unknown values in some parameter space $\mathscr{A}$, e.g $\alpha$ can be an underlying parametric model or a tuning parameter.\\
\\
The idea of \textit{Cross-Validation} is to choose some $\alpha^\ast$ to minimize the loss between our prediction $\hat{y}$ and a corresponding real value of $y$. More explicitly we want $\alpha^\ast$ to solve
\[
	\alpha^\ast=\arg\min_{\alpha\in\mathscr{A}}\{L_{n_\nu}(Y,X\alpha)\}
\]
where $L_{n_\nu}$ is some loss function.\footnote{In statistics a loss function, is a function that maps estimation errors to the set of real numbers.  }
 
Since \cite{larson1931shrinkage} claimed that training an algorithm and evaluating it's statistical performance on the same data yields an overoptimistic result, we need to decompose our observations in order to train and evaluating our predictor. \\
\\
Denote $P^{(N-n_\nu)}_i$ to be the $i$'th partition of $Y$ and $X$ into $N-n_\nu$ retained and $n_\nu$ omitted observations, with $0<n_\nu<M$ where $M$ is the largest integer s.t $\hat{y}$ can be calculated, i.e,
\[
	P^{(N-n_\nu)}_i=(Y_{i,r}^{(N-n_\nu)},X_{i,r}^{(N-n_\nu)},Y_{i,o}^{(N-n_\nu)},X_{i,o}^{(N-n_\nu)})
\]
and let $\Gamma=\Gamma_{N-n_\nu}$ be the set of all relevant partitions of $N-n_\nu$ retained and $n_\nu$ omitted observations. \textit{Cross-Validation} chooses $\alpha$ to minimize the average loss over the partitions in $\Gamma$ 
\[
	L_{N,n_\nu}^{CV}(\alpha)=P^{-1}n_\nu^{-1}\sum_{i\in\Gamma}L_{n_\nu}(Y_{i,o},\hat{Y})
\]
where $P$ denotes the number of elements in $\Gamma$.\\
\\
The name \textit{Cross-Validation} refers to the way of splitting up the observations into several partitions. The same procedure with just one partition of observations is called hold out or validation. Thus \textit{Cross-Validation} yields an averaging of several hold-out estimators.
% Das ist das Kapitel, das für die Motivation unserer Fragestellungen vorgesehen war.(Wie groß muss K gewählt werden, ist CV konsistent...?). Ich hab mir jetzt einfach mal nen "Working titel" ausgedacht...
\subsection{Dependency between the number of omitted data points and the  parameter choice of Cross-Validation}
A natural question is: How many data points should be omitted in order to evaluate our prediction of $\alpha$ and in which way does this influences the prediction properties of the \textit{Cross-Validation} procedure.\\
\\
The in practice most often used variant of \textit{Cross-Validation} is the so called \textit{ Leave-one-out Cross-Validation} with $n_\nu=1$ omitted data points. The reason for the popularity of this type of \textit{Cross-Validation} is it's low computational complexity.\footnote{The computational complexity of CV depends strongly on the number of possible partitions in $\Gamma$, which increases as $n_\nu$ increases, i.e, $|P|=\binom{n}{n_\nu}$}.\\
\\
Throughout this paper we want to study both, the asymptotic and small sample properties of different methods of \textit{Cross-Validation} in the framework of model selection. Especially we will show that \textit{ Leave-one-out Cross-Validation} is asymptotically incorrect and chooses too conservative models.\\
%HABE das mal rausgenommen:
%Moreover we will use simulation studies to find a %rough rule of thumb of choosing $n_\nu$ for a given %sample and model selecting problem.


\end{document}