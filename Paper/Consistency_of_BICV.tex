\documentclass[Research_Module_ES.tex]{subfiles}
\begin{document}
\subsection{Consistency of $BICV(n_v)$}
%SHao vllt. irgendwo zitieren?
Another version is the \textit{Balanced Incomplete CV($n_\nu$)}, denoted for simplicity by $BICV(n_v)$. This variant uses only a subset $\mathcal{B}$ of the set of all possible partitions for a given $n_\nu$, which was given by $\mathcal{R}^\ast= \{s\subseteq\{1,\dots,n\}|\# s=n_v\}$. With $|\mathcal{B}|\equiv b$ as the number of partitions in the collection of partitions $\mathcal{B}$. This method does model selection by 
\begin{align*}
	BICV(n_\nu)=\min_{\alpha\in\mathcal{A}}\hat{\Gamma}_{\alpha,n}^{BICV}
\end{align*}
where $\hat{\Gamma}_{\alpha,n}^{BICV}$ is the estimator of $\Gamma_{\alpha,n-n_\nu}$. The structure of this estimator is the same as in Claim \ref{estimator CV(n_v)} by only using the subset $\mathcal{B}$ instead of $\mathcal{R}^\ast$, thus the estimator has the following form
\begin{align*}
	\hat{\Gamma}_{\alpha,n}^{BICV}=|\mathcal{B}|^{-1}n_\nu^{-1}\sum_{s\in\mathcal{B}}\parallel y_s-\hat{y}_{\alpha,s^c}\parallel^2
\end{align*}
 and in the next step the reader will be introduced on how $BICV(n_v)$ chooses his partitions, namely by a combinatorical procedure such that the following \textit{balance conditions} hold.
%VIelleicht ein Lemma daraus machen??
\begin{defi}
\label{balance_property}
Let $\mathcal{B}$ be a collection of $b$ subsets of $\{ 1,\dots,n\}$ that have size $n_v$ so that the following two conditions hold:
\begin{enumerate}
\item[(a)] every $i$, $1\le i \le n$, appears in the same number of subsets of $\mathcal{B}$
\item[(b)] every pair $(i,j)$, $1\le i < j \le n$, appears in the same number of subsets of $\mathcal{B}$
\end{enumerate}
\end{defi}

Thus we get the following claim\footnote{Proof of Claim \ref{Claim_BICV} is given in Appendix \RM{1}}.

\begin{claim}~
	\label{Claim_BICV}
	\begin{enumerate}[(I)]
	\item $\#\{s\in \mathcal{B}| i\in s\} = \frac{n_v}{n} b$ 
	\item $\#\{s\in\mathcal{B}|(i,j)\in s, j>i\}=n_vb\frac{n_v-1}{n(n-1)}$
	\end{enumerate}
\end{claim}


%HIER vielleicht was zur BLock desing matrix schreiben?



%FUelltext für die Fußnote
And the following Theorem\footnote{Proof of Theorem \ref{THM_Consistency_BICV} is given in Appendix \RM{1}}
\begin{thm}[Consistency of $BICV(n_v)$]
	\label{THM_Consistency_BICV}
Under the conditions of Theorem \ref{THM_Consistency of $CV(1)$} and
\begin{align}
\lim_{n\to\infty} \max_{s\in \mathcal{B}}\biggl\lVert \frac{1}{n_v}\sum_{i\in s}x_ix_i' - \frac{1}{n-n_v}\sum_{i\in s^c}x_ix_i'\biggr\rVert =0. \label{gram_matrix_condition_BICV}
\end{align}
Suppose furthermore that $n_v$ is selected such that
\begin{align}
\frac{n_v}{n}\to 1 \quad \textrm{and} \quad n-n_v \to \infty. \label{growth_rates_nv_BICV}
\end{align}
Then the following holds
\begin{enumerate}[(I)]
\item If $\mathcal{M}_\alpha$ is in Category I, then there exists $R_n \ge 0$ such that $\hat{\Gamma}_{\alpha,n}^{BICV} = \frac{1}{n}\varepsilon'\varepsilon + \Delta_{\alpha,n} + R_n + o_P(1)$.
\item If $\mathcal{M}_\alpha$ is in Category II, then $\hat{\Gamma}_{\alpha,n}^{BICV} = \frac{1}{n}\varepsilon'\varepsilon + \frac{1}{n-n_v}d_\alpha\sigma^2  + o_P((n-n_v)^{-1})$.
\item $\lim_{n\to\infty}P(\mathcal{M}_{BICV}=\mathcal{M}_\ast) = 1$
\end{enumerate}
where $\mathcal{M}_{BICV}$ denotes the model selected by using $BICV(n_v)$.
\end{thm}
~
\\\\
~
%IRgendwie einen übergang und eine eiene Beschreibung schaffen(keine eigenen Worte, nur notizen hier benutzt und versucht zu sortieren)
\textbf{NOTIZEN/Überlegungen}\\
-explanation why $BICV(n_v)$ improves over CV(1):\\
-$n_\nu$ should be choosen according to\\ \begin{align*}
\frac{n_v}{n}\to 1 \quad \textrm{and} \quad n-n_v \to \infty.
\end{align*}
-need a large $n_\nu$ (for assessing the prediction ability) and a relative small $n-n_\nu$ (for construction data) because then
\begin{align*}
	\Gamma_{\alpha,n-n_\nu}=\sigma^2+(n-n_\nu)^{-1}d_\alpha\sigma^2
\end{align*}
is not a flat function and therefore it is easier to find a minimum of $\Gamma_{\alpha,n-n_\nu}$ for a small $n-n_\nu$\\
-But the difference (that has to be small in comparison to the number of data used for validation) has to fullfill $n-n_v \to \infty$ to ensure the consistency of the model fit\\
-Need condition $\frac{n_v}{n}\to 1$ for the choice of $n_\nu$, because if this condition does not hold, $BICV(n_v)$ cannot distinguish in between the models in Category II and thus this method would be inconsistent (This is the main problem of CV(1) that occurs)\\
%ICh denke das ist gemeint beim teleskop argument, BICV ist fähiger die unterschiede besser zu erkennen, also unter gegebenen umständen kann es sogar innerhalb der cat2 models unterscheiden, was cv1 einfach nicht kann- so habe ich es verstanden, damit ist die vorhersagegenauigkeit einfach besser bei bicv als bei cv1 (also bicv erkennt unterschiede noch besser als cv1 und gewährt somit eine bessere modelselection)
-As we have seen, CV(1) is asymptotically inconsistent, it cannot distinugish in between category II models; if we choose $n_\nu$ such that these two conditions for $n_\nu$ in Theorem \ref{THM_Consistency_BICV} do hold, then $BICV(n_v)$ improves over CV(1) because it is able to distinguish and consistent

\end{document}