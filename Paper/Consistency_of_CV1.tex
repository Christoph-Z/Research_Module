\documentclass[Research_Module_ES.tex]{subfiles}
\begin{document}
\subsection{Theory of CV(1)}
%NOch ausschreiben
A special case of the \textit{CV($n_\nu$)} is the \textit{Leave-one-Out Cross-Validation}, which might be the most popular version in literature.
To compute the $CV(1)$ estimate, one deletes one observation of the data set ($n_\nu\equiv1$) and uses the remaining to construct the estimator of the model. Then one uses this model to predict the
deleted observation.

Thus $CV(1)$ is the computationally simplest \textit{Cross-Validation} estimator as only $n$ terms have to be estimated whereas in other $CV(n_v)$ estimators one has to compute $\binom{n}{n_v}$ terms.\\

We capture the asymptotic properties of $CV(1)$ in the following theorem of \cite{shao}, which we will prove in the appendix\footnote{Proof of Theorem \ref{THM_Consistency of $CV(1)$} is given in Appendix \RM{1}}.

\begin{thm}[Consistency of $CV(1)$]
Under the conditions 
\label{THM_Consistency of $CV(1)$}
\begin{enumerate}[(i)]
\item $X'X = O(n) \quad \textrm{and} \quad (X'X)^{-1}=O(\frac{1}{n})$
\item $ \lim_{n \to \infty} \max_{1\le i\le n} p_{ii\alpha} =0 \quad \forall \alpha \in \mathcal{A} $ 
\end{enumerate}
the following holds
\begin{enumerate}[(I)]
\item If $\mathcal{M}_\alpha$ is in Category I, then $\hat{\Gamma}_{\alpha,n}^{CV} = \Gamma_{\alpha,n} + o_P(1)$.
\item If $\mathcal{M}_\alpha$ is in Category II, then $\hat{\Gamma}_{\alpha,n}^{CV} = \frac{1}{n}\varepsilon'\varepsilon + \frac{2}{n}d_\alpha\sigma^2 - \frac{1}{n}\varepsilon'P_\alpha\varepsilon + o_P(n^{-1})$.
\end{enumerate}
If furthermore the following condition holds
\begin{itemize}
\item[$(iii)$] $\liminf_{n\to \infty} \Delta_{\alpha,n} > 0,$
\end{itemize}
then 
\begin{enumerate}
\item[(III)] $\lim_{n\to\infty} P(\mathcal{M}_{CV}\textrm{ is in Category I})=0$,
\item[(IV)]If $\mathcal{M}_\ast$ is not of size $p$, then $\lim_{n\to\infty}P(\mathcal{M}_{CV}=\mathcal{M}_\ast) \neq 1$
\end{enumerate}
where $\mathcal{M}_{CV}$ denotes the model selected by using $CV(1)$.
\end{thm}

Assertions $(I)$ and $(II)$ of the theorem state that $CV(1)$ consistently estimates the average squared prediction error. To see this in the case of $(II)$, note that $\frac{1}{n}\varepsilon'\varepsilon$ converges almost surely to $\sigma^2$ and that 
$\frac{1}{n}\varepsilon'P_\alpha\varepsilon$ converges in probability to $\frac{1}{n}d_\alpha\sigma^2$.\

However the second statement gives a more precise description of the behavior of the $CV(1)$ estimate for models in Category II. \\

But consistency isn't enough to ensure that $CV(1)$ will detect the true model. 
%INDEED ist falsch getrennt, warum weiß ich leider nicht
Indeed, for any model in Category II the average squared prediction error reduces to the variance of the error term and hence are asymptotically indestinguishable. 
%DAZU vielleicht noch $\Gamma_{\alpha,n}\rightarrow\sigma^2$ angeben, als formale komponente ausgeschrieben?
This idea is made precise by the statement $(IV)$ that the there exists some positive probability that the true model isn't chosen. On the other hand Shao shows in statement $(III)$ that the selected model is with probability approaching one in Category II.\\

A heuristic explanation for this incorrectness is delivered by statement $(II)$. The model defining terms are $\frac{1}{n}d_\alpha\sigma^2$ and the difference $\frac{1}{n}d_\alpha\sigma^2 - \frac{1}{n}\varepsilon'P_\alpha\varepsilon$. This difference has mean zero and is of the same order of magnitude as the first term. This gives an explanation why $CV(1)$ tends to select to large models.\\

The assumption $(i)$ isn't that restrictive. It bounds the eigenvalues of $\frac{1}{n}X'X$ from above and below and therefore assures the assumptotic invertibility of this matrix. If the $x_i$ are stochastic, it is fulfilled for example if the $x_i$ are iid and if the matrix $\mathrm{E}[x_ix_i']$ is invertible.\\

Under assumption $(i)$ one can show that assumption $(ii)$ is equivalent to 
%HIERZU hätte ich gerne einen kurzen nachweis
$\lim \max n^{-1}\lVert x_i\rVert^2=0$. That is, the observations are allowed to grow but not too fast. This condition is for example satisfied if the $x_i$ are bounded.




\end{document}