\documentclass[Research_Module_ES.tex]{subfiles}
\begin{document}
\subsection{Theory of CV(1) (Titel finden)}
To compute the $CV(1)$ or \textit{leave one out cross validtation} estimate one deletes one observation of the data set and uses the remaining to construct the estimator of the model. Then one uses this model to predict the deleted observation. ... bla bla bla\

$CV(1)$ is the computationally simplest cross validation estimator as only $n$ terms have to be estimated whereas in other $CV(n_v)$ estimators one has to compute $\binom{n}{n_v}$ terms.\

We capture the asymptotic properties of $CV(1)$ in the following theorem, which we will prove in the appendix.

\begin{satz}[Consistency of $CV(1)$]
Under the conditions 
\begin{enumerate}[(i)]
\item $X'X = O(n) \quad \textrm{and} \quad (X'X)^{-1}=O(\frac{1}{n})$
\item $ \lim_{n \to \infty} \max_{1\le i\le n} p_{ii\alpha} =0 \quad \forall \alpha \in \mathcal{A} $ 
\end{enumerate}
the following holds
\begin{enumerate}[(I)]
\item If $\mathcal{M}_\alpha$ is in Category I, then $\hat{\Gamma}_{\alpha,n}^{CV} = \Gamma_{\alpha,n} + o_P(1)$.
\item If $\mathcal{M}_\alpha$ is in Category II, then $\hat{\Gamma}_{\alpha,n}^{CV} = \frac{1}{n}\varepsilon'\varepsilon + \frac{2}{n}d_\alpha\sigma^2 - \frac{1}{n}\varepsilon'P_\alpha\varepsilon + o_P(n^{-1})$.
\end{enumerate}
If furthermore the following condition holds
\begin{itemize}
\item[$(iii)$] $\liminf_{n\to \infty} \Delta_{\alpha,n} > 0,$
\end{itemize}
then 
\begin{enumerate}
\item[(III)] $\lim_{n\to\infty} P(\mathcal{M}_{CV}\textrm{ is in Category I})=0$,
\item[(IV)]If $\mathcal{M}_\ast$ is not of size $p$, then $\lim_{n\to\infty}P(\mathcal{M}_{CV}=\mathcal{M}_\ast) \neq 1$
\end{enumerate}
where $\mathcal{M}_{CV}$ denotes the model selected by using $CV(1)$.
\end{satz}

Assertions $(I)$ and $(II)$ of the theorem state that $CV(1)$ consistently estimates the average squared prediction error. To see this in the case of $(II)$, note that $\frac{1}{n}\varepsilon'\varepsilon$ converges almost surely to $\sigma^2$ and that 
$\frac{1}{n}\varepsilon'P_\alpha\varepsilon$ converges in probability to $\frac{1}{n}d_\alpha\sigma^2$.\

However the second statement gives a more precise description of the behavior of the $CV(1)$ estimate for models in Category II. \

But consistency isn't enough to ensure that $CV(1)$ will detect the true model. Indeed, for any model in Category II the average squared prediction error reduces to the variance of the error term and hence are asymptotically indestinguishable. This idea is made precise by the statement $(IV)$ that the there exists some positive probability that the true model isn't chosen. On the other hand Shao shows in statement $(III)$ that the selected model is with probability approaching one in Category II.\

A heuristic explanation for this incorrectness is delivered by statement $(II)$. The model defining terms are $\frac{1}{n}d_\alpha\sigma^2$ and the difference $\frac{1}{n}d_\alpha\sigma^2 - \frac{1}{n}\varepsilon'P_\alpha\varepsilon$. This difference  is a random variable with mean zero and is of the same order of magnitude as the first term.
Hence, there is a positive probability that $CV(1)$ will select to large models. 

Assumption $(i)$ isn't that restrictive. It bounds the eigenvalues of $\frac{1}{n}X'X$ from above and below and therefore assures the assymptotic invertibility of this matrix. 
For examplfe it is fulfilled if the $x_i$ are iid with finite second moments that are bounded from below as then 
\begin{align*}
\lim_{n\to \infty} \frac{1}{n} \sum_{i=1}^n x_ix_i' = \mathrm{E}\bigl[ x_ix_i' \bigr] \quad a.s.
\end{align*} 
by the strong law of large numbers. Since the $x_i$ are iid their covariance matrix is positive definit and invertible. 

\textbf{Claim:} In the iid setting it holds that
\begin{align*}
\mathrm{rank}(X) = p \quad \forall n\ge p \quad a.s.
\end{align*}
(could it be that this statement is too strong? Should I restate it in the sufficiently large terminology?)
\textbf{Proof:}(UEBERARBEITEN!!! Und ich muss mir noch Gedanken machen was für annahmen ich bzgl der einzelnen variablen in einem der Zufallsvektoren stellen muss; reicht keine lineare starke lineare Abhangigkeit?) 
Gedanken sammeln: Gilt 
Spalten von $X$ linear abhängig $\iff$ Korrelation zwischen einzelnen Spalten ist 1?
for $p=2$ let $x_{i,1}=\lambda x_{i,2}$. Then the correlation of these columns is 
\begin{align*}
\mathrm{cor}(x_1,x_2) &= \frac{\frac{1}{n}\sum_{i=1}^n x_{i,1}x_{i,2}}
{\sqrt{\frac{1}{n}\sum_{i=1}^n x_{i,1}^2}\sqrt{\frac{1}{n}\sum_{i=1}^n x_{i,2}^2}}\\
&= \frac{\frac{1}{n}\sum_{i=1}^n \lambda x_{i,2}^2}
{\sqrt{\frac{1}{n}\sum_{i=1}^n \lambda^2 x_{i,2}^2}\sqrt{\frac{1}{n}\sum_{i=1}^n x_{i,2}^2}} = 1
\end{align*}
Evtl gehts schneller ueber masstheorie: Wenn der Rang von X voll ist, spannen die Spaltenvektoren den gesamten Rp auf. Irgednwas das der zufallsvektor ein mass bzgl ner kombination zwischen lebesgue und dirac massen ist.
dann argumentiere, dass ein linearer unterraum kein volumen bzgl des lebesgue maßes hat, also nur mit wahrscheinlichkeit null eintritt. und andererseits, dass das ereignis der linearen abhangigkeit bei punktmassen nur eintreffen kann, wenn die variablen verteilungen in einem geeingeten verhaltnis zueinander legen, welches ich im vorfeld ausschliessen musste.


Under assumption $(i)$ one can show that the second assumption is equivalent to $\lim \max n^{-1}\lVert x_i\rVert^2=0$. That is, the observations are allowed to grow but not too fast. This condition is for example satisfied if the $x_i$ are bounded.

For arbitrary $\alpha \in \mathcal{A}$ holds
\begin{align*}
\lim_{n\to\infty} \max_{1\le i\le n} p_{ii\alpha} &= \lim_{n\to\infty} \max_{1\le i\le n} x_{i,\alpha}'(X_\alpha'X_\alpha)^{-1} x_{i,\alpha} \\
&= \lim_{n\to\infty} \max_{1\le i\le n} \frac{1}{n} x_{i,\alpha}'\biggl(\frac{X_\alpha'X_\alpha}{n}\biggr)^{-1} x_{i,\alpha} \\
&\le \lim_{n\to\infty} \max_{1\le i\le n} \frac{1}{n} \underbrace{\max_{z\neq 0} \frac{z'\bigl(\frac{X_\alpha'X_\alpha}{n}\bigr)^{-1}z}{z'z}}_{\le C^\ast < \infty \textrm{ for sufficiently large $n$} }  \lVert x_{i,\alpha} \rVert ^2 \\
&\le C^\ast \lim_{n\to \infty} \max_{1\le i \le n} \frac{\lVert x_{i,\alpha}\rVert ^2}{n}
\end{align*}
and 
\begin{align*}
\lim_{n\to \infty} \max_{1\le i \le n} p_{ii\alpha} &=  \lim_{n\to\infty} \max_{1\le i\le n} \frac{1}{n} x_{i,\alpha}'\biggl(\frac{X_\alpha'X_\alpha}{n}\biggr)^{-1} x_{i,\alpha} \\
&\ge \lim_{n\to\infty} \max_{1\le i\le n} \frac{1}{n} \underbrace{\min_{z\neq 0} \frac{z'\bigl(\frac{X_\alpha'X_\alpha}{n}\bigr)^{-1}z}{z'z}}_{\ge C_\ast > 0 \textrm{ for $n$ sufficiently large}} \lVert x_{i,\alpha} \rVert ^2\\
&\ge C_\ast \lim_{n\to\infty} \max_{1\le i\le n} \frac{\lVert x_{i,\alpha}\rVert^2}{n}.
\end{align*}
Hence, 
\begin{align*}
\lim_{n\to \infty} \max_{1\le i \le n} p_{ii\alpha} = 0 \iff \lim_{n\to\infty} \max_{1\le i\le n} \frac{\lVert x_{i,\alpha}\rVert^2}{n} = 0.
\end{align*}
Furthermore there is no relation between condition $(i)$ and $(ii)$. To clarify this, note the following to examples:

1. 
condition $(ii)$ isn't sufficient to ensure asymptotic invertibility of $X_\alpha'X_\alpha$. To see this, take a look at the following design for $p=2$:
\begin{align*}
x_{i,1}= \begin{cases}
1 & \textrm{$i$ is even}\\
0 & \textrm{else}
\end{cases}
\end{align*}
and 
\begin{align*}
x_{i,2} = \begin{cases}
\frac{1}{n} & \textrm{$i$ is odd}\\
0 & \textrm{else}
\end{cases}
\end{align*}
Then for any $n$ it holds that
\begin{align*}
\sum_{i=1} ^n x_{i,1}x_{i,2} = 0 \\
\sum_{i=1} ^n x_{i,1}^2 \le \frac{n}{2}\\
\sum_{i=1} ^n x_{i,2}^2 \to \frac{\pi ^2}{6}
\end{align*}
THIS IS NO EXAMPLE AS WE NEED ASYMPTOTIC INVERTEBILITY FOR BEING ABLE TO SPEAK ABOUT THE SEQUENCE!!!
I want to find a design that doesn't grow at n but the diagonal entries of the projection matrix are still converging uniformly to zero. 
Actually this should be quite simple.. Just take some sequence which grows at speed say n squared 
e.g. $x_{i,1} = \sqrt{i} \Rightarrow \sum_i x_i^2 = n(n-1)/2$ 
HOORAY!!THIS ONE WORKS!!!
As for the gram matrix (if I choose the second variable orthogonal to the first but else in the same manner as the first (or just some constant sequence with holes for orthogonality. Then teh inverse is shrinking at speed n-2 where the maximal elemts are growing root n resp n (as we're looking at a quadratic form) 

(2.) the second example was my jumping powers of two design which shows that there exists designs that grow at order n and whose variances aren't bounded.

\textbf{Proof of $(I)$:} A first order Taylor expansion of $(1-x)^{-2}$ in $x$ around $0$ yields 
\begin{align*}
(1- x)^{-2} = 1+ 2 x + o(x^2) \quad \textrm{as} \quad x\to 0 .
\end{align*}
By condition $(ii)$ and as
\begin{align*}
2p_{ii\alpha} + o(p_{ii\alpha}^2) 
\le 2 \max_{1\le i \le n} p_{ii\alpha} +  o(p_{ii\alpha}^2) 
= O\bigl(\max _{1\le i \le n} p_{ii\alpha}\bigr)
\end{align*}
it holds that 
\begin{align*}
(1- p_{ii\alpha})^{-2} = 1+ O(\max _{1\le i \le n} p_{ii\alpha}) 
= 1 +o(1) \quad \textrm{as} \quad n \to \infty. 
\end{align*}

Using this expression to rewrite $\hat{\Gamma}_{\alpha,n}^{CV}$ yields
\begin{align*}
\hat{\Gamma}_{\alpha,n}^{CV} &= \frac{1}{n}\sum_{i=1}^n (1- p_{ii\alpha})^{-2}(y_i-x_{i,\alpha}\hat{\beta}_\alpha)^2\\
&= (1+o(1))\frac{1}{n}\lVert y-X_{\alpha}\hat{\beta}_\alpha\rVert^2\\
&= (1+o(1))\frac{1}{n}\lVert (I_n-P_\alpha) y\rVert^2\\
&= (1+o(1))\frac{1}{n}\lVert (I_n-P_\alpha) X_\alpha\beta_\alpha +(I_n-P_\alpha)\varepsilon \rVert^2\\
&= (1+o(1))\biggl[\frac{1}{n}\lVert(I_n-P_\alpha)\varepsilon \rVert^2+ \underbrace{\frac{1}{n}\lVert (I_n-P_\alpha) X_\alpha\beta_\alpha\rVert^2}_{=\Delta_{\alpha,n}} +\frac{2}{n}\varepsilon'(I_n-P_\alpha) X_\alpha\beta_\alpha\biggr]\\
&= (1+o(1)) \biggl[\frac{\varepsilon'\varepsilon}{n} + \Delta_{\alpha,n} - \frac{\varepsilon'P_\alpha\varepsilon}{n} +\frac{2}{n}\varepsilon'(I_n-P_\alpha) X_\alpha\beta_\alpha\biggr]. \quad (\ast)
\end{align*}

Since the $\varepsilon_i$ are iid with finite second moments it follows by the law of large numbers that
$\frac{\varepsilon'\varepsilon}{n} = \sigma^2 + o_P(1)$.
Furthermore for any positive $\delta$
\begin{align*}
P\biggl(\biggl|\frac{1}{n}\varepsilon'P_\alpha\varepsilon\biggr|>\delta\biggr) \le \frac{\mathrm{E}\bigl[|\varepsilon'P_\alpha\varepsilon|\bigr]}{n\delta}=\frac{\mathrm{E}\bigl[\lVert P_\alpha\varepsilon\rVert^2\bigr]}{n\delta}=\frac{d_\alpha \sigma^2}{n\delta} \to 0 \quad \textrm{as} \quad n\to \infty
\end{align*}
by the Markov inequality and hence $\frac{1}{n}\varepsilon'P_\alpha\varepsilon = o_P(1)$.\\
Finally by the Cebysev inequality it holds for any positve $\delta$ that
\begin{align*}
P\biggl(&\biggl|\frac{2}{n}\varepsilon'(I_n-P_\alpha)X_\alpha\beta_\alpha\biggr|>\delta\biggr) \le \frac{4\mathrm{E}\bigl[(\varepsilon'(I_n-P_\alpha)X_\alpha\beta_\alpha)^2\bigr]}{n^2\delta^2}\\
&=\frac{4}{n^2\delta^2}\sum_{i,j}\mathrm{E}\bigl[\varepsilon_i[(I_n-P_\alpha)X_\alpha\beta_\alpha]_i\varepsilon_j[(I_n-P_\alpha)X_\alpha\beta_\alpha]_j\bigr]\\
&=\frac{4}{n^2\delta^2}\sum_{i=1}^n\mathrm{E}\bigl[\varepsilon_i^2\bigr][(I_n-P_\alpha)X_\alpha\beta_\alpha]_i^2 \quad \textrm{by independence of the $\varepsilon_i$}\\
&=\frac{4\sigma^2}{n^2\delta^2}\lVert(I_n-P_\alpha)X_\alpha\beta_\alpha\rVert^2 \\
&\le \frac{4\sigma^2}{n^2\delta^2} \underbrace{\lVert I_n-P_\alpha \rVert_2}_{=1} \underbrace{\max_{z\in \mathbb{R}^{d_\alpha}\backslash 0}\frac{z'X'_\alpha X_\alpha z}{z'z}}_{=O(n)} \lVert \beta_\alpha \rVert^2
=O(n^{-1})
\end{align*}
implying $\frac{2}{n}\varepsilon'(I_n-P_\alpha)X_\alpha\beta_\alpha=o_P(1)$. 
In the last inequality we used condition $(i)$ and that $I_n-P_\alpha$ is a projection matrix and hence $\lVert I_n-P_\alpha \rVert_2 = 1$.\\
Combining $\frac{\varepsilon'\varepsilon}{n} = \sigma^2 + o_P(1)$, $\frac{1}{n}\varepsilon'P_\alpha\varepsilon = o_P(1)$ and $\frac{2}{n}\varepsilon'(I_n-P_\alpha)X_\alpha\beta_\alpha=o_P(1)$ with $(\ast)$ yields $\hat{\Gamma}_{\alpha,n}^{CV} = \sigma^2 + \Delta_{\alpha,n} + o_P(1)$ which is asymptotically equivalent to $\Gamma_{\alpha,n}$.This proves the first assertion.\\

\textbf{Proof of $(II)$:}To show the second statement, assume that $\mathcal{M}_\alpha$ is in Category II.
Using the linearization of the proof of $(I)$ we get
\begin{align*}
\hat{\Gamma}_{\alpha,n}^{CV} &= \frac{1}{n}\sum_{i=1}^n (1- p_{ii\alpha})^{-2}(y_i-x_{i,\alpha}\hat{\beta}_\alpha)^2\\
&= (1+o(1))\frac{1}{n}\lVert y-X_{\alpha}\hat{\beta}_\alpha\rVert^2 + \frac{2}{n}\sum_{i=1}^n p_{ii\alpha}(y_i-x_{i,\alpha}\hat{\beta}_\alpha)^2. 
\end{align*}
Repeating the same calculations as in $(\ast)$ and using that for all $\alpha$ in Category II holds $\Delta_{\alpha,n}=0$ and $\varepsilon'(I_n-P_\alpha)X_\alpha\beta_\alpha =0$, yields
\begin{align*}
\hat{\Gamma}_{\alpha,n}^{CV} &=\frac{\varepsilon'\varepsilon}{n} + \Delta_{\alpha,n} - \frac{\varepsilon'P_\alpha\varepsilon}{n} +\frac{2}{n}\varepsilon'(I_n-P_\alpha) X_\alpha\beta_\alpha+ \frac{2}{n}\sum_{i=1}^n p_{ii\alpha}(y_i-x_{i,\alpha}\hat{\beta}_\alpha)^2 +o(1)\\
&= \frac{\varepsilon'\varepsilon}{n}- \frac{\varepsilon'P_\alpha\varepsilon}{n} + \frac{2}{n}\sum_{i=1}^n p_{ii\alpha}(y_i-x_{i,\alpha}\hat{\beta}_\alpha)^2 +o(1).
\end{align*}
Thus it remains to show that $\sum_{i=1}^n p_{ii\alpha}(y_i-x_{i,\alpha}\hat{\beta}_\alpha)^2= d_\alpha \sigma^2 + o_P(1)$.
To establish this result we first show that $\sum_{i=1}^n p_{ii\alpha}(y_i-x_{i,\alpha}\hat{\beta}_\alpha)^2= \sum_{i=1}^n p_{ii\alpha}\varepsilon_i^2 + o_P(1)$ and then we argue that $\sum_{i=1}^n p_{ii\alpha}\varepsilon_i^2=d_\alpha\sigma^2+o_P(1)$.\\

(1.) One can rewrite 
\begin{align*}
\sum_{i=1}^n p_{ii\alpha}(y_i-x_{i,\alpha}\hat{\beta}_\alpha)^2 &= (y-X_\alpha\hat{\beta}_\alpha)'\mathrm{diag}(p_{ii\alpha})(y-X_\alpha\hat{\beta}_\alpha) 
= \varepsilon'(I-P_\alpha)\mathrm{diag}(p_{ii\alpha})(I-P_\alpha)\varepsilon\\
&= \varepsilon'\mathrm{diag}(p_{ii\alpha})\varepsilon-2\varepsilon'\mathrm{diag}(p_{ii\alpha})P_\alpha\varepsilon+\varepsilon'P_\alpha\mathrm{diag}(p_{ii\alpha})P_\alpha \varepsilon. \quad (\Delta)
\end{align*}
$\varepsilon'\mathrm{diag}(p_{ii\alpha})P_\alpha\varepsilon$ vanishes in probability as
\begin{align*}
\varepsilon'\mathrm{diag}(p_{ii\alpha})P_\alpha\varepsilon= \sum_{i=1}^n\sum_{j=1}^np_{ii\alpha}p_{ij\alpha}\varepsilon_i\varepsilon_j
=\sum_{i}p_{ii\alpha}^2\varepsilon_i^2 + \sum_{i}\sum_{j\neq i}p_{ii\alpha}p_{ij\alpha}\varepsilon_i\varepsilon_j=o_P(1).
\end{align*}
The first part converges to zero by Markov inequality 
\begin{align*}
P\biggl(\biggl|\sum_{i}p_{ii\alpha}^2\varepsilon_i^2\biggr|>\delta\biggr)
\le \frac{1}{\delta}\sum_{i}p_{ii\alpha}^2\mathrm{E}\bigl[\varepsilon_i^2\bigr]
\le \frac{\sigma^2}{\delta}\biggl(\max_{1\le i\le n}p_{ii\alpha}\biggr) \sum_{i}p_{ii\alpha} 
= \frac{d_\alpha\sigma^2}{\delta}\max_{1\le i\le n}p_{ii\alpha} \to 0 
\end{align*}
and the second part similarly 
\begin{align*}
P\biggl(\biggl|\sum_{i}&\sum_{j\neq i}p_{ii\alpha}p_{ij\alpha}\varepsilon_i\varepsilon_j\biggr|>\delta\biggr)\le\frac{1}{\delta^2}\sum_{i}\sum_{j\neq i}p_{ii\alpha}^2p_{ij\alpha}^2\mathrm{E}\bigl[\varepsilon_i^2\varepsilon_j^2\bigr]
= \frac{\sigma^4}{\delta^2}\sum_{i}p_{ii\alpha}^2\sum_{j\neq i}p_{ij\alpha}^2\\
&\le \frac{\sigma^4}{\delta^2}\sum_{i}p_{ii\alpha}^3 \le \frac{d_\alpha\sigma^4}{\delta^2}\biggl(\max_{1\le i\le n}p_{ii\alpha}\biggr)^2 \to 0.
\end{align*}
Where we used that for the projection matrix $P_\alpha$ holds that $\sum_{j}p_{ij\alpha}^2=p_{ii\alpha}$.\\

The last part in $(\Delta)$ is asymptotically negligible as well since
\begin{align*}
\varepsilon'P_\alpha\mathrm{diag}(p_{ii\alpha})P_\alpha \varepsilon = \sum_i\sum_j\sum_k p_{ii\alpha}p_{ij\alpha}p_{ik\alpha}\varepsilon_j\varepsilon_k\\
=\sum_i p_{ii\alpha}^3 \varepsilon_i^2 +2\sum_i\sum_{j\neq i}p_{ii\alpha}^2p_{ij\alpha}\varepsilon_i\varepsilon_j + 
\sum_i\sum_{j\neq i}\sum_{k\neq i}p_{ii\alpha}p_{ij\alpha}p_{ik\alpha}\varepsilon_j\varepsilon_k=o_P(1).
\end{align*}
Its first term vanishes as $0\le \sum_i p_{ii\alpha}^3 \varepsilon_i^2 \le \sum_i p_{ii\alpha}^2 \varepsilon_i^2 = o_P(1)$.
For the second term holds
\begin{align*}
P\biggl(\biggl|\sum_{i}&\sum_{j\neq i}p_{ii\alpha}^2p_{ij\alpha}\varepsilon_i\varepsilon_j\biggr|>\delta\biggr)\le\frac{1}{\delta^2}\sum_{i}\sum_{j\neq i}p_{ii\alpha}^4p_{ij\alpha}^2\mathrm{E}\bigl[\varepsilon_i^2\varepsilon_j^2\bigr]
= \frac{\sigma^4}{\delta^2}\sum_{i}p_{ii\alpha}^4\sum_{j\neq i}p_{ij\alpha}^2\\
&\le \frac{\sigma^4}{\delta^2}\sum_{i}p_{ii\alpha}^5 \le \frac{d_\alpha\sigma^4}{\delta^2}\biggl(\max_{1\le i\le n}p_{ii\alpha}\biggr)^4 \to 0.
\end{align*}
Third term
\begin{align*}
P\biggl(\biggl|\sum_i&\sum_{j\neq i}\sum_{k\neq i}p_{ii\alpha}p_{ij\alpha}p_{ik\alpha}\varepsilon_j\varepsilon_k\biggr|>\delta\biggr)\le\frac{1}{\delta^2}\sum_i\sum_{j\neq i}\sum_{k\neq i}p_{ii\alpha}^2p_{ij\alpha}^2p_{ik\alpha}^2\mathrm{E}\bigl[\varepsilon_j^2\varepsilon_k^2\bigr]\\
&\le \frac{\sigma^4}{\delta^2} \sum_i p_{ii\alpha}^4 \le \frac{d_\alpha\sigma^4}{\delta^2}\biggl(\max_{1\le i\le n}p_{ii\alpha}\biggr)^3 \to 0.
\end{align*}
Hence we have shown that $(\ast)$ is 
\begin{align*}
\sum_{i=1}^n p_{ii\alpha}(y_i-x_{i,\alpha}\hat{\beta}_\alpha)^2 = \varepsilon'\mathrm{diag}(p_{ii\alpha})\varepsilon + o_P(1).
\end{align*}

(2.)To show that $\varepsilon'\mathrm{diag}(p_{ii\alpha})\varepsilon=d_\alpha \sigma^2+o_P(1)$ we use the following truncation technique:\\
Let $\delta>0$ and $C_n=O((\max p_{ii\alpha})^{-0.5})$, then 
\begin{align*}
P\biggl(\biggl|\sum_{i=1}^n p_{ii\alpha} (\varepsilon_i^2-\sigma^2)\biggr|>\delta\biggr)&\le
 P\biggl(\biggl|\sum_{i=1}^n p_{ii\alpha}(\varepsilon_i^2-\sigma^2)I(|(\varepsilon_i^2-\sigma^2)|\le C_n)\biggr|>\frac{\delta}{2}\biggr)\\
&+  P\biggl(\biggl|\sum_{i=1}^n p_{ii\alpha}(\varepsilon_i^2-\sigma^2)I(|(\varepsilon_i^2-\sigma^2)|> C_n)\biggr|>\frac{\delta}{2}\biggr).
\end{align*}
By Cebysev's inequality and using that the $\varepsilon_i$ are iid, we can bound the first probability on the right hand side by
\begin{align*}
P\biggl(\biggl|\sum_{i=1}^n &p_{ii\alpha}(\varepsilon_i^2-\sigma^2)I\bigl(|(\varepsilon_i^2-\sigma^2)|\le C_n\bigr)\biggr|>\frac{\delta}{2}\biggr)\le \frac{4}{\delta^2}\sum_{i=1}^n p_{ii\alpha}^2\mathrm{E}\bigl[|\varepsilon_i^2-\sigma^2|^2I\bigl(|(\varepsilon_i^2-\sigma^2)|\le C_n\bigr)\bigr]\\
&\le \frac{4}{\delta^2}\bigl(\max_{1\le i\le n}p_{ii\alpha}\bigr)\sum_{i=1}^n p_{ii\alpha}\mathrm{E}\bigl[|\varepsilon_i^2-\sigma^2|^2I\bigl(|(\varepsilon_i^2-\sigma^2)|\le C_n\bigr)\bigr]\\
&= \frac{4}{\delta^2}\bigl(\max_{1\le i\le n}p_{ii\alpha}\bigr)\underbrace{\mathrm{E}\bigl[|\varepsilon_1^2-\sigma^2|^2I\bigl(|(\varepsilon_1^2-\sigma^2)|\le C_n\bigr)\bigr]}_{\le C_n\mathrm{E}[|\varepsilon_1^2-\sigma^2|]}\sum_{i=1}^n p_{ii\alpha}\\
&\le \frac{4}{\delta^2}\bigl(\max_{1\le i\le n}p_{ii\alpha}\bigr)C_n\mathrm{E}\bigl[|\varepsilon_1^2-\sigma^2|\bigr]d_\alpha\\
&\le \frac{8d_\alpha\sigma^2}{\delta^2}\bigl(\max_{1\le i\le n}p_{ii\alpha}\bigr)C_n = O\bigl(\sqrt{\max_{1\le i\le n}p_{ii\alpha}}\bigr).
\end{align*}
To see that the second probability on the right hand side vanishes, note that $|\varepsilon_1^2-\sigma^2|I\bigl(|(\varepsilon_1^2-\sigma^2)|> C_n\bigr)\to 0$ and $|\varepsilon_1^2-\sigma^2|I\bigl(|(\varepsilon_1^2-\sigma^2)|> C_n\bigr)\le |\varepsilon_1^2-\sigma^2|$. Hence by the dominated convergence theorem it holds that $\mathrm{E}\bigl[|\varepsilon_1^2-\sigma^2|I\bigl(|(\varepsilon_1^2-\sigma^2)|> C_n\bigr)\bigr] \to 0$. Applying the Markov inequality at the second probability yields
\begin{align*}
P\biggl(\biggl|\sum_{i=1}^n &p_{ii\alpha}(\varepsilon_i^2-\sigma^2)I(|(\varepsilon_i^2-\sigma^2)|> C_n)\biggr|>\frac{\delta}{2}\biggr) \le \frac{2}{\delta}\sum_{i=1}^n p_{ii\alpha}\mathrm{E}\bigl[|\varepsilon_i^2-\sigma^2|I\bigl(|(\varepsilon_i^2-\sigma^2)|\le C_n\bigr)\bigr]\\
&\le  \frac{2}{\delta}\mathrm{E}\bigl[|\varepsilon_1^2-\sigma^2|I\bigl(|(\varepsilon_1^2-\sigma^2)|\le C_n\bigr)\bigr]\sum_{i=1}^n p_{ii\alpha} \\
&=  \frac{2d_\alpha}{\delta}\mathrm{E}\bigl[|\varepsilon_1^2-\sigma^2|I\bigl(|(\varepsilon_1^2-\sigma^2)|\le C_n\bigr)\bigr] \to 0
\end{align*}
establishing that $\sum_{i=1}^n p_{ii\alpha}\varepsilon_i^2= d_\alpha \sigma^2 + o_P(1)$.
Combining the results from (1.) and (2.) we obtain $\hat{\Gamma}_{\alpha,n}^{CV} =\frac{1}{n}\varepsilon'\varepsilon- \frac{1}{n}\varepsilon'P_\alpha\varepsilon + \frac{2}{n}d_\alpha\sigma^2 +o_P(n^{-1})$
proving the second claim.$\quad\square$

\textbf{Proof of $(III)$:}
Since $(I)$ and $(II)$ imply that $\hat{\Gamma}_{\alpha,n}^{CV}$ consistently estimates $\Gamma_{\alpha,n}$, it suffices to show that $CV(1)$ asymptotically wouldn't choose any model in Category $I$, if we compare the $\Gamma_{\alpha,n}$. 
Note that for any model in Category $II$ holds
\begin{align*}
\lim_{n\to \infty} \Gamma_{\alpha,n} = \sigma^2 
\end{align*}
and for any model in Category $I$
\begin{align*}
\liminf_{n\to \infty} \Gamma_{\alpha,n} = \sigma^2 + \liminf_{n \to \infty} \Delta_{\alpha,n} >  \sigma^2,
\end{align*}
by condition $(iii)$ of the theorem. Hence, the expected squared prediction error for any model in Category $I$ is larger than the expected squared prediction error for models in category $II$. Or put differently, $CV(1)$ won't choose any model in Category $I$ asymptotically with probability approaching one. $\quad\square$.

\textbf{Proof of $(IV)$:}
Suppose that $\mathcal{M}_\ast$ is not of size $p$, i.e. $d_\ast < p$.
By $(II)$ 
\begin{align*}
\hat{\Gamma}_{\alpha,n}^{CV} = \frac{1}{n}\varepsilon'\varepsilon + \frac{2}{n}d_\alpha\sigma^2 - \frac{1}{n}\varepsilon'P_\alpha\varepsilon + o_P\biggl(\frac{1}{n}\biggr)
\end{align*}
By $(III)$ it suffices to look at models in Category $II$.
Furthermore 
\begin{align*}
&\{ \exists\alpha\in \textrm{Category $II$} \textrm{ s.t. $\mathcal{M}_\alpha$ is preferable to $\mathcal{M}_\ast$ w.r.t. $CV(1)$ at step $n$ }\} \\
= &\{ \exists\alpha\in \textrm{Category $II$}: \hat{\Gamma}_{\alpha,n}^{CV} < \hat{\Gamma}_{\ast,n}^{CV} \} \\
= &\biggl\{ \exists\alpha\in \textrm{Category $II$}: \frac{2}{n}(d_\alpha-d_\ast)\sigma^2 < \frac{1}{n}\varepsilon'(P_\alpha-P_\ast)\varepsilon + o_P\biggl(\frac{1}{n}\biggr) \biggr\}\\
= &\{ \exists\alpha\in \textrm{Category $II$}: 2(d_\alpha-d_\ast)\sigma^2 <\varepsilon'(P_\alpha-P_\ast)\varepsilon + o_P(1) \}.
\end{align*}
And hence, 
\begin{align*}
&P(\mathcal{M}_{CV}\neq \mathcal{M}_\ast) \\
= &P\bigl(\exists\alpha\in \textrm{Category $II$} \textrm{ s.t. $\mathcal{M}_\alpha$ is preferable to $\mathcal{M}_\ast$ w.r.t. $CV(1)$ at step $n$ }\bigr)\\
= &P\bigl(\exists\alpha\in \textrm{Category $II$}: 2(d_\alpha-d_\ast)\sigma^2 <\varepsilon'(P_\alpha-P_\ast)\varepsilon\bigr) + o(1)
\end{align*}

Nebenrechnung:
By definition of $\mathcal{M}_\ast$, we know that $d_\alpha > d_\ast$ for any $\alpha \in $Category $II$
\begin{align*}
P_\alpha-P_\ast = X_\alpha(X_\alpha'X_\alpha)^{-1}X_\alpha' - X_\ast (X_\ast'X_\ast)^{-1} X_\ast'
\end{align*}
this doesn't help me anywhere. 
But maybe a geometric interpretation helps:
$\varepsilon'(P_\alpha-P_\ast) \varepsilon$
could be interpreted as the inner product between $\varepsilon$ and the difference between its projections onto $\alpha$ space  ($P_\alpha\varepsilon$) and $\ast$ space ($P_\ast\varepsilon$).

Intuitively it makes sense that the this still is a projetion operator
\begin{align*}
(P_\alpha-P_\ast) (P_\alpha-P_\ast) &= \underbrace{P_\alpha^2}_{=P_\alpha} -P_\ast P_\alpha - P_\alpha P_\ast + \underbrace{P_\ast^2}_{=P_\ast}\\
P_\ast P_\alpha &= X_\ast (X_\ast'X_\ast)^{-1} X_\ast'X_\alpha(X_\alpha'X_\alpha)^{-1}X_\alpha'\\
&= X_\ast (X_\ast'X_\ast)^{-1} ( X_\ast' X_\ast +  X_\ast'X_{\alpha \cup \ast^c})(X_\alpha'X_\alpha)^{-1}X_\alpha'\\
&= X_\ast (X_\alpha'X_\alpha)^{-1}X_\alpha' + X_\ast (X_\ast'X_\ast)^{-1} X_\ast'X_{\alpha \cup \ast^c}(X_\alpha'X_\alpha)^{-1}X_\alpha'
\end{align*}
but I don't see why it should be idempotent?!

Maybe the difference of two projections is only itself a projection if the difference space is orthogonal to the other space; 
if there is corellation between these two spaces, there might be correlation between ...

$\lim_{n\to\infty}P(\mathcal{M}_{CV}=\mathcal{M}_\ast) \neq 1$
\end{document}