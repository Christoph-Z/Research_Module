\documentclass[Research_Module_ES.tex]{subfiles}
\begin{document}
\subsection{The Leave-one-Out Cross-Validation}
A special case of the \textit{CV($n_\nu$)} is the \textit{Leave-one-Out Cross-Validation}, which might be the most popular version in the literature.
To compute the $CV(1)$ estimate, one deletes one observation of the data set ($n_\nu\equiv1$) and uses the remaining to construct the estimator of the model. Then one uses this model to predict the
deleted observation.

$CV(1)$ is the computationally simplest \textit{Cross-Validation} estimator as only $n$ terms have to be estimated, whereas in other $CV(n_v)$ estimators one has to compute $\binom{n}{n_v}$ terms.\\

We capture the asymptotic properties of $CV(1)$ in the following theorem based on \cite{shao}\footnote{Proof of Theorem \ref{THM_Consistency of $CV(1)$} is given in Appendix \RM{1}}.

\begin{thm}[Asymptotic Properties of $CV(1)$]
Under the conditions 
\label{THM_Consistency of $CV(1)$}
\begin{enumerate}[(i)]
\item $X'X = O(n) \quad \textrm{and} \quad (X'X)^{-1}=O(\frac{1}{n})$
\item $ \lim_{n \to \infty} \max_{1\le i\le n} p_{ii\alpha} =0 \quad \forall \alpha \in \mathcal{A} $ 
\end{enumerate}
the following holds
\begin{enumerate}[(I)]
\item If $\mathcal{M}_\alpha$ is in Category I, then $\hat{\Gamma}_{\alpha,n}^{CV} = \Gamma_{\alpha,n} + o_P(1)$.
\item If $\mathcal{M}_\alpha$ is in Category II, then $\hat{\Gamma}_{\alpha,n}^{CV} = \frac{1}{n}\varepsilon'\varepsilon + \frac{2}{n}d_\alpha\sigma^2 - \frac{1}{n}\varepsilon'P_\alpha\varepsilon + o_P(n^{-1})$.
\end{enumerate}
If furthermore condition (\ref{liminf_condition}) holds,
then 
\begin{enumerate}
\item[(III)] $\lim_{n\to\infty} P(\mathcal{M}_{CV}\textrm{ is in Category I})=0$,
\item[(IV)]If $\mathcal{M}_\ast$ is not of size $p$ and if $E[(\varepsilon^2-\sigma^2)^2]\in\mathbb{R}_{>0}$, then \\\mbox{$\lim_{n\to\infty}P(\mathcal{M}_{CV}=\mathcal{M}_\ast) \neq 1$}
\end{enumerate}
where $\mathcal{M}_{CV}$ denotes the model selected by using $CV(1)$.
\end{thm}

Assertions $(I)$ and $(II)$ of the theorem state that $CV(1)$ consistently estimates the \textit{Average Squared Prediction Error}. To see this in the case of $(II)$, note that $\frac{1}{n}\varepsilon'\varepsilon$ converges almost surely to $\sigma^2$ and that 
$\frac{1}{n}\varepsilon'P_\alpha\varepsilon$ converges in probability to $\frac{1}{n}d_\alpha\sigma^2$.\

However the second statement gives a more precise description of the behavior of the $CV(1)$ estimate for models in Category II. \\

But consistency isn't enough to ensure that $CV(1)$ will detect the true model. 
Indeed, for any model in Category II the \textit{Average Squared Prediction Error} reduces to the variance of the error term and hence the models asymptotically indistinguishable. 
%DAZU vielleicht noch $\Gamma_{\alpha,n}\rightarrow\sigma^2$ angeben, als formale komponente ausgeschrieben?
This idea is made precise by the statement $(IV)$ that there exists some positive probability that the true model isn't chosen. On the other hand \cite{shao} shows in statement $(III)$ that the selected model is with probability approaching to one in Category II.\\

A heuristic explanation for this incorrectness is delivered by statement $(II)$. The model defining terms are $\frac{1}{n}d_\alpha\sigma^2$ and the difference $\frac{1}{n}d_\alpha\sigma^2 - \frac{1}{n}\varepsilon'P_\alpha\varepsilon$. This difference has mean zero and is of the same order of magnitude as the first term. This gives an explanation why $CV(1)$ tends to select too large models.\\

The assumption $(i)$ isn't that restrictive. It bounds the eigenvalues of $\frac{1}{n}X'X$ from above and below and therefore assures the asymptotic invertibility of this matrix. If for example the $x_i$ are iid and if the matrix $\mathrm{E}[x_ix_i']$ is invertible, then assumption $(i)$ is fulfilled. Furthermore we show in the appendix that $(i)$ is a sufficient condition for condition (\ref{liminf_condition})\footnote{See Claim \ref{claim_CV(1)_conditions_2} in Appendix \RM{1}}\\

One can show that $(ii)$ is equivalent to\footnote{See Claim \ref{claim_CV(1)_conditions} in Appendix \RM{1}} 
%HIERZU h√§tte ich gerne einen kurzen nachweis
$\lim \max n^{-1}\lVert x_i\rVert^2=0$. That is, the observations are allowed to grow but not too fast. This condition is for example satisfied if the $x_i$ are bounded.
\end{document}