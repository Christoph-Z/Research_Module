\documentclass[Research_Module_ES.tex]{subfiles}
\begin{document}
\section{The Leave-$n_\nu$-Out Cross-Validation}
%NOCH keine eigenen Worte (aus shao)
In this chapter we describe the \textit{Leave-$n_\nu$-Out Cross-Validation} by \cite{shao}. This Cross-Validation method does model selection by
\begin{align*}
CV(n_\nu)=\min_{\alpha\in\mathcal{A}}\hat{\Gamma}_{\alpha,n}^{CV}
\end{align*}
while $\hat{\Gamma}_{\alpha,n}^{CV}$ is the estimated \textit{Unconditional Expected Squared Prediction Error} of $\Gamma_{\alpha,n-n_\nu}$. It yields an $\alpha_{CV}$ for getting the chosen model $\mathcal{M}_{CV}$ for a given $n_\nu$, which is not necessarily the optimal model $\mathcal{M}_\ast$.\\\\
First thing to do is splitting the given data into two parts, the \textit{set of validation} with $\{(y_i,x_i), i\in s\}$ and a \textit{training set} with $\{(y_i,x_i), i\in s^c\}$, where $s\subset\{1,...,n\}$. We have that $|s|\equiv n_\nu$ (number of datapoints used for assessing the prediction ability) and $|s^c|=n-n_\nu$ (number of datapoints used for
%ANDers gehts glaube ich nicht ohne n_c wieder reinzunehmen (anndere wortwahl)
construction data) such that $n=|s|+|s^c|$. There are in total
$\binom{n}{n_\nu}$ possible ways of splitting the data into subsets $s$ with $n_\nu$ integers in it. That means our set of possible Partitions for a given $n_\nu$ is denoted by $\mathcal{R}^\ast:= \{s\subseteq\{1,\dots,n\}|\# s=n_v\}$ with the above mentioned cardinality $|\mathcal{R}^\ast|=\binom{n}{n_\nu}$.\\

For the construction of the estimator $\hat{\Gamma}_{\alpha,n}^{CV}$, we first have to calculate the \textit{average squared prediction error of split $s$} under $\mathcal{M}_\alpha$. For this,   $\mathcal{M}_\alpha$ uses the construction data $\{(y_i,x_i), i\in s^c\}$. We know the definition of $ASPE(\mathcal{M}_\alpha)$ in (\ref{ASPE}). Thus for split $s$ we have a vector $y_s\in\mathbb{R}^{n_\nu}$ of future values out of validation set and the vector of the fitted value components under $\mathcal{M}_\alpha$, which is $\hat{y}_{\alpha,s^c}$ that uses the construction data. We have to average the squared prediction error by $|s|=n_\nu$ such that we achieve the following Lemma\footnote{Proof of Lemma \ref{Equation3.1} is given in Apendix \RM{1}}.
\begin{lemma}
	\label{Equation3.1}
	The average squared prediction error of split $s$ is given by
	\begin{align*}
		ASPE_{s}(\mathcal{M}_\alpha)&=\frac{1}{n_\nu}\parallel y_s-\hat{y}_{\alpha,s^c}\parallel^2\\
		&=\frac{1}{n_\nu}\parallel (I_{n_\nu}-Q_{\alpha,s})^{-1}(y_s-X_{\alpha,s}\hat{\beta}_\alpha)\parallel^2
	\end{align*}
	with $Q_{\alpha,s}=X_{\alpha,s}(X_\alpha^\prime X_\alpha)^{-1}X_{\alpha,s}^\prime$.
\end{lemma}


Under the usage of Lemma \ref{Equation3.1}, we have to calculate the $ASPE_{s}(\mathcal{M}_\alpha)$ for all subsets $s$  for a given $n_\nu$ in the corresponding set of partitions $\mathcal{R}^\ast$. And the last step is to average this expression by cardinality of $\mathcal{R}^\ast$, such that we achieve the following claim.

\begin{claim}
	\label{estimator CV(n_v)}
	For each model $\mathcal{M}_\alpha$ (and $n_\nu$ given), the estimate $~\hat{\Gamma}_{\alpha,n}^{CV}$ of $~\Gamma_{\alpha,n-n_\nu}$ is received by
	\begin{align*}
		\hat{\Gamma}_{\alpha,n}^{CV}&=|\mathcal{R}^\ast|^{-1}\sum_{s\in \mathcal{R}^\ast}n_\nu^{-1}\parallel y_s-\hat{y}_{\alpha,s^c}\parallel^2\\
		&=|\mathcal{R}^\ast|^{-1}n_\nu^{-1}\sum_{s\in \mathcal{R}^\ast}\parallel (I_{n_\nu}-Q_{\alpha,s})^{-1}(y_s-X_{\alpha,s}\hat{\beta}_\alpha)\parallel^2
	\end{align*}	
	with $Q_{\alpha,s}=X_{\alpha,s}(X_\alpha^\prime X_\alpha)^{-1}X_{\alpha,s}^\prime$.
\end{claim}
In the next subsections we show the different variants of the \textit{Leave-$n_\nu$-Out Cross-Validation} and some asymptotic properties of these.

\subfile{Consistency_of_CV1}	

\subfile{Consistency_of_BICV}

\subfile{Consistency_of_MCCV}

\subfile{Consistency_of_APCV}

\subsection{General Extension of CV($n_\nu$)}
A very helpful tool is the general extended version of the CV($n_\nu$) for different and more complicated types of initial models denoted as before by $\mathcal{M}_\alpha$. The general version under $\mathcal{M}_\alpha$ of this method of \cite{shao} does model selection by 
\begin{align*}
	GECV(n_\nu)=\min_{\alpha\in\mathcal{A}}\hat{\Gamma}_{\alpha,n}^{GECV}
\end{align*}
by the uncomplicate usage of the $GECV$-estimator for $\Gamma_{\alpha,n-n_\nu}$, denoted by
\begin{align*}
	\hat{\Gamma}_{\alpha,n}^{GECV}=\frac{1}{n_\nu b}\sum_{s\in\mathcal{L}}L(y_s,\hat{y}_{\alpha,s^c})
\end{align*} 
with $\mathcal{L}=\mathcal{B}$ for the $BICV(n_\nu)$ method and $\mathcal{L}=\mathcal{R}$ for the $MCCV(n_\nu)$ method. While $L(\cdot,\cdot)$ is the suitable loss function. As before, the data is splitted $b$ times, multiplied with the \textit{average squared prediction error} under $\mathcal{M}_\alpha$ for all splits $s\in\mathcal{L}$.
\end{document}