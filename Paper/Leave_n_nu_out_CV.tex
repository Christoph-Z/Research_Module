\documentclass[Research_Module_ES.tex]{subfiles}
\begin{document}
\section{The Leave-$n_\nu$-Out Cross-Validation}
In this chapter we describe the \textit{Leave-$n_\nu$-Out Cross-Validation} by \cite{shao}. This \textit{Cross-Validation} method does model selection by
\begin{align*}
CV(n_\nu)=\min_{\alpha\in\mathcal{A}}\hat{\Gamma}_{\alpha,n}^{CV}
\end{align*}
while $\hat{\Gamma}_{\alpha,n}^{CV}$ is the estimated \textit{Expected Squared Prediction Error}. It yields an $\alpha_{CV}$ for getting the chosen model $\mathcal{M}_{CV}$ for a given $n_\nu$, which is not necessarily the optimal model $\mathcal{M}_\ast$.\\\\
First thing to do is splitting the given data into two parts, the \textit{set of validation} with $\{(y_i,x_i), i\in s\}$ and a \textit{training set} with $\{(y_i,x_i), i\in s^c\}$, where $s\subset\{1,...,n\}$. We have that $|s|\equiv n_\nu$ (number of datapoints used for assessing the prediction ability) and $|s^c|=n-n_\nu$ (number of datapoints used for
construction data) such that $n=|s|+|s^c|$. There are in total
$\binom{n}{n_\nu}$ possible ways of splitting the data into subsets $s$ with $n_\nu$ integers in it. Our set of possible partitions for a given $n_\nu$ is denoted by $\mathcal{R}^\ast:= \{s\subseteq\{1,\dots,n\}|\# s=n_v\}$ with the above mentioned cardinality $|\mathcal{R}^\ast|=\binom{n}{n_\nu}$.\\

For the construction of the estimator $\hat{\Gamma}_{\alpha,n}^{CV}$, we first have to calculate the \textit{Average Squared Prediction Error of Split $s$} under $\mathcal{M}_\alpha$. For this,   $\mathcal{M}_\alpha$ uses the construction data $\{(y_i,x_i), i\in s^c\}$. We recall the definition of $ASPE(\mathcal{M}_\alpha)$ from (\ref{ASPE}). Thus for split $s$ we have a vector $y_s\in\mathbb{R}^{n_\nu}$ of future values out of validation set. The vector of the fitted value components under $\mathcal{M}_\alpha$ is $\hat{y}_{\alpha,s^c}$ and uses the construction data. We have to average the squared prediction error by $|s|=n_\nu$.\\

We can reformulate the $ASPE$, as follows \footnote{Proof of Lemma \ref{Equation3.1} is given in Apendix A.2}.
\begin{lemma}
	\label{Equation3.1}
	The Average Squared Prediction Error of Split $s$ is given by
	\begin{align}
		ASPE_{s}(\mathcal{M}_\alpha)&=\frac{1}{n_\nu}\parallel y_s-\hat{y}_{\alpha,s^c}\parallel^2\label{DefASPE}\\\nonumber
		&=\frac{1}{n_\nu}\parallel (I_{n_\nu}-Q_{\alpha,s})^{-1}(y_s-X_{\alpha,s}\hat{\beta}_\alpha)\parallel^2
	\end{align}
	with $Q_{\alpha,s}=X_{\alpha,s}(X_\alpha^\prime X_\alpha)^{-1}X_{\alpha,s}^\prime$.
\end{lemma}
Under the usage of (\ref{DefASPE}), we have to calculate the $ASPE_{s}(\mathcal{M}_\alpha)$ for all subsets $s$  for a given $n_\nu$ in the corresponding set of partitions $\mathcal{R}^\ast$. And the last step is to average this expression by the cardinality of $\mathcal{R}^\ast$, such that we achieve the following definition.
\begin{defi}
	\label{estimator CV(n_v)}
	For each model $\mathcal{M}_\alpha$ (and $n_\nu$ given), the estimate $~\hat{\Gamma}_{\alpha,n}^{CV}$ is received by
	\begin{align*}
		\hat{\Gamma}_{\alpha,n}^{CV}&=|\mathcal{R}^\ast|^{-1}\sum_{s\in \mathcal{R}^\ast}n_\nu^{-1}\parallel y_s-\hat{y}_{\alpha,s^c}\parallel^2\\
		&=|\mathcal{R}^\ast|^{-1}n_\nu^{-1}\sum_{s\in \mathcal{R}^\ast}\parallel (I_{n_\nu}-Q_{\alpha,s})^{-1}(y_s-X_{\alpha,s}\hat{\beta}_\alpha)\parallel^2
	\end{align*}	
	with $Q_{\alpha,s}=X_{\alpha,s}(X_\alpha^\prime X_\alpha)^{-1}X_{\alpha,s}^\prime$.
\end{defi}
In the next subsections we show the different variants of the \textit{Leave-$n_\nu$-Out Cross-Validation} and some asymptotic properties of these.

\subfile{Consistency_of_CV1}	

\subfile{Consistency_of_BICV}

\subfile{Consistency_of_MCCV}

\subfile{Consistency_of_APCV}

%\subsection{General Extension of CV($n_\nu$)}
%A very helpful tool is the general extended version of the CV($n_\nu$) for different and more complicated types of initial models denoted as before by $\mathcal{M}_\alpha$. The general version under $\mathcal{M}_\alpha$ of this method of \cite{shao} does model selection by 
%\begin{align*}
%	GECV(n_\nu)=\min_{\alpha\in\mathcal{A}}\hat{\Gamma}_{\alpha,n}^{GECV}
%\end{align*}
%by the uncomplicate usage of the $GECV$-estimator for $\Gamma_{\alpha,n-n_\nu}$, denoted by
%\begin{align*}
%	\hat{\Gamma}_{\alpha,n}^{GECV}=\frac{1}{n_\nu b}\sum_{s\in\mathcal{L}}L(y_s,\hat{y}_{\alpha,s^c})
%\end{align*} 
%with $\mathcal{L}=\mathcal{B}$ for the $BICV(n_\nu)$ method and $\mathcal{L}=\mathcal{R}$ for the $MCCV(n_\nu)$ method. While $L(\cdot,\cdot)$ is the suitable loss function. As before, the data is splitted $b$ times, multiplied with the \textit{average squared prediction error} under $\mathcal{M}_\alpha$ for all splits $s\in\mathcal{L}$.
\end{document}