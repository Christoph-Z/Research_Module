\documentclass[Research_Module_ES.tex]{subfiles}
\begin{document}
\section{The Linear Setting}
%HABE KURZ DIE REIHENFOLGE GEÄNDERT, damit es der reihenfolge aus dem paper entspricht, können es ja später umbasteln

In this section we consider the classical linear regression model which will be modified for the subsequent use of the CV($n_\nu$). Our linear regression model will be defined for each observation $i=1,...,n$ as
\begin{align*}
y_i=x_i^\prime\beta+\varepsilon_i
\end{align*}
with $x_i\in\mathbb{R}^p$ as the vector of covariates, $\beta\in\mathbb{R}^p$ as the true parameter vector and the residual $\varepsilon_i$ for the obseravtion $i$.
%WEIß nicht ob man das so sagen kann:
W.l.o.g. we assume that the residuals are independent and identically distributed and $E[\varepsilon_i|x_i]=0$ such as $E[\varepsilon_i^2|x_i]=\sigma^2$. Now we can write the linear regression model in matrix notation as
\begin{align*}
	y=X\beta+\varepsilon
\end{align*}
with $X=[x_1,...,x_n]^\prime\in\mathbb{R}^{n\times p}$ as our adequate matrix, $y\in\mathbb{R}^{n}$ and $\varepsilon\in\mathbb{R}^{n}$.
\\\\
For the application of CV($n_\nu$) we have to describe the so called \textit{Compact model} of \cite{shao}, by using nearly the same notation. The idea behind this model is, that some $\beta_i's$ of the given model might be $0$ and we use a set $\alpha$ consisting of some integers from $1,...,p$ that indexed some of the $\beta_i's$ to get a more compact version of our initial linear regression model. A more detailed explanation of the indexing is given in chapter \ref{chapter_compact_model}. Therefore let 
$\mathcal{A}=\{\alpha|\alpha\subseteq\{1,...,p\},\alpha\neq\emptyset\}$ be a collection of all indexsets $\alpha$, with 
$|\alpha|\equiv d_\alpha\leq p$. Where $d_\alpha$ is the number of predictors in the compact model. Thus for any $\alpha\in\mathcal{A}$ and observation $i=1,...,n$ it is given by
\begin{align*}
	y_i=x_{i,\alpha}^\prime\beta_\alpha+\varepsilon_i
\end{align*}
which will be more easily denoted for observation $i$ as
\begin{align*}
	\mathcal{M}_\alpha=\{y_i=x_{i,\alpha}^\prime\beta_\alpha+\varepsilon_i|\alpha\in\mathcal{A}\}
\end{align*}
with $dim(\mathcal{M}_\alpha)=d_\alpha$. Because $x_{i,\alpha}\in\mathbb{R}^{d_\alpha}$ and $\beta_\alpha\in\mathbb{R}^{d_\alpha}$ are both vectors which include all the components of the initial model which where indexed by $\alpha$. Thus there are $2^p-1$ different $\alpha's$ possible and therefore $2^p-1$ different forms of $\mathcal{M}_\alpha$ for one observation $i$.

\subsection{Compact Model Categories} \label{chapter_compact_model}
%KANN man das so machen?/Keine eigenen worte, alles aus shao
For the following model selection problem, we have to introduce two possible types of $\mathcal{M}_\alpha$.We define
\begin{align*}
\begin{tabular}{ll}
Category~\RM{1}:& At least one $\beta_i\neq 0$ is \textit{not} in $\beta_\alpha$\\
Category~\RM{2}:& $\beta_\alpha$ contains \textit{all} $\beta_i\neq 0$
\end{tabular}
\end{align*}
Where Category~\RM{1} contains all the incorrect models and Category~\RM{2} contains all models which may be inefficient. If the components of $\beta$ in the initial model are acquainted, then $\mathcal{M}_\alpha$ can be assigned to one of these two categories.\\\\
%VERBESSERUNG machen 
The optimal model will be
\begin{align*}
\mathcal{M}_\star=\{\mathcal{M}_{\alpha_\star}|\mathcal{M}_{\alpha_\star}\in \text{Cat.~\RM{2}~and~} dim(\mathcal{M}_{\alpha_\star})<dim(\mathcal{M}_{\alpha})\text{~for~any~}\mathcal{M}_{\alpha}\in \text{Cat.~\RM{2}} \}
\end{align*}

\subsection{Ordinary Least Squares Estimator under $\mathcal{M}_\alpha$}
%KEINE eigenen Worte (alles aus shao)
For the following chapters should we mention briefly the ordinary least squares estimator under $\mathcal{M}_\alpha$. Recall that our compact model for any $\alpha\in\mathcal{A}$ for observation $i=1,...,n$ was
\begin{align*}
	y_i=x_{i,\alpha}^\prime\beta_\alpha+\varepsilon_i
\end{align*}
Written in matrix notation we get that
\begin{align}
	y=X_\alpha \beta_\alpha+\varepsilon  \label{CM_model_matrix}
\end{align}
Where $\beta_\alpha\in\mathbb{R}^{d_\alpha}$ is the true parameter vector indexed by $\alpha$. For the suitable matrix we have $X_\alpha=[x_{1,\alpha},...,x_{n,\alpha}]^\prime \in\mathbb{R}^{n\times d_\alpha}$ with the assumption
%HIER vielleicht aho zitieren
of full rank, furthermore $y=(y_1,...,y_n)^\prime\in\mathbb{R}^n$ and $\varepsilon=(\varepsilon_1,...,\varepsilon_n)^\prime\in\mathbb{R}^n$. Thus the unique %STimmte das? 
ordinary least squares estimator under $\mathcal{M}_\alpha$ is given by 
\begin{align}
	\hat{\beta}_\alpha=(X_\alpha^\prime X_\alpha)^{-1}X_\alpha^\prime y
	\label{OLS}
\end{align}
If $\alpha=\{1,...,p\}$, then we have $d_\alpha=|\alpha|=p$ exactly when $X_\alpha=X$. But we have to be careful. It does not hold for $X\beta=X_\alpha\beta_\alpha$ that it follows that $d_\alpha=p$.



\begin{proof}[Proof of (\ref{OLS})]~\\
	The minimization problem of a least squares estimator is given by
	\begin{align*}
	\min_{b_\alpha}(y-X_\alpha b_\alpha)^\prime(y-X_\alpha b_\alpha)
	\end{align*}
	By using the subsequent rules for matrix diferentation:\\\\
	\textit{For vectors $a,\beta$ with adequate dimensions and a symmetric matrix A with adequate dimension, it holds:}
	\begin{align*}
	1)&~ \frac{\partial}{\partial \beta}(\beta^\prime a)=a\\
	2)&~\frac{\partial}{\partial \beta}(\beta^\prime A\beta)=2A\beta
	\end{align*}
	The first order conditions yields
	\begin{align*}
		& \frac{\partial}{\partial b_\alpha}(y-X_\alpha b_\alpha)^\prime(y-X_\alpha b_\alpha)\stackrel{!}{=}0\\
	\Leftrightarrow & \frac{\partial}{\partial b_\alpha}(y^\prime y-b_\alpha^\prime X_\alpha^\prime y-y^\prime X_\alpha b_\alpha+b_\alpha^\prime X_\alpha^\prime X_\alpha b_\alpha)=0\\
	\Leftrightarrow & \frac{\partial}{\partial b_\alpha}(y^\prime y-2b_\alpha^\prime X_\alpha^\prime y+b_\alpha^\prime X_\alpha^\prime X_\alpha b_\alpha)=0\\
	\Leftrightarrow & -2X_\alpha^\prime y +2 X_\alpha^\prime X_\alpha b_\alpha=0\\
	\Leftrightarrow & X_\alpha^\prime X_\alpha b_\alpha=X_\alpha^\prime y
	\end{align*}
	And under the assumption that $rank(X_\alpha)=rank(X_\alpha^\prime X_\alpha)=d_\alpha$ is full, we know that $X_\alpha^\prime X_\alpha$ is invertible, thus 
	\begin{align*}
		 \hat{\beta}_\alpha=(X_\alpha^\prime X_\alpha)^{-1}X_\alpha^\prime y
	\end{align*}
\end{proof}



\subsection{Prediction Error (anderer Titel?)}
%EINBASTELN (aus shao)
The \textit{Average Squared Prediction Error} under $\mathcal{M}_\alpha$ is given by
\begin{align*}
ASPE(\mathcal{M}_\alpha)=\frac{1}{n}\sum_{i=1}^{n}(z_i-x_{i,\alpha}^\prime\hat{\beta}_\alpha)^2
\end{align*}
The \textit{Conditional Expected Squared Prediction Error} under $\mathcal{M}_\alpha$ is given by
\begin{align*}
CESPE(\mathcal{M}_\alpha)=\sigma^2+\frac{1}{n}\sum_{i=1}^{n}(x_i^\prime\beta-x_{i,\alpha}^\prime\hat{\beta}_\alpha)^2
\end{align*}

%PROOF evtl. in den Anhang + PRÜFEN
\begin{proof}[Proof of Conditional Expected Squared Prediction Error] ~\\Under the use of linearity of expectation and the fact that $\hat{\beta}_\alpha$ is conditioned on $X,Y$ we get 
	\begin{align*}
	\begin{tabular}{ll}
	$CESPE(\mathcal{M}_\alpha)$&$=E[ASPE(\mathcal{M}_\alpha)|X,Y]$\\
	&$=E[\frac{1}{n}(Y-X_\alpha\hat{\beta}_\alpha)^\prime(Y-X_\alpha\hat{\beta}_\alpha)|X,Y]$\\
	&$=E[\frac{1}{n}(X\beta+\varepsilon-X_\alpha\hat{\beta}_\alpha)^\prime(X\beta+\varepsilon-X_\alpha\hat{\beta}_\alpha)|X,Y]$\\
	&$=E[\frac{1}{n}(\beta^\prime X^\prime+\varepsilon^\prime-\hat{\beta}_\alpha^\prime X_\alpha^\prime)(X\beta+\varepsilon-X_\alpha\hat{\beta}_\alpha)|X,Y]$\\
	&$=E[\frac{1}{n}(\beta^\prime X^\prime X\beta+\beta^\prime X^\prime\varepsilon-\beta^\prime X^\prime X_\alpha\hat{\beta}_\alpha+\varepsilon^\prime X\beta+\varepsilon^\prime\varepsilon$\\&$~~~-\varepsilon^\prime X_\alpha\hat{\beta}_\alpha-\hat{\beta}_\alpha^\prime X_\alpha^\prime X\beta-\hat{\beta}_\alpha^\prime X_\alpha^\prime\varepsilon+\hat{\beta}_\alpha^\prime X_\alpha^\prime X_\alpha\hat{\beta}_\alpha)|X,Y]$\\
	&$=\frac{1}{n}\beta^\prime X^\prime X\beta+\frac{1}{n}\beta^\prime X^\prime\cdot E[\varepsilon|X,Y]-\frac{1}{n}\beta^\prime X^\prime X_\alpha\hat{\beta}_\alpha$\\
	&$~~~+\frac{1}{n}E[\varepsilon^\prime|X,Y]\cdot X\beta+\frac{1}{n}E[\varepsilon^\prime\varepsilon|X,Y]-\frac{1}{n}E[\varepsilon^\prime|X,Y]X_\alpha\hat{\beta}_\alpha$\\
	&$~~~-\frac{1}{n}\hat{\beta}_\alpha^\prime X_\alpha^\prime X\beta-\frac{1}{n}\hat{\beta}_\alpha^\prime X_\alpha^\prime\cdot E[\varepsilon|X,Y]+\frac{1}{n}\hat{\beta}_\alpha^\prime X_\alpha^\prime X_\alpha\hat{\beta}_\alpha$\\
	&$=\frac{1}{n}\parallel X\beta\parallel^2+0-\frac{1}{n} (X\beta)^\prime X_\alpha\hat{\beta}_\alpha+0+\frac{1}{n} n\sigma^2-0-\frac{1}{n}(X_\alpha\hat{\beta}_\alpha)^\prime X\beta$\\
	&$~~~-0+\frac{1}{n}\parallel X_\alpha\hat{\beta}_\alpha\parallel^2$\\
	&$=\sigma^2+\frac{1}{n}\sum_{i=1}^{n}[(x_i^\prime\beta)^2-2(x_i^\prime\beta)(x_{i,\alpha}^\prime\hat{\beta}_\alpha)+(x_{i,\alpha}^\prime\hat{\beta}_\alpha)^2]$\\
	&$=\sigma^2+\frac{1}{n}\sum_{i=1}^{n}(x_i^\prime\beta-x_{i,\alpha}^\prime\hat{\beta}_\alpha)^2$
	\end{tabular}
	\end{align*}
\end{proof}

The overall \textit{Unconditional Expected Squared Prediction Error} (unconditional on Y) is given by 
\begin{align*}
\Gamma_{\alpha,n}=\sigma^2+n^{-1}d_\alpha\sigma^2+\vartriangle_{\alpha,n}
\end{align*}
with
\begin{align*}
\begin{tabular}{ll}
$\vartriangle_{\alpha,n}$&$=n^{-1}(X\beta)^\prime(I_n-P_\alpha)X\beta$\\
$P_\alpha$&$=X_\alpha(X_\alpha^\prime X_\alpha)^{-1}X_\alpha^\prime$
\end{tabular}
\end{align*}
%EVTL. in den Anhang+PRÜFEN (Mein proof ,den hat christoph auch gemacht)

\begin{proof}[Proof of Unconditional Expected Squared Prediction Error]~\\
	Under the usage of the 'Law of iterated expectations' and the following useful transformation of
	%AUS Skript vom Liebl
	\begin{align*}
	\begin{tabular}{ll}
	$E[\varepsilon^\prime P_\alpha^\prime P_\alpha\varepsilon|X]$&$=E[\varepsilon^\prime P_\alpha\varepsilon|X]$\\
	&$=\sum_{i=1}^{n}\sum_{j=1}^{n}p_{ij,\alpha}\cdot E[\varepsilon_i\varepsilon_j|X]$\\
	&$=\sum_{i=1}^{n}p_{ii,\alpha}\cdot E[\varepsilon_i^2|X]$\\
	&$=\sum_{i=1}^{n}p_{ii,\alpha}\cdot \sigma^2$\\
	&$=\sigma^2\cdot tr(P_\alpha)$\\
	&$=\sigma^2\cdot d_\alpha$
	\end{tabular}
	\end{align*}
	while respecting, that $P_\alpha$ is a symmetric and idempotent projection matrix with diagonal elements $p_{ii,\alpha}$. Also recall that for two matrices $A$ and $B$ with the same dimension, we are allowed to use $\parallel A-B\parallel_2^2=\parallel A\parallel_2^2-2AB+\parallel B\parallel_2^2$. And at least notice, that the orthogonal projection matrix $(I_n-P_\alpha)\in\mathbb{R}^{n\times n}$ is also symmetric and idempotent.\\\\
	Thus now we can show, that
	\begin{align*}
	\begin{tabular}{ll}
	$\Gamma_{\alpha,n}$&$=E[ASPE(\mathcal{M}_\alpha)|X]$\\
	&$=E[E[ASPE(\mathcal{M}_\alpha)|X,Y]|X]$\\
	&$=E[\sigma^2+\frac{1}{n}\sum_{i=1}^{n}(x_i^\prime\beta-x_{i,\alpha}^\prime\hat{\beta}_\alpha)^2|X]$\\
	&$=\sigma^2+\frac{1}{n}E[\parallel X\beta-X_\alpha\hat{\beta}_\alpha\parallel_2^2|X]$\\
	&$=\sigma^2+\frac{1}{n}E[\parallel X\beta-P_\alpha Y\parallel_2^2|X]$\\
	&$=\sigma^2+\frac{1}{n}E[\parallel X\beta-P_\alpha X\beta-P_\alpha\varepsilon\parallel_2^2|X]$\\
	&$=\sigma^2+\frac{1}{n}E[\parallel (I_n-P_\alpha)X\beta-P_\alpha\varepsilon\parallel_2^2|X]$\\
	&$=\sigma^2+\frac{1}{n}E[\parallel (I_n-P_\alpha)X\beta\parallel_2^2-2(I_n-P_\alpha)X\beta P_\alpha\varepsilon+\parallel P_\alpha\varepsilon\parallel_2^2|X]$\\
	&$=\sigma^2+\frac{1}{n}E[((I_n-P_\alpha)X\beta)^\prime((I_n-P_\alpha)X\beta)|X]-\frac{1}{n}2(I_n-P_\alpha)X\beta P_\alpha E[\varepsilon|X]$\\
	&$~~~+\frac{1}{n}E[(P_\alpha\varepsilon)^\prime (P_\alpha\varepsilon)|X]$\\
	&$=\sigma^2+\frac{1}{n}\beta^\prime X^\prime (I_n-P_\alpha)X\beta+\frac{1}{n}E[\varepsilon^\prime P_\alpha^\prime P_\alpha\varepsilon|X]$\\
	&$=\sigma^2+\vartriangle_{\alpha,n}+\frac{1}{n}\cdot\sigma^2\cdot tr(P_\alpha)$\\
	&$=\sigma^2+n^{-1}\sigma^2d_\alpha+\vartriangle_{\alpha,n}$
	\end{tabular}
	\end{align*}
\end{proof}	

%HIER Christophs Lemma + Beweis mal reingeschoben
%BEWEIS muss ausgebessert werden
\begin{lemma}
	\label{Equation2.3-2.4}
	It holds true that:
	\begin{align*}
		&&\Delta_{\alpha,n}>0 && \text{,for } \mathcal{M}_\alpha\in\text{Category I}&&\\
		&&\Delta_{\alpha,n}=0 && \text{,for } \mathcal{M}_\alpha\in\text{Category II}&&
	\end{align*}
\end{lemma}
\begin{proof}[Proof Lemma \ref{Equation2.3-2.4}]~\\
	For the first part, assume $\mathcal{M}_\alpha\in$ Category I and note that we can rewrite $n\Delta_{\alpha,n}$ as follows
	\begin{align}\nonumber
		n\Delta_{\alpha,n}&= \left(X \beta \right)^\prime \left(I_n-P_\alpha\right)X\beta\\\nonumber
		&= \left(X \beta \right)^\prime \left(I_n-P_\alpha\right)^\prime\left(I_n-P_\alpha\right)X\beta\\\nonumber
		&=||\left(I_n-P_\alpha\right)X\beta||^2\\\nonumber
		&=||\left(I_n-P_\alpha\right)(X_\alpha\beta_\alpha+X_{\alpha^c}\beta_{\alpha^c})||^2\\
		&=||\left(I_n-P_\alpha\right)X_{\alpha^c}\beta_{\alpha^c}||^2>0 \label{larger0}
	\end{align}
	Hence 
	%\[
	%(\mathcal{M}_\alpha\in \text{ Cat I } \wedge \text{ r}(X)=p) \Rightarrow(\exists\beta_i\in \alpha^c:\beta_i\neq0 \wedge X_{\alpha^c}\text{ r}(X_{\alpha^c})=p-d_\alpha)\Rightarrow X_{\alpha^c}\beta_{\alpha^c}\neq0
	%\]
	$\mathcal{M}_\alpha\in$ Category I, it must exist at least one none zero component in $\beta_{\alpha^c}$. Together with $X$ having full rank, we can conclude that $X_{\alpha^c}\beta_{\alpha^c}$ is always unequal to zero.\\ 
	Note further, that $(I_n-P_\alpha)$ is the Projection matrix onto the orthogonal complement of span$\{x_{\alpha,1},\ldots,x_{\alpha,d_\alpha}\}$. Since
	\begin{align*}
		X_{\alpha^c}\beta_{\alpha^c}&\neq 0\\
		X_{\alpha^c}\beta_{\alpha^c}\not\perp (&I_n-P_\alpha)
	\end{align*}
	it holds that $(I_n-P_\alpha) X_{\alpha^c}\beta_{\alpha^c}\neq 0$ and therefore (\ref{larger0}) remains true.\\
	\\
	For the second part, assume $\mathcal{M}_\alpha\in$ Category II, then $X\beta=X_\alpha\beta_\alpha$. Thus
	\begin{align*}
		n\Delta_\alpha,n&=\beta^\prime X^\prime X\beta-\beta^\prime X^\prime X_\alpha \left(X_\alpha^\prime X_\alpha\right)^{-1}X_\alpha^\prime X\beta\\
		&=\beta_\alpha^\prime X_\alpha^\prime X_\alpha \beta_\alpha -\beta_\alpha^\prime X_\alpha^\prime X_\alpha\left(X_\alpha^\prime X_\alpha\right)^{-1} X_\alpha^\prime X_\alpha\beta_\alpha\\
		&=\beta_\alpha^\prime X_\alpha^\prime X_\alpha \beta_\alpha - \beta_\alpha^\prime X_\alpha^\prime X_\alpha \beta_\alpha\\
		&=0
	\end{align*}
	
\end{proof}

%Lemma 2.5
\begin{lemma}
	\label{Equation2.5}
	Under the assumption that $p$ doesn't change for any $n$ (while $n$ is large) and the following condition:
	\begin{align}
		\liminf_{n\rightarrow\infty}\Delta_{\alpha,n}>0 \text{~~~for~}\mathcal{M}_\alpha\in\text{Category}~\RM{1} \label{liminf_condition}
	\end{align}
	Then for any $\mathcal{M}_\alpha\in\text{Category}~\RM{1}$ and $\mathcal{M}_\gamma\in\text{Category}~\RM{2}$ with $d_\alpha=d_\gamma$ it holds that:
	\begin{align*}
		\liminf_{n\rightarrow\infty}\frac{\Gamma_{\alpha,n}}{\Gamma_{\gamma,n}}>1~~~\Leftrightarrow~~~\liminf_{n\rightarrow\infty}\Delta_{\alpha,n}>0
	\end{align*}
\end{lemma}

\begin{proof}[Proof of Lemma \ref{Equation2.5}]~\\
	Let $\mathcal{M}_\alpha\in\text{Category}~\RM{1}$ and $\mathcal{M}_\alpha\in\text{Category}~\RM{2}$ with $d_\alpha=d_\gamma$. It follows for the \textit{unconditional expected squared prediction errors} with Lemma \ref{Equation2.3-2.4}:
	\begin{align*}
	\begin{tabular}{ll}
	$\Gamma_{\alpha,n}$&$=\sigma^2+n^{-1}d_\alpha\sigma^2+\Delta_{\alpha,n}~~~~~$ with $\Delta_{\alpha,n}>0$\\
	$\Gamma_{\gamma,n}$&$=\sigma^2+n^{-1}d_\gamma\sigma^2~~~~~~~~~~~~~~~~$with $\Delta_{\gamma,n}=0$
	\end{tabular}
	\end{align*}
	Then:
	\begin{align*}
	\liminf_{n\rightarrow\infty}\frac{\Gamma_{\alpha,n}}{\Gamma_{\gamma,n}}&=\liminf_{n\rightarrow\infty}\frac{\sigma^2+n^{-1}d_\alpha\sigma^2+\Delta_{\alpha,n}}{\sigma^2+n^{-1}d_\gamma\sigma^2}	\\
	&=\liminf_{n\rightarrow\infty}\Big[\frac{\sigma^2+n^{-1}d_\alpha\sigma^2}{\sigma^2+n^{-1}d_\alpha\sigma^2}+\frac{\Delta_{\alpha,n}}{\sigma^2+n^{-1}d_\alpha\sigma^2}\Big]\\
	&=\liminf_{n\rightarrow\infty}\Big[1+\frac{\Delta_{\alpha,n}}{\sigma^2+n^{-1}d_\alpha\sigma^2}\Big]\\
	&=1+\liminf_{n\rightarrow\infty}\frac{\Delta_{\alpha,n}}{\sigma^2+n^{-1}d_\alpha\sigma^2}\\
	&=1+\frac{\liminf_{n\rightarrow\infty}\Delta_{\alpha,n}}{\sigma^2+ \liminf_{n\rightarrow\infty} n^{-1}d_\alpha\sigma^2}\\
	&=1+\frac{\liminf_{n\rightarrow\infty}\Delta_{\alpha,n}}{\sigma^2}\\
	&>1
	\end{align*}
	Exactly when condition (\ref{liminf_condition}) for $\mathcal{M}_\alpha\in\text{Category}~\RM{1}$ holds.
\end{proof}

\begin{lemma}
	\label{Equation2.5 condition does not hold}
	Under the assumption that $p$ doesn't change for any $n$ (while $n$ is large) and $\mathcal{M}_\alpha\in\text{Category}~\RM{1}$ and $\mathcal{M}_\alpha\in\text{Category}~\RM{2}$ with $d_\alpha=d_\gamma$. If
	\begin{align*}
		\lim_{n\rightarrow\infty}\frac{\Gamma_{\alpha,n}}{\Gamma_{\gamma,n}}=1~~~\Rightarrow~~~\mathcal{M}_\alpha=\mathcal{M}_\gamma
	\end{align*} 
\end{lemma}

\begin{proof}[Proof of Lemma \ref{Equation2.5 condition does not hold}]~\\
	Let $\mathcal{M}_\alpha\in\text{Category}~\RM{1}$ and $\mathcal{M}_\alpha\in\text{Category}~\RM{2}$ with $d_\alpha=d_\gamma$. Then 
	\begin{align*}
		\lim_{n\rightarrow\infty}\frac{\Gamma_{\alpha,n}}{\Gamma_{\gamma,n}}&=\liminf_{n\rightarrow\infty}\frac{\Gamma_{\alpha,n}}{\Gamma_{\gamma,n}}\\
		&=1+\frac{\liminf_{n\rightarrow\infty}\Delta_{\alpha,n}}{\sigma^2}\\
		&\stackrel{!}{=}1
	\end{align*}
	Exactly when
	\begin{align*}
		\liminf_{n\rightarrow\infty}\Delta_{\alpha,n}=0
	\end{align*}
	Thus condition (\ref{liminf_condition}) does not hold and it follows that
	\begin{align*}
		\mathcal{M}_\alpha=\mathcal{M}_\gamma
	\end{align*}
\end{proof}

Considering condition (\ref{liminf_condition}) from an interpretational point of view we can say that
this is a necessary condition that $CV(n_\nu)$ works correctly. Otherwise if the condition does not hold, the above described case in Lemma \ref{Equation2.5 condition does not hold} with $\mathcal{M}_\alpha=\mathcal{M}_\gamma$ is possible. That means, the $CV(n_\nu)$ can't distinguish between $Category~\RM{1}$ and $Category~\RM{2}$ if condition (\ref{liminf_condition}) does not hold and therefore the usage of this method is useless.



\end{document}
	
