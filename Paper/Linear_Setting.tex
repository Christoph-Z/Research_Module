\documentclass[Research_Module_ES.tex]{subfiles}
\begin{document}
\section{The Linear Setting}
%HABE KURZ DIE REIHENFOLGE GEÄNDERT, damit es der reihenfolge aus dem paper entspricht, können es ja später umbasteln
In this section we consider the classical linear regression model which will be modified for the subsequent use of the CV($n_\nu$). For our following chapters and proofs we assume that everything is conditioned on $X$, however, for the sake of simplicity we refrain from explicitly mentioning this. Our linear regression model will be defined for each observation $i=1,...,n$ as
\begin{align*}
y_i=x_i^\prime\beta+\varepsilon_i
\end{align*}
with $x_i\in\mathbb{R}^p$ as the vector of covariates which are deterministic, the assumption that $p$ is fixed, $\beta\in\mathbb{R}^p$ as the true parameter vector and the residual $\varepsilon_i$ for the obseravtion $i$.
We assume that the residuals are independent and identically distributed and $E[\varepsilon_i]=0$ such as $E[\varepsilon_i^2]=\sigma^2<\infty$. Now we can write the linear regression model in matrix notation as
\begin{align*}
	y=X\beta+\varepsilon
\end{align*}
with $X=[x_1,...,x_n]^\prime\in\mathbb{R}^{n\times p}$ as our matrix with the assumption of full rank, $y\in\mathbb{R}^{n}$ and $\varepsilon\in\mathbb{R}^{n}$.
\\\\
For the application of CV($n_\nu$) we have to describe another model which we call \textit{Compact model} referring to \cite{shao}, by using nearly the same notation. The idea behind this model is, that some $\beta_i's$ of the given model might be $0$ and we use a set $\alpha$ consisting of some integers from $1,...,p$ that indexed some of the $\beta_i's$ to get a submodel of our initial linear regression model. A more detailed explanation of the indexing is given in chapter \ref{chapter_compact_model}. Therefore let 
$\mathcal{A}=\{\alpha|\alpha\subseteq\{1,...,p\},\alpha\neq\emptyset\}$ be a collection of all indexsets $\alpha$, with 
$|\alpha|\equiv d_\alpha\leq p$. Where $d_\alpha$ is the number of predictors in the compact model. Thus for any $\alpha\in\mathcal{A}$ and observation $i=1,...,n$ it is given by
\begin{align*}
	y_i=x_{i,\alpha}^\prime\beta_\alpha+\varepsilon_i
\end{align*}
which will be more easily denoted for observation $i$ as $\mathcal{M}_\alpha$. It has  $dim(\mathcal{M}_\alpha)=d_\alpha$, because $x_{i,\alpha}\in\mathbb{R}^{d_\alpha}$ and $\beta_\alpha\in\mathbb{R}^{d_\alpha}$ are both vectors which include all the components of the initial model which where indexed by $\alpha$.\footnote{There are $2^p-1$ different $\alpha's$ possible and therefore $2^p-1$ different forms of $\mathcal{M}_\alpha$ for one observation $i$. The $2^p-1$ is received by $2^p$ elements in the powerset of $\alpha$ minus 1 by leaving out the emptyset.}

\subsection{Compact Model Categories} \label{chapter_compact_model}
%KANN man das so machen?/Keine eigenen worte, alles aus shao
For the following model selection problem, we have to introduce two possible types of $\mathcal{M}_\alpha$.We define
\begin{align*}
\begin{tabular}{ll}
Category~\RM{1}:& At least one $\beta_i\neq 0$ is \textit{not} in $\beta_\alpha$\\
Category~\RM{2}:& $\beta_\alpha$ contains \textit{all} $\beta_i\neq 0$
\end{tabular}
\end{align*}
Where Category~\RM{1} contains all the wrong models and Category~\RM{2} contains all models which may be too large. If the components of $\beta$ in the initial model are known, then $\mathcal{M}_\alpha$ can be assigned to one of these two categories.\\\\
One short example of how indexing works, might be for an $\alpha$ that consists for example of the positive integers $\{1,3,4,6\}$ such that it yields $\beta_\alpha=(\beta_1,\beta_3,\beta_4,\beta_6)^\prime$ and so on. This gives an illustration of what is meant by ``under model $\mathcal{M}_\alpha$''.\\\\
The optimal model will be
\begin{align*}
\mathcal{M}_\ast=\{\mathcal{M}_{\alpha_\ast}|\mathcal{M}_{\alpha_\ast}\in \text{Cat.~\RM{2}~and~} dim(\mathcal{M}_{\alpha_\ast})\leq dim(\mathcal{M}_{\alpha})\text{~for~any~}\mathcal{M}_{\alpha}\in \text{Cat.~\RM{2}} \}
\end{align*}

\subsection{Ordinary Least Squares Estimator under $\mathcal{M}_\alpha$}
%KEINE eigenen Worte (alles aus shao)
For the following chapters should we mention briefly the ordinary least squares estimator under $\mathcal{M}_\alpha$. Recall that our compact model for any $\alpha\in\mathcal{A}$ for observation $i=1,...,n$ was
\begin{align*}
	y_i=x_{i,\alpha}^\prime\beta_\alpha+\varepsilon_i
\end{align*}
Written in matrix notation we get that
\begin{align}
	y=X_\alpha \beta_\alpha+\varepsilon  \label{CM_model_matrix}
\end{align}
Where $\beta_\alpha\in\mathbb{R}^{d_\alpha}$ is the true parameter vector indexed by $\alpha$. For the suitable matrix we have $X_\alpha=[x_{1,\alpha},...,x_{n,\alpha}]^\prime \in\mathbb{R}^{n\times d_\alpha}$
%HIER vielleicht aho zitieren
which has full rank, furthermore $y=(y_1,...,y_n)^\prime\in\mathbb{R}^n$ and $\varepsilon=(\varepsilon_1,...,\varepsilon_n)^\prime\in\mathbb{R}^n$. Thus the unique %STimmte das? 
ordinary least squares estimator under $\mathcal{M}_\alpha$ is given by\footnote{Proof of equation (\ref{OLS}) is given in Appendix \RM{1}}
\begin{align}
	\hat{\beta}_\alpha=(X_\alpha^\prime X_\alpha)^{-1}X_\alpha^\prime y
	\label{OLS}
\end{align}
If $\alpha=\{1,...,p\}$, then we have $d_\alpha=|\alpha|=p$ exactly when $X_\alpha=X$. But we have to be careful. It does not hold for $X\beta=X_\alpha\beta_\alpha$ that it follows that $d_\alpha=p$.

\subsection{Prediction Errors}
%EINBASTELN (aus shao)

In this section we give a preview to the different types of prediction errors, this is needed in the next chapter for constructing the estimator of the \textit{Cross-Validation}.\\\\
For a given explanatory variable $x_i$, we denote $z_i$ as the \textit{future value} of the dependent variable as in \cite{shao}. Such that the \textit{Average Squared Prediction Error} under $\mathcal{M}_\alpha$ is given by
\begin{align}
ASPE(\mathcal{M}_\alpha)=\frac{1}{n}\sum_{i=1}^{n}(z_i-x_{i,\alpha}^\prime\hat{\beta}_\alpha)^2 \label{ASPE}
\end{align}
Then we can use the $ASPE(\mathcal{M}_\alpha)$ for calcualting the \textit{Conditional Expected Squared Prediction Error} under $\mathcal{M}_\alpha$, which is given by\footnote{Proof of $CESPE(\mathcal{M}_\alpha)$ is given in Appendix \RM{1}}
\begin{align*}
CESPE(\mathcal{M}_\alpha)=\sigma^2+\frac{1}{n}\sum_{i=1}^{n}(x_i^\prime\beta-x_{i,\alpha}^\prime\hat{\beta}_\alpha)^2
\end{align*}
Thus we easily achive with the help of the formula above in the last step the overall \textit{Unconditional Expected Squared Prediction Error} (unconditional on Y), that is given by\footnote{Proof of $\Gamma_{\alpha,n}$ is given in Appendix \RM{1}} 
\begin{align}
\Gamma_{\alpha,n}=\sigma^2+n^{-1}d_\alpha\sigma^2+\vartriangle_{\alpha,n} 
\end{align}
with
\begin{align*}
\begin{tabular}{ll}
$\vartriangle_{\alpha,n}$&$=n^{-1}(X\beta)^\prime(I_n-P_\alpha)X\beta$\\
$P_\alpha$&$=X_\alpha(X_\alpha^\prime X_\alpha)^{-1}X_\alpha^\prime$
\end{tabular}
\end{align*} 
Where $\vartriangle_{\alpha,n}$ shows the model misspecification which is more precisely given in Lemma \ref{Equation2.3-2.4} and $P_\alpha$ as the corresponding hat matrix under model $\mathcal{M}_\alpha.$ The first expression of $\Gamma_{\alpha,n}$ which is given by $\sigma^2$ is the error term variance and $n^{-1}d_\alpha\sigma^2+\vartriangle_{\alpha,n}$ is a mirror of the errors in estimation and model selection. This second error term vanishes if $n$ goes to infinity and $\vartriangle_{\alpha,n}=0$ by the correct model category. Thus now we introduce the following Lemma \ref{Equation2.3-2.4}\footnote{Proof of Lemma \ref{Equation2.3-2.4} is given in Appendix \RM{1}}
and Lemma \ref{Equation2.5}\footnote{Proof of Lemma \ref{Equation2.5} is given in Appendix \RM{1}}.

\begin{lemma}
	\label{Equation2.3-2.4}
	For any fixed $n$, it holds true that:
	\begin{align*}
		&&\Delta_{\alpha,n}>0 &&\text{,if } \mathcal{M}_\alpha\in\text{Category I}&&\\
		&&\Delta_{\alpha,n}=0 &&\text{,if } \mathcal{M}_\alpha\in\text{Category II}&&
	\end{align*}
\end{lemma}


\begin{lemma}
	\label{Equation2.5}
	For any $\mathcal{M}_\alpha\in\text{Category}~\RM{1}$ and $\mathcal{M}_\gamma\in\text{Category}~\RM{2}$ with $d_\alpha=d_\gamma$ it holds that:
	\begin{align*}
	\liminf_{n\rightarrow\infty}\frac{\Gamma_{\alpha,n}}{\Gamma_{\gamma,n}}>1
	\end{align*}
	If and only if
	\begin{align}
	\liminf_{n\rightarrow\infty}\Delta_{\alpha,n}>0 \text{~~~for~}\mathcal{M}_\alpha\in\text{Category}~\RM{1} \label{liminf_condition}
	\end{align}
\end{lemma}

\textbf{Remark:}
	Under the assumption that $\mathcal{M}_\alpha\in\text{Category}~\RM{1}$ and $\mathcal{M}_\gamma\in\text{Category}~\RM{2}$ with $d_\alpha=d_\gamma$. We have that if
	\begin{align*}
		\lim_{n\rightarrow\infty}\frac{\Gamma_{\alpha,n}}{\Gamma_{\gamma,n}}=1
	\end{align*} 
	condition (\ref{liminf_condition}) does not hold, because
	\begin{align*}
	\liminf_{n\rightarrow\infty}\Delta_{\alpha,n}=0.
	\end{align*}
Considering the condition (\ref{liminf_condition}) from an interpretational point of view we can say that
this is a necessary condition that $CV(n_\nu)$ works correctly. Otherwise if the condition does not hold, the above described case in the Remark is possible. That means, the $CV(n_\nu)$ can't distinguish between $Category~\RM{1}$ and $Category~\RM{2}$ if condition (\ref{liminf_condition}) does not hold and therefore the usage of this method is useless.



\end{document}
	
