\documentclass[Research_Module_ES.tex]{subfiles}
\begin{document}
\section{The Linear Setting}
In this section we consider the classical linear regression model. For our following chapters and proofs we assume that everything is conditioned on $X$, however, for the sake of simplicity we refrain from explicitly mentioning this. Our linear regression model will be defined for each observation $i=1,...,n$ as
\begin{align*}
y_i=x_i^\prime\beta+\varepsilon_i
\end{align*}
with $x_i\in\mathbb{R}^p$ as the vector of covariates, $p$ fixed, $\beta\in\mathbb{R}^p$ as the true parameter vector and the residual $\varepsilon_i$ for the obseravtion $i$.
We assume that the residuals are independent and identically distributed and $E[\varepsilon_i]=0$ such as $E[\varepsilon_i^2]=\sigma^2\in\mathbb{R}_{>0}$. Now we can write the linear regression model in matrix notation as
\begin{align*}
	y=X\beta+\varepsilon
\end{align*}
with $X=[x_1,...,x_n]^\prime\in\mathbb{R}^{n\times p}$ as our matrix with the assumption of full rank, $y\in\mathbb{R}^{n}$ and $\varepsilon\in\mathbb{R}^{n}$.
\\\\
For the application of CV($n_\nu$) we have to describe another model which we call \textit{Compact model} referring to \cite{shao}, by using nearly the same notation. The idea behind this model is that some $\beta_i's$ of the given model might be $0$ and we use a set $\alpha$ consisting of integers from $1,...,p$ that indexes some of the $\beta_i's$ to get a submodel of our initial linear regression model. A more detailed explanation of the indexing is given below in this chapter. Therefore let 
$\mathcal{A}=\{\alpha|\alpha\subseteq\{1,...,p\},\alpha\neq\emptyset\}$ be a collection of all indexsets $\alpha$, with 
$|\alpha|\equiv d_\alpha\leq p$. Where $d_\alpha$ is the number of predictors in the \textit{Compact model}. Thus for any $\alpha\in\mathcal{A}$ and observation $i=1,...,n$ the submodel is given by
\begin{align*}
	y_i=x_{i,\alpha}^\prime\beta_\alpha+\varepsilon_i
\end{align*}
which will be denoted by $\mathcal{M}_\alpha$. It has  $dim(\mathcal{M}_\alpha)=d_\alpha$, because $x_{i,\alpha}\in\mathbb{R}^{d_\alpha}$ and $\beta_\alpha\in\mathbb{R}^{d_\alpha}$ are both vectors which include all the components of the initial model which where indexed by $\alpha$.\footnote{There are $2^p-1$ different $\alpha's$ possible and therefore $2^p-1$ different forms of $\mathcal{M}_\alpha$ for one observation $i$. The $2^p-1$ is received by $2^p$ elements in the powerset of $\alpha$ minus 1 by leaving out the emptyset.}\\\\
One short example of how indexing works might be for an $\alpha$ that consists for example of the positive integers $\{1,3,4,6\}$ such that it yields $\beta_\alpha=(\beta_1,\beta_3,\beta_4,\beta_6)^\prime$ and so on. This gives an illustration of what is meant by ``under model $\mathcal{M}_\alpha$''.

\subsection{Compact Model Categories} \label{chapter_compact_model}
For the following model selection problem, we have to introduce two possible types of $\mathcal{M}_\alpha$.We define
\begin{align*}
\begin{tabular}{ll}
Category~\RM{1}:& At least one $\beta_i\neq 0$ is \textit{not} in $\beta_\alpha$\\
Category~\RM{2}:& $\beta_\alpha$ contains \textit{all} $\beta_i\neq 0$
\end{tabular}
\end{align*}
where Category~\RM{1} contains all wrong models and Category~\RM{2} contains all models which may be too large.\\\\
The optimal model has to be a model out of Category \RM{2} and it will be denoted by
\begin{align*}
\mathcal{M}_\ast=\{\mathcal{M}_{\alpha_\ast}|\mathcal{M}_{\alpha_\ast}\in \text{Cat.~\RM{2}~and~} dim(\mathcal{M}_{\alpha_\ast})\leq dim(\mathcal{M}_{\alpha})\text{~for~any~}\mathcal{M}_{\alpha}\in \text{Cat.~\RM{2}} \}
\end{align*}

\subsection{Ordinary Least Squares Estimator under $\mathcal{M}_\alpha$}
For the following chapters we should briefly mention the ordinary least squares estimator under $\mathcal{M}_\alpha$. Recall that our \textit{Compact model} for any $\alpha\in\mathcal{A}$ for observation $i=1,...,n$ is
\begin{align*}
	y_i=x_{i,\alpha}^\prime\beta_\alpha+\varepsilon_i
\end{align*}
Written in matrix notation we get that
\begin{align}
	y=X_\alpha \beta_\alpha+\varepsilon  \label{CM_model_matrix}
\end{align}
where $\beta_\alpha\in\mathbb{R}^{d_\alpha}$ is the true parameter vector indexed by $\alpha$. For the suitable matrix we have $X_\alpha=[x_{1,\alpha},...,x_{n,\alpha}]^\prime \in\mathbb{R}^{n\times d_\alpha}$
which has full rank, furthermore $y=(y_1,...,y_n)^\prime\in\mathbb{R}^n$ and $\varepsilon=(\varepsilon_1,...,\varepsilon_n)^\prime\in\mathbb{R}^n$. Thus the unique 
ordinary least squares estimator under $\mathcal{M}_\alpha$ is given by\footnote{Proof of equation (\ref{OLS}) is given in Appendix \RM{1}}
\begin{align}
	\hat{\beta}_\alpha=(X_\alpha^\prime X_\alpha)^{-1}X_\alpha^\prime y
	\label{OLS}
\end{align}
If $\alpha=\{1,...,p\}$, then we have $d_\alpha=|\alpha|=p$ and $X_\alpha=X$. But we have to be careful, it does not hold for $X\beta=X_\alpha\beta_\alpha$ that it follows that $d_\alpha=p$.

\subsection{Prediction Errors}
In this section we give a preview to the different types of prediction errors, we will need this in the next chapters for the construction of the \textit{Cross-Validation} estimator.\\\\
For a given explanatory variable $x_i$, we denote $z_i$ as the \textit{future value}\footnote{Independent of $\hat{\beta}_\alpha$} of the dependent variable as in \cite{shao}, such that the \textit{Average Squared Prediction Error} under $\mathcal{M}_\alpha$ is given by
\begin{align}
ASPE(\mathcal{M}_\alpha)=\frac{1}{n}\sum_{i=1}^{n}(z_i-x_{i,\alpha}^\prime\hat{\beta}_\alpha)^2 \label{ASPE}
\end{align}
Then we can use the $ASPE(\mathcal{M}_\alpha)$ for calculating the \textit{Conditional Expected Squared Prediction Error} under $\mathcal{M}_\alpha$, which is given by\footnote{Proof of $CESPE(\mathcal{M}_\alpha)$ is given in Appendix \RM{1}}
\begin{align*}
CESPE(\mathcal{M}_\alpha)=\sigma^2+\frac{1}{n}\sum_{i=1}^{n}(x_i^\prime\beta-x_{i,\alpha}^\prime\hat{\beta}_\alpha)^2
\end{align*}
Thus we achieve, with the help of the formula above, the overall \textit{(Unconditional) Expected Squared Prediction Error}\footnote{Unconditional on Y}, that is given by\footnote{Proof of $\Gamma_{\alpha,n}$ is given in Appendix \RM{1}} 
\begin{align}
\Gamma_{\alpha,n}=\sigma^2+n^{-1}d_\alpha\sigma^2+\vartriangle_{\alpha,n} 
\end{align}
with
\begin{align*}
\begin{tabular}{ll}
$\vartriangle_{\alpha,n}$&$=n^{-1}(X\beta)^\prime(I_n-P_\alpha)X\beta$\\
$P_\alpha$&$=X_\alpha(X_\alpha^\prime X_\alpha)^{-1}X_\alpha^\prime$
\end{tabular}
\end{align*} 
where $P_\alpha$ is the corresponding hat matrix under model $\mathcal{M}_\alpha$.\\

 The first term in $\Gamma_{\alpha,n}$ which is given by $\sigma^2$ is the error term variance. The second term, $n^{-1}d_\alpha\sigma^2$, can be interpreted as the prediction error due to model dimensionality and the last term, $\Delta_{\alpha,n}$, can be interpreted as prediction error due to omitted important regressors. The interpretation of $\Delta_{\alpha,n}$ is supported by Lemma \ref{Equation2.3-2.4}\footnote{Proof of Lemma \ref{Equation2.3-2.4} is given in Appendix \RM{1}}.
\begin{lemma}
	\label{Equation2.3-2.4}
	For any fixed $n$, it holds true that:
	\begin{align*}
		&&\Delta_{\alpha,n}>0 &&\text{,if } \mathcal{M}_\alpha\in\text{Category I}&&\\
		&&\Delta_{\alpha,n}=0 &&\text{,if } \mathcal{M}_\alpha\in\text{Category II}&&
	\end{align*}
\end{lemma}
Since the second term in $\Gamma_{\alpha,n}$ vanishes as $n\to\infty$, $\Gamma_{\alpha,n}$ is asymptotically characterized by $\Delta_{\alpha,n}$, which we show in Lemma \ref{Equation2.5}\footnote{Proof of Lemma \ref{Equation2.5} is given in Appendix \RM{1}}. 
\begin{lemma}
	\label{Equation2.5}
	For any $\mathcal{M}_\alpha\in\text{Category}~\RM{1}$ and $\mathcal{M}_\gamma\in\text{Category}~\RM{2}$ with $d_\alpha=d_\gamma$ it holds that:
	\begin{align*}
	\liminf_{n\rightarrow\infty}\frac{\Gamma_{\alpha,n}}{\Gamma_{\gamma,n}}>1
	\end{align*}
	If and only if
	\begin{align}
	\liminf_{n\rightarrow\infty}\Delta_{\alpha,n}>0 \text{~~~for~}\mathcal{M}_\alpha\in\text{Category}~\RM{1} \label{liminf_condition}
	\end{align}
\end{lemma}

\textbf{Remark:}
	Under the assumption that $\mathcal{M}_\alpha\in\text{Category}~\RM{1}$ and $\mathcal{M}_\gamma\in\text{Category}~\RM{2}$ with $d_\alpha=d_\gamma$. We have that if
	\begin{align*}
		\lim_{n\rightarrow\infty}\frac{\Gamma_{\alpha,n}}{\Gamma_{\gamma,n}}=1
	\end{align*} 
	condition (\ref{liminf_condition}) does not hold, because
	\begin{align*}
	\lim_{n\rightarrow\infty}\Delta_{\alpha,n}=0.
	\end{align*}
Considering the condition (\ref{liminf_condition}) from an interpretational point of view we can say that
this is a necessary condition that $CV(n_\nu)$ works correctly. Otherwise if the condition does not hold, the above described case in the remark is possible. That means, the $CV(n_\nu)$ can't distinguish between $Category~\RM{1}$ and $Category~\RM{2}$ if condition (\ref{liminf_condition}) does not hold and therefore the usage of this method is useless.
\end{document}
	
