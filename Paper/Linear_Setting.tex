\documentclass[Research_Module_ES.tex]{subfiles}
\begin{document}
\section{The Linear Setting}
%HABE KURZ DIE REIHENFOLGE GEÄNDERT, damit es der reihenfolge aus dem paper entspricht, können es ja später umbasteln
In this section we consider the classical linear regression model. For our following chapters and proofs we assume that everything is conditioned on $X$, however, for the sake of simplicity we refrain from explicitly mentioning this. Our linear regression model will be defined for each observation $i=1,...,n$ as
\begin{align*}
y_i=x_i^\prime\beta+\varepsilon_i
\end{align*}
with $x_i\in\mathbb{R}^p$ as the vector of covariates, $p$ fixed, $\beta\in\mathbb{R}^p$ as the true parameter vector and the residual $\varepsilon_i$ for the obseravtion $i$.
We assume that the residuals are independent and identically distributed and $E[\varepsilon_i]=0$ such as $E[\varepsilon_i^2]=\sigma^2\in\mathbb{R}_{>0}$. Now we can write the linear regression model in matrix notation as
\begin{align*}
	y=X\beta+\varepsilon
\end{align*}
with $X=[x_1,...,x_n]^\prime\in\mathbb{R}^{n\times p}$ as our matrix with the assumption of full rank, $y\in\mathbb{R}^{n}$ and $\varepsilon\in\mathbb{R}^{n}$.
\\\\
For the application of CV($n_\nu$) we have to describe another model which we call \textit{Compact model} referring to \cite{shao}, by using nearly the same notation. The idea behind this model is that some $\beta_i's$ of the given model might be $0$ and we use a set $\alpha$ consisting of integers from $1,...,p$ that indexes some of the $\beta_i's$ to get a submodel of our initial linear regression model. A more detailed explanation of the indexing is given below in this chapter. Therefore let 
$\mathcal{A}=\{\alpha|\alpha\subseteq\{1,...,p\},\alpha\neq\emptyset\}$ be a collection of all indexsets $\alpha$, with 
$|\alpha|\equiv d_\alpha\leq p$. Where $d_\alpha$ is the number of predictors in the compact model. Thus for any $\alpha\in\mathcal{A}$ and observation $i=1,...,n$ the submodel is given by
\begin{align*}
	y_i=x_{i,\alpha}^\prime\beta_\alpha+\varepsilon_i
\end{align*}
which will be denoted by $\mathcal{M}_\alpha$. It has  $dim(\mathcal{M}_\alpha)=d_\alpha$, because $x_{i,\alpha}\in\mathbb{R}^{d_\alpha}$ and $\beta_\alpha\in\mathbb{R}^{d_\alpha}$ are both vectors which include all the components of the initial model which where indexed by $\alpha$.\footnote{There are $2^p-1$ different $\alpha's$ possible and therefore $2^p-1$ different forms of $\mathcal{M}_\alpha$ for one observation $i$. The $2^p-1$ is received by $2^p$ elements in the powerset of $\alpha$ minus 1 by leaving out the emptyset.}\\\\
One short example of how indexing works, might be for an $\alpha$ that consists for example of the positive integers $\{1,3,4,6\}$ such that it yields $\beta_\alpha=(\beta_1,\beta_3,\beta_4,\beta_6)^\prime$ and so on. This gives an illustration of what is meant by ``under model $\mathcal{M}_\alpha$''.

\subsection{Compact Model Categories} \label{chapter_compact_model}
%KANN man das so machen?/Keine eigenen worte, alles aus shao
For the following model selection problem, we have to introduce two possible types of $\mathcal{M}_\alpha$.We define
\begin{align*}
\begin{tabular}{ll}
Category~\RM{1}:& At least one $\beta_i\neq 0$ is \textit{not} in $\beta_\alpha$\\
Category~\RM{2}:& $\beta_\alpha$ contains \textit{all} $\beta_i\neq 0$
\end{tabular}
\end{align*}
where Category~\RM{1} contains all wrong models and Category~\RM{2} contains all models which may be too large.\\\\
The optimal model has to be a model out of Category \RM{2} and it will be denoted by
\begin{align*}
\mathcal{M}_\ast=\{\mathcal{M}_{\alpha_\ast}|\mathcal{M}_{\alpha_\ast}\in \text{Cat.~\RM{2}~and~} dim(\mathcal{M}_{\alpha_\ast})\leq dim(\mathcal{M}_{\alpha})\text{~for~any~}\mathcal{M}_{\alpha}\in \text{Cat.~\RM{2}} \}
\end{align*}

\subsection{Ordinary Least Squares Estimator under $\mathcal{M}_\alpha$}
%KEINE eigenen Worte (alles aus shao)
For the following chapters we should briefly mention the ordinary least squares estimator under $\mathcal{M}_\alpha$. Recall that our compact model for any $\alpha\in\mathcal{A}$ for observation $i=1,...,n$ is
\begin{align*}
	y_i=x_{i,\alpha}^\prime\beta_\alpha+\varepsilon_i
\end{align*}
Written in matrix notation we get that
\begin{align}
	y=X_\alpha \beta_\alpha+\varepsilon  \label{CM_model_matrix}
\end{align}
where $\beta_\alpha\in\mathbb{R}^{d_\alpha}$ is the true parameter vector indexed by $\alpha$. For the suitable matrix we have $X_\alpha=[x_{1,\alpha},...,x_{n,\alpha}]^\prime \in\mathbb{R}^{n\times d_\alpha}$
%HIER vielleicht aho zitieren
which has full rank, furthermore $y=(y_1,...,y_n)^\prime\in\mathbb{R}^n$ and $\varepsilon=(\varepsilon_1,...,\varepsilon_n)^\prime\in\mathbb{R}^n$. Thus the unique %STimmte das? 
ordinary least squares estimator under $\mathcal{M}_\alpha$ is given by\footnote{Proof of equation (\ref{OLS}) is given in Appendix \RM{1}}
\begin{align}
	\hat{\beta}_\alpha=(X_\alpha^\prime X_\alpha)^{-1}X_\alpha^\prime y
	\label{OLS}
\end{align}
If $\alpha=\{1,...,p\}$, then we have $d_\alpha=|\alpha|=p$ and $X_\alpha=X$. But we have to be careful, it does not hold for $X\beta=X_\alpha\beta_\alpha$ that it follows that $d_\alpha=p$.

\subsection{Prediction Errors}
%EINBASTELN (aus shao)

In this section we give a preview to the different types of prediction errors, we will need this in the next chapters for the construction of the \textit{Cross-Validation}.\\\\
For a given explanatory variable $x_i$, we denote $z_i$ as the \textit{future value}\footnote{Independent of $\hat{\beta}_\alpha$} of the dependent variable as in \cite{shao}, such that the \textit{Average Squared Prediction Error} under $\mathcal{M}_\alpha$ is given by
\begin{align}
ASPE(\mathcal{M}_\alpha)=\frac{1}{n}\sum_{i=1}^{n}(z_i-x_{i,\alpha}^\prime\hat{\beta}_\alpha)^2 \label{ASPE}
\end{align}
Then we can use the $ASPE(\mathcal{M}_\alpha)$ for calculating the \textit{Conditional Expected Squared Prediction Error} under $\mathcal{M}_\alpha$, which is given by\footnote{Proof of $CESPE(\mathcal{M}_\alpha)$ is given in Appendix \RM{1}}
\begin{align*}
CESPE(\mathcal{M}_\alpha)=\sigma^2+\frac{1}{n}\sum_{i=1}^{n}(x_i^\prime\beta-x_{i,\alpha}^\prime\hat{\beta}_\alpha)^2
\end{align*}
Thus we achieve, with the help of the formula above, the overall \textit{(Unconditional) Expected Squared Prediction Error} \footnote{Unconditional on Y}, that is given by\footnote{Proof of $\Gamma_{\alpha,n}$ is given in Appendix \RM{1}} 
\begin{align}
\Gamma_{\alpha,n}=\sigma^2+n^{-1}d_\alpha\sigma^2+\vartriangle_{\alpha,n} 
\end{align}
with
\begin{align*}
\begin{tabular}{ll}
$\vartriangle_{\alpha,n}$&$=n^{-1}(X\beta)^\prime(I_n-P_\alpha)X\beta$\\
$P_\alpha$&$=X_\alpha(X_\alpha^\prime X_\alpha)^{-1}X_\alpha^\prime$
\end{tabular}
\end{align*} 
where $P_\alpha$ is the corresponding hat matrix under model $\mathcal{M}_\alpha$.\\

 The first term in $\Gamma_{\alpha,n}$ which is given by $\sigma^2$ is the error term variance. The second term, $n^{-1}d_\alpha\sigma^2$, can be interpreted as the prediction error due to model dimensionality and the last term, $\Delta_{\alpha,b}$, can be interpreted as prediction error due to omitted important regressors. The interpretation of $\Delta_{\alpha,n}$ is supported by Lemma \ref{Equation2.3-2.4}\footnote{Proof of Lemma \ref{Equation2.3-2.4} is given in Appendix \RM{1}}.
\begin{lemma}
	\label{Equation2.3-2.4}
	For any fixed $n$, it holds true that:
	\begin{align*}
		&&\Delta_{\alpha,n}>0 &&\text{,if } \mathcal{M}_\alpha\in\text{Category I}&&\\
		&&\Delta_{\alpha,n}=0 &&\text{,if } \mathcal{M}_\alpha\in\text{Category II}&&
	\end{align*}
\end{lemma}
Since the second term in $\Gamma_{\alpha,n}$ vanishes as $n\to\infty$, $\Gamma_{\alpha,n}$ is asymptotically characterized by $\Delta_{\alpha,n}$, which we show in Lemma \ref{Equation2.5}\footnote{Proof of Lemma \ref{Equation2.5} is given in Appendix \RM{1}}. 
\begin{lemma}
	\label{Equation2.5}
	For any $\mathcal{M}_\alpha\in\text{Category}~\RM{1}$ and $\mathcal{M}_\gamma\in\text{Category}~\RM{2}$ with $d_\alpha=d_\gamma$ it holds that:
	\begin{align*}
	\liminf_{n\rightarrow\infty}\frac{\Gamma_{\alpha,n}}{\Gamma_{\gamma,n}}>1
	\end{align*}
	If and only if
	\begin{align}
	\liminf_{n\rightarrow\infty}\Delta_{\alpha,n}>0 \text{~~~for~}\mathcal{M}_\alpha\in\text{Category}~\RM{1} \label{liminf_condition}
	\end{align}
\end{lemma}

\textbf{Remark:}
	Under the assumption that $\mathcal{M}_\alpha\in\text{Category}~\RM{1}$ and $\mathcal{M}_\gamma\in\text{Category}~\RM{2}$ with $d_\alpha=d_\gamma$. We have that if
	\begin{align*}
		\lim_{n\rightarrow\infty}\frac{\Gamma_{\alpha,n}}{\Gamma_{\gamma,n}}=1
	\end{align*} 
	condition (\ref{liminf_condition}) does not hold, because
	\begin{align*}
	\lim_{n\rightarrow\infty}\Delta_{\alpha,n}=0.
	\end{align*}
Considering the condition (\ref{liminf_condition}) from an interpretational point of view we can say that
this is a necessary condition that $CV(n_\nu)$ works correctly. Otherwise if the condition does not hold, the above described case in the remark is possible. That means, the $CV(n_\nu)$ can't distinguish between $Category~\RM{1}$ and $Category~\RM{2}$ if condition (\ref{liminf_condition}) does not hold and therefore the usage of this method is useless.
\end{document}
	
