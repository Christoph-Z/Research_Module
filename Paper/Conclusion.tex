\documentclass[Research_Module_ES.tex]{subfiles}
\begin{document}
\section{Conclusion}
As we have seen in our simulation examples, the \textit{MCCV$(n_\nu)$} version of \cite{shao} behaves much better than the asymptotic incorrect \textit{CV$(1)$}. So we can verify the theoretical part, which has shown, that the different \textit{CV$(n_\nu)$} methods improve over \textit{CV$(1)$} by choosing $n_\nu$ with the heuristically recommended size of $n-\lfloor
n^{3/4}\rfloor$ and under the mentioned conditions. We have also seen that they behave better than \textit{AIC} which was not unexpected, because the \textit{AIC} and \textit{CV$(1)$} are asymptotically equivalent (this can be also seen in the simulation). A disadvantage of the \textit{CV$(n_\nu)$} methods is the complexitiy of computation that increases by $n_\nu$ increasing, so we had to improve the code efficiency by redundant operations, to get a faster calculation. Without doing this, some functions need to calculate extremly long, thus it is understandable that most authors in theory prefer the much simpler, but incorrect \textit{CV$(1)$} method. But in total we can see that the results of using the \textit{CV$(n_\nu)$} methods of \cite{shao} in comparison to the \textit{AIC} and \textit{CV$(1)$} for getting the model with the best prediction ability are much better, thus we can accept a more complex calculation for a more precise outcome. 
\end{document}