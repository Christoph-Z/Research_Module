\documentclass[Research_Module_ES.tex]{subfiles}
\begin{document}
\subsection{The Monte Carlo CV($n_\nu$)}

\begin{lemma} Denote by $\mathcal{R}^\ast:= \{s\subseteq\{1,\dots,n\}|\# s=n_v\}$ and let $\mathrm{E}_\mathcal{R}$ and $V_\mathcal{R}$ denote the expectation and variance with respect to the random selection of $\mathcal{R}$. Then for any function $a:\mathcal{R}^\ast\to \mathbb{R}$ it holds that
\begin{enumerate}
\item $\mathrm{E}_\mathcal{R} \bigl[ b^{-1}\sum_{s\in \mathcal{R}}a(s)\bigr] = \binom{n}{n_v}^{-1}\sum_{s\in\mathcal{R}^\ast}a(s)$
\item $V_\mathcal{R} \bigl( b^{-1}\sum_{s\in \mathcal{R}}a(s)\bigr) \le b^{-1} \mathrm{E}_\mathcal{R} \bigl[a(s)^2\bigr]$.
\end{enumerate}
\end{lemma}
\textbf{Proof:}
(1.)Denote by $s_i$ the $i$th selected subset in $\mathcal{R}$. Then the probability that subset $s \in \mathcal{R}^\ast$ is selected at the $i$th selection step, i.e. $s_i=s$, is given by
\begin{align*}
P(s_i=s)= \frac{1}{\# \mathcal{R}^\ast} = \binom{n}{n_v}^{-1}.
\end{align*}
Furthermore the selected subsets at step $i$ and $j\neq i$ are independent and identically distributed, i.e for any $s,s'\in \mathcal{R}^\ast$ it holds
\begin{align*}
P(s_i=s,s_j=s')=P(s_i=s)P(s_j=s')=\binom{n}{n_v}^{-2}.
\end{align*}
As the choice of sets $s\in \mathcal{R}$ is iid it follows that the random variables $(a(s_i))_{i=1}^b$ are iid as well. Hence, 
\begin{align*}
\mathrm{E}_\mathcal{R} \biggl[ \frac{1}{b}\sum_{s\in \mathcal{R}}a(s)\biggr] 
= \frac{1}{b}\underbrace{\sum_{s\in \mathcal{R}}\mathrm{E}_\mathcal{R} [a(s)]}_{=b\mathrm{E}_\mathcal{R}[a(s)]}
= \sum_{s\in \mathcal{R}^\ast}a(s)P(s_i = s)
=\binom{n}{n_v}^{-1}\sum_{s\in\mathcal{R}^\ast}a(s).
\end{align*}
(2.) Since the choice of sets $s\in \mathcal{R}$ and the random variables $(a(s_i))_{i=1}^b$ are iid
\begin{align*}
V_\mathcal{R} \biggl(\frac{1}{b}\sum_{s\in \mathcal{R}}a(s)\biggr)
= \frac{1}{b^2}\underbrace{\sum_{s\in \mathcal{R}}V_\mathcal{R} [a(s)]}_{=bV_\mathcal{R} [a(s)]}
\le \frac{1}{b}\mathrm{E}_\mathcal{R}\bigl[a(s)^2\bigr]. \quad \square
\end{align*}


\begin{satz}[Consistency of $MCCV(n_v)$]
Under the conditions of Theorem XY and
\begin{align*}
\max_{s\in \mathcal{R}}\biggl\lVert \frac{1}{n_v}\sum_{i\in s}x_ix_i' - \frac{1}{n-n_v}\sum_{i\in s^c}x_ix_i'\biggr\rVert =o_P(1)
\end{align*}
where $\mathcal{R}$ contains $b$ subsets selected randomly with $b$ satisfying
\begin{align*}
\frac{n^2}{b(n-n_v)^2}\to 0.
\end{align*}
Suppose furthermore that $n_v$ is selected such that
\begin{align*}
\frac{n_v}{n}\to 1 \quad \textrm{and} \quad n-n_v \to \infty.
\end{align*}
Then the following holds
\begin{enumerate}[(I)]
\item If $\mathcal{M}_\alpha$ is in Category I, then there exists $R_n \ge 0$ such that $\hat{\Gamma}_{\alpha,n}^{MCCV} = \frac{1}{n_vb}\sum_{s\in \mathcal{R}}\varepsilon_s'\varepsilon_s + \Delta_{\alpha,n} + R_n + o_P(1)$.
\item If $\mathcal{M}_\alpha$ is in Category II, then $\hat{\Gamma}_{\alpha,n}^{MCCV} = \frac{1}{n_vb}\sum_{s\in \mathcal{R}}\varepsilon_s'\varepsilon_s + \frac{1}{n-n_v}d_\alpha\sigma^2  + o_P((n-n_v)^{-1})$.
\item $\lim_{n\to\infty}P(\mathcal{M}_{MCCV}=\mathcal{M}_\ast) = 1$
\end{enumerate}
where $\mathcal{M}_{MCCV}$ denotes the model selected by using $MCCV(n_v)$.
\end{satz}

\textbf{Proof of $(I)$:} 
Shao doesn't proof this assertion in his paper but gives a guideline which we follow.

Analogously as in the proof of theorem XY, we can bound $\hat{\Gamma}_{\alpha,n}^{MCCV}$ by
\begin{align*}
\hat{\Gamma}_{\alpha,n}^{MCCV}= \frac{1}{n_vb}\sum_{s\in \mathcal{R}}\lVert y_s-\hat{y}_{\alpha,s^c}\rVert^2
= \frac{1}{n_vb}\sum_{s\in \mathcal{R}}\lVert (I_{n_v}-Q_{\alpha,s})^{-1}(y_s-X_{\alpha,s}\hat{\beta}_\alpha)\rVert^2\\
\ge \frac{1}{n_vb}\sum_{s\in \mathcal{R}}\lVert y_s-X_{\alpha,s}\hat{\beta}_\alpha\rVert^2\\
\end{align*}
As $\mathcal{R}^\ast$ is a balanced incomplete block design it holds that
\begin{align*}
\mathrm{E}_\mathcal{R} \biggl[\frac{1}{n_vb}\sum_{s\in \mathcal{R}}\lVert y_s-X_{\alpha,s}\hat{\beta}_\alpha\rVert^2 \biggr] 
= \frac{1}{n_v}\binom{n}{n_v}^{-1}\sum_{s\in \mathcal{R}^\ast}\sum_{i\in s}(y_i-x_{\alpha,i}'\hat{\beta}_\alpha)^2
= \frac{1}{n}\sum_{i\in s}(y_i-x_{\alpha,i}'\hat{\beta}_\alpha)^2
\end{align*}
And finally, by the strong law of large numbers 
we can establish that $\hat{\Gamma}_{\alpha,n}^{MCCV} = \frac{1}{n}\lVert y-X_{\alpha}\hat{\beta}_\alpha\rVert^2 + o_P(1)$. The remaining proof of statement $(I)$ is the same as in the case of $BICV(n_v)$.

\textbf{Proof of $(II)$:}
To proof the second statement, Shao uses a more refined decomposition as in the proof of theorem XY. 
\begin{enumerate}
\item  $\hat{\Gamma}_{n,\alpha}^{MCCV} = \frac{1}{n_vb}\sum_{s\in \mathcal{R}}\varepsilon_s'\varepsilon_s+ A_{\alpha1}-A_{\alpha2}+a_{\alpha3}+B_\alpha$
\item $A_{\alpha1} = B_{\alpha1}+ B_{\alpha2}$
\item $ A_{\alpha1}-A_{\alpha2}+a_{\alpha3} = \frac{1}{n-n_v}d_\alpha\sigma^2 + o_P((n-n_v)^{-1})$
\item $B_{\alpha2} = o_P((n-n_v)^{-1})$
\item $B_{\alpha} = o_P((n-n_v)^{-1})$
\end{enumerate}

\textbf{Proof of $(III)$:} 
The proof is essentially the same as that of the corresponding statement for $BICV(n_v)$. One just has to replace $\frac{1}{n}\varepsilon'\varepsilon$ with $frac{1}{n_vb}\sum_{s\in \mathcal{R}}\varepsilon_s'\varepsilon_s$. Anyway these two terms are asymptocially closely related as
\begin{align*}
\mathrm{E}_\mathcal{R} \biggl[ \frac{1}{n_vb}\sum_{s\in \mathcal{R}}\varepsilon_s'\varepsilon_s\biggr] 
=\frac{1}{n_v}\binom{n}{n_v}^{-1}\sum_{s\in \mathcal{R}^\ast}\varepsilon_s'\varepsilon_s
\end{align*}
by the lemma above. As furthermore $\mathcal{R}^\ast$ is a balanced incomplete block design by lemma XY 
\begin{align*}
\frac{1}{n_v}\binom{n}{n_v}^{-1}\sum_{s\in \mathcal{R}^\ast}\varepsilon_s'\varepsilon_s
= \frac{1}{n_v}\binom{n}{n_v}^{-1}\sum_{i=1}^n \varepsilon_i^2 \underbrace{\#\{s\in \mathcal{R}^\ast | i\in s\}}_{=\binom{n}{n_v}\frac{n_v}{n}}
= \frac{\varepsilon'\varepsilon}{n}
\end{align*}
In addition,
\begin{align*}
V_\mathcal{R} \biggl[ \frac{1}{n_vb}\sum_{s\in \mathcal{R}}\varepsilon_s'\varepsilon_s\biggr]
&\le \frac{1}{n_v^2b}\mathrm{E}_\mathcal{R} \bigl[(\varepsilon_s'\varepsilon_s)^2\bigr]
= \frac{1}{n_v^2b}\binom{n}{n_v}^{-1}\sum_{s\in \mathcal{R}^\ast}\sum_{i\in s} \varepsilon_i^2\\
&= \frac{1}{n_vn} \sum_{i=1}^n \varepsilon_i^2 
= \frac{\sigma^2}{n_v} + o_P\biggl(\frac{1}{n_v}\biggr)
= o_P\biggl(\frac{1}{n_v}\biggr).
\end{align*}
Hence, 
\begin{align*}
\frac{1}{n_vb}\sum_{s\in \mathcal{R}}\varepsilon_s'\varepsilon_s =\frac{\varepsilon'\varepsilon}{n} +o_P\biggl(\frac{1}{n_v}\biggr). \quad \square
\end{align*}

\end{document}