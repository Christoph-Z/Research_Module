\documentclass[Research_Module_ES.tex]{subfiles}
\begin{document}
\section*{Appendix \RM{1}}

%HIER kommen die Proofs rein, beim reinkopieren muss man auf den grünen doppelfpeil um das dokument einmal zu erstellen und anzuzeigen, damit das ausgeführt wird , danach ist es auch in der großen datei sichtbar, sonst nicht (die anzeige links zum appendix ist wohl deshalb rot, da das kapitel appendix keine nummer bekommt, also nicht wundern)
\addcontentsline{toc}{subsection}{Proofs of Chapter 3}
\subsection*{Proofs of Chapter 3}
\begin{proof}[Proof of the OLS estimator in equation (\ref{OLS})]~\\
	The minimization problem of a least squares estimator is given by
	\begin{align*}
	\min_{b_\alpha}(y-X_\alpha b_\alpha)^\prime(y-X_\alpha b_\alpha)
	\end{align*}
	By using the subsequent rules for matrix diferentation:\\\\
	\textit{For vectors $a,\beta$ with adequate dimensions and a symmetric matrix A with adequate dimension, it holds:}
	\begin{align*}
	1)&~ \frac{\partial}{\partial \beta}(\beta^\prime a)=a\\
	2)&~\frac{\partial}{\partial \beta}(\beta^\prime A\beta)=2A\beta
	\end{align*}
	The first order conditions yield
	\begin{align*}
	& \frac{\partial}{\partial b_\alpha}(y-X_\alpha b_\alpha)^\prime(y-X_\alpha b_\alpha)\stackrel{!}{=}0\\
	\Leftrightarrow & \frac{\partial}{\partial b_\alpha}(y^\prime y-b_\alpha^\prime X_\alpha^\prime y-y^\prime X_\alpha b_\alpha+b_\alpha^\prime X_\alpha^\prime X_\alpha b_\alpha)=0\\
	\Leftrightarrow & \frac{\partial}{\partial b_\alpha}(y^\prime y-2b_\alpha^\prime X_\alpha^\prime y+b_\alpha^\prime X_\alpha^\prime X_\alpha b_\alpha)=0\\
	\Leftrightarrow & -2X_\alpha^\prime y +2 X_\alpha^\prime X_\alpha b_\alpha=0\\
	\Leftrightarrow & X_\alpha^\prime X_\alpha b_\alpha=X_\alpha^\prime y
	\end{align*}
	And under the assumption that $rank(X_\alpha)=rank(X_\alpha^\prime X_\alpha)=d_\alpha$ is full, we know that $X_\alpha^\prime X_\alpha$ is invertible, thus 
	\begin{align*}
	\hat{\beta}_\alpha=(X_\alpha^\prime X_\alpha)^{-1}X_\alpha^\prime y
	\end{align*}
\end{proof}


\begin{proof}[Proof of Conditional Expected Squared Prediction Error] ~\\Under the use of linearity of expectation and the fact that $\hat{\beta}_\alpha$ is conditioned on $Y$ we get 
	\begin{align*}
	\begin{tabular}{ll}
	$CESPE(\mathcal{M}_\alpha)$&$=E[ASPE(\mathcal{M}_\alpha)|Y]$\\
	&$=E[\frac{1}{n}(Y-X_\alpha\hat{\beta}_\alpha)^\prime(Y-X_\alpha\hat{\beta}_\alpha)|Y]$\\
	&$=E[\frac{1}{n}(X\beta+\varepsilon-X_\alpha\hat{\beta}_\alpha)^\prime(X\beta+\varepsilon-X_\alpha\hat{\beta}_\alpha)|Y]$\\
	&$=E[\frac{1}{n}(\beta^\prime X^\prime+\varepsilon^\prime-\hat{\beta}_\alpha^\prime X_\alpha^\prime)(X\beta+\varepsilon-X_\alpha\hat{\beta}_\alpha)|Y]$\\
	&$=E[\frac{1}{n}(\beta^\prime X^\prime X\beta+\beta^\prime X^\prime\varepsilon-\beta^\prime X^\prime X_\alpha\hat{\beta}_\alpha+\varepsilon^\prime X\beta+\varepsilon^\prime\varepsilon$\\&$~~~-\varepsilon^\prime X_\alpha\hat{\beta}_\alpha-\hat{\beta}_\alpha^\prime X_\alpha^\prime X\beta-\hat{\beta}_\alpha^\prime X_\alpha^\prime\varepsilon+\hat{\beta}_\alpha^\prime X_\alpha^\prime X_\alpha\hat{\beta}_\alpha)|Y]$\\
	&$=\frac{1}{n}\beta^\prime X^\prime X\beta+\frac{1}{n}\beta^\prime X^\prime\cdot E[\varepsilon|Y]-\frac{1}{n}\beta^\prime X^\prime X_\alpha\hat{\beta}_\alpha$\\
	&$~~~+\frac{1}{n}E[\varepsilon^\prime|Y]\cdot X\beta+\frac{1}{n}E[\varepsilon^\prime\varepsilon|Y]-\frac{1}{n}E[\varepsilon^\prime|Y]X_\alpha\hat{\beta}_\alpha$\\
	&$~~~-\frac{1}{n}\hat{\beta}_\alpha^\prime X_\alpha^\prime X\beta-\frac{1}{n}\hat{\beta}_\alpha^\prime X_\alpha^\prime\cdot E[\varepsilon|Y]+\frac{1}{n}\hat{\beta}_\alpha^\prime X_\alpha^\prime X_\alpha\hat{\beta}_\alpha$\\
	&$=\frac{1}{n}\parallel X\beta\parallel^2+0-\frac{1}{n} (X\beta)^\prime X_\alpha\hat{\beta}_\alpha+0+\frac{1}{n} n\sigma^2-0-\frac{1}{n}(X_\alpha\hat{\beta}_\alpha)^\prime X\beta$\\
	&$~~~-0+\frac{1}{n}\parallel X_\alpha\hat{\beta}_\alpha\parallel^2$\\
	&$=\sigma^2+\frac{1}{n}\sum_{i=1}^{n}[(x_i^\prime\beta)^2-2(x_i^\prime\beta)(x_{i,\alpha}^\prime\hat{\beta}_\alpha)+(x_{i,\alpha}^\prime\hat{\beta}_\alpha)^2]$\\
	&$=\sigma^2+\frac{1}{n}\sum_{i=1}^{n}(x_i^\prime\beta-x_{i,\alpha}^\prime\hat{\beta}_\alpha)^2$
	\end{tabular}
	\end{align*}
\end{proof}


\begin{proof}[Proof of Unconditional Expected Squared Prediction Error]~\\
	Under the usage of the Law of iterated expectations and the following useful transformation of
	%AUS Skript vom Liebl
	\begin{align*}
	\begin{tabular}{ll}
	$E[\varepsilon^\prime P_\alpha^\prime P_\alpha\varepsilon]$&$=E[\varepsilon^\prime P_\alpha\varepsilon]$\\
	&$=\sum_{i=1}^{n}\sum_{j=1}^{n}p_{ij,\alpha}\cdot E[\varepsilon_i\varepsilon_j]$\\
	&$=\sum_{i=1}^{n}p_{ii,\alpha}\cdot E[\varepsilon_i^2]$\\
	&$=\sum_{i=1}^{n}p_{ii,\alpha}\cdot \sigma^2$\\
	&$=\sigma^2\cdot tr(P_\alpha)$\\
	&$=\sigma^2\cdot d_\alpha$
	\end{tabular}
	\end{align*}
	while respecting, that $P_\alpha$ is a symmetric and idempotent projection matrix with diagonal elements $p_{ii,\alpha}$. Also recall that for two vectors $a$ and $b$ with the same dimension, we are allowed to use $\parallel a-b\parallel^2=\parallel a\parallel^2-2a^\prime b+\parallel b\parallel^2$. And at least notice, that the orthogonal projection matrix $(I_n-P_\alpha)\in\mathbb{R}^{n\times n}$ is also symmetric and idempotent.\\\\
	Thus now we can show, that
	\begin{align*}
	\begin{tabular}{ll}
	$\Gamma_{\alpha,n}$&$=E[ASPE(\mathcal{M}_\alpha)]$\\
	&$=E[E[ASPE(\mathcal{M}_\alpha)|Y]]$\\
	&$=E[CESPE(\mathcal{M}_\alpha)]$\\
	&$=E[\sigma^2+\frac{1}{n}\sum_{i=1}^{n}(x_i^\prime\beta-x_{i,\alpha}^\prime\hat{\beta}_\alpha)^2]$\\
	&$=\sigma^2+\frac{1}{n}E[\parallel X\beta-X_\alpha\hat{\beta}_\alpha\parallel^2]$\\
	&$=\sigma^2+\frac{1}{n}E[\parallel X\beta-P_\alpha Y\parallel^2]$\\
	&$=\sigma^2+\frac{1}{n}E[\parallel X\beta-P_\alpha X\beta-P_\alpha\varepsilon\parallel^2]$\\
	&$=\sigma^2+\frac{1}{n}E[\parallel (I_n-P_\alpha)X\beta-P_\alpha\varepsilon\parallel^2]$\\
	&$=\sigma^2+\frac{1}{n}E[\parallel (I_n-P_\alpha)X\beta\parallel^2-2((I_n-P_\alpha) X\beta)^\prime P_\alpha\varepsilon+\parallel P_\alpha\varepsilon\parallel^2]$\\
	&$=\sigma^2+\frac{1}{n}E[((I_n-P_\alpha)X\beta)^\prime((I_n-P_\alpha)X\beta)]-\frac{1}{n}2(I_n-P_\alpha)X\beta P_\alpha E[\varepsilon]$\\
	&$~~~+\frac{1}{n}E[(P_\alpha\varepsilon)^\prime (P_\alpha\varepsilon)]$\\
	&$=\sigma^2+\frac{1}{n}\beta^\prime X^\prime (I_n-P_\alpha)X\beta+\frac{1}{n}E[\varepsilon^\prime P_\alpha^\prime P_\alpha\varepsilon]$\\
	&$=\sigma^2+\vartriangle_{\alpha,n}+\frac{1}{n}\cdot\sigma^2\cdot tr(P_\alpha)$\\
	&$=\sigma^2+n^{-1}\sigma^2d_\alpha+\vartriangle_{\alpha,n}$
	\end{tabular}
	\end{align*}\\
\end{proof}	


\begin{proof}[Proof of Lemma \ref{Equation2.3-2.4}]~\\
	For the first part, assume $\mathcal{M}_\alpha\in$ Category I and note that we can rewrite $n\Delta_{\alpha,n}$ as follows
	\begin{align}\nonumber
	n\Delta_{\alpha,n}&= \left(X \beta \right)^\prime \left(I_n-P_\alpha\right)X\beta\\\nonumber
	&= \left(X \beta \right)^\prime \left(I_n-P_\alpha\right)^\prime\left(I_n-P_\alpha\right)X\beta\\\nonumber
	&=||\left(I_n-P_\alpha\right)X\beta||^2\\\nonumber
	&=||\left(I_n-P_\alpha\right)(X_\alpha\beta_\alpha+X_{\alpha^c}\beta_{\alpha^c})||^2\\
	&=||\left(I_n-P_\alpha\right)X_{\alpha^c}\beta_{\alpha^c}||^2>0 \label{larger0}
	\end{align}
	Hence 
	%\[
	%(\mathcal{M}_\alpha\in \text{ Cat I } \wedge \text{ r}(X)=p) \Rightarrow(\exists\beta_i\in \alpha^c:\beta_i\neq0 \wedge X_{\alpha^c}\text{ r}(X_{\alpha^c})=p-d_\alpha)\Rightarrow X_{\alpha^c}\beta_{\alpha^c}\neq0
	%\]
	$\mathcal{M}_\alpha\in$ Category I, it must exist at least one none zero component in $\beta_{\alpha^c}$. Together with $X$ having full rank, we can conclude that $X_{\alpha^c}\beta_{\alpha^c}$ is always unequal to zero.\\ 
	Note further, that $(I_n-P_\alpha)$ is the Projection matrix onto the orthogonal complement of span$\{x_{\alpha,1},\ldots,x_{\alpha,d_\alpha}\}$. Since
	\begin{align*}
	X_{\alpha^c}\beta_{\alpha^c}&\neq 0\\
	X_{\alpha^c}\beta_{\alpha^c}\not\perp (&I_n-P_\alpha)
	\end{align*}
	it holds that $(I_n-P_\alpha) X_{\alpha^c}\beta_{\alpha^c}\neq 0$ and therefore (\ref{larger0}) remains true.\\
	\\
	For the second part, assume $\mathcal{M}_\alpha\in$ Category II, then $X\beta=X_\alpha\beta_\alpha$. Thus
	\begin{align*}
	n\Delta_\alpha,n&=\beta^\prime X^\prime X\beta-\beta^\prime X^\prime X_\alpha \left(X_\alpha^\prime X_\alpha\right)^{-1}X_\alpha^\prime X\beta\\
	&=\beta_\alpha^\prime X_\alpha^\prime X_\alpha \beta_\alpha -\beta_\alpha^\prime X_\alpha^\prime X_\alpha\left(X_\alpha^\prime X_\alpha\right)^{-1} X_\alpha^\prime X_\alpha\beta_\alpha\\
	&=\beta_\alpha^\prime X_\alpha^\prime X_\alpha \beta_\alpha - \beta_\alpha^\prime X_\alpha^\prime X_\alpha \beta_\alpha\\
	&=0
	\end{align*}\\
\end{proof}


\begin{proof}[Proof of Lemma \ref{Equation2.5}]~\\
	Let $\mathcal{M}_\alpha\in\text{Category}~\RM{1}$ and $\mathcal{M}_\gamma\in\text{Category}~\RM{2}$ with $d_\alpha=d_\gamma$. It follows for the \textit{unconditional expected squared prediction errors} with Lemma \ref{Equation2.3-2.4}:
	\begin{align*}
	\begin{tabular}{ll}
	$\Gamma_{\alpha,n}$&$=\sigma^2+n^{-1}d_\alpha\sigma^2+\Delta_{\alpha,n}~~~~~$ with $\Delta_{\alpha,n}>0$\\
	$\Gamma_{\gamma,n}$&$=\sigma^2+n^{-1}d_\gamma\sigma^2~~~~~~~~~~~~~~~~$with $\Delta_{\gamma,n}=0$
	\end{tabular}
	\end{align*}
	Then:
	\begin{align*}
	\liminf_{n\rightarrow\infty}\frac{\Gamma_{\alpha,n}}{\Gamma_{\gamma,n}}&=\liminf_{n\rightarrow\infty}\frac{\sigma^2+n^{-1}d_\alpha\sigma^2+\Delta_{\alpha,n}}{\sigma^2+n^{-1}d_\gamma\sigma^2}	\\
	&=\liminf_{n\rightarrow\infty}\Big[\frac{\sigma^2+n^{-1}d_\alpha\sigma^2}{\sigma^2+n^{-1}d_\alpha\sigma^2}+\frac{\Delta_{\alpha,n}}{\sigma^2+n^{-1}d_\alpha\sigma^2}\Big]\\
	&=\liminf_{n\rightarrow\infty}\Big[1+\frac{\Delta_{\alpha,n}}{\sigma^2+n^{-1}d_\alpha\sigma^2}\Big]\\
	&=1+\liminf_{n\rightarrow\infty}\frac{\Delta_{\alpha,n}}{\sigma^2+n^{-1}d_\alpha\sigma^2}\\
	&=1+\frac{\liminf_{n\rightarrow\infty}\Delta_{\alpha,n}}{\sigma^2+ \liminf_{n\rightarrow\infty} n^{-1}d_\alpha\sigma^2}\\
	&=1+\frac{\liminf_{n\rightarrow\infty}\Delta_{\alpha,n}}{\sigma^2}\\
	&>1
	\end{align*}
	Exactly when condition (\ref{liminf_condition}) for $\mathcal{M}_\alpha\in\text{Category}~\RM{1}$ holds.\\
\end{proof}

\addcontentsline{toc}{subsection}{Proofs of Chapter 4}
\subsection*{Proofs of Chapter 4}

\begin{proof}[Proof of Lemma \ref{Equation3.1}]~\\
	Given by construction, we have
	\begin{align*}
	X_\alpha=\Big(\begin{matrix}
	X_{\alpha,s}\\ X_{\alpha,s^c}
	\end{matrix}\Big)
	\text{~~~consisting of the two submatrices~} X_{\alpha,s}\in\mathbb{R}^{n_\nu\times d_\alpha} \text{~and~} X_{\alpha,s^c}\in\mathbb{R}^{(n-n_\nu)\times d_\alpha}
	\end{align*}
	and 
	\begin{align*}
	y=\Big(\begin{matrix}
	y_{s}\\ y_{s^c}
	\end{matrix}\Big),~~\hat{\beta}_\alpha=\Big(\begin{matrix}
	\hat{\beta}_{\alpha,s}\\ \hat{\beta}_{\alpha,s^c}
	\end{matrix}\Big)
	\end{align*}
	With the help of the following decompositions
	\begin{align*}
	X_\alpha^\prime y&=X_{\alpha,s}^\prime y_s+X_{\alpha,s^c}^\prime y_{s^c}\\
	X_\alpha^\prime X_\alpha&=X_{\alpha,s}^\prime X_{\alpha,s}+X_{\alpha,s^c}^\prime X_{\alpha,s^c}
	\end{align*}
	Thus now we can rewrite $\hat{\beta}_\alpha$ as
	\begin{align}
	\hat{\beta}_\alpha&=(X_\alpha^\prime X_\alpha)^{-1}X_\alpha^\prime y   \nonumber\\
	&=(X_\alpha^\prime X_\alpha)^{-1}[X_{\alpha,s}^\prime y_s+X_{\alpha,s^c}^\prime y_{s^c}]  \nonumber \\
	&=(X_\alpha^\prime X_\alpha)^{-1}[(X_{\alpha,s}^\prime X_{\alpha,s})(X_{\alpha,s}^\prime X_{\alpha,s})^{-1}X_{\alpha,s}^\prime y_s+X_{\alpha,s^c}^\prime y_{s^c}]  \nonumber \\
	&=(X_\alpha^\prime X_\alpha)^{-1}[(X_{\alpha,s}^\prime X_{\alpha,s}) \hat{\beta}_{\alpha,s}+(X_{\alpha,s^c}^\prime X_{\alpha,s^c})\hat{\beta}_{\alpha,s^c}]
	\label{beta_alpha_hat_decomposition}
	\end{align}
	The average squared prediction error for split $s$ satisfies
	\begin{align*}
	ASPE_{s}(\mathcal{M}_\alpha)&=\frac{1}{n_\nu}\parallel y_s-\hat{y}_{\alpha,s^c}\parallel^2\\
	&=\frac{1}{n_\nu}\parallel y_s-X_{\alpha,s}\hat{\beta}_{\alpha,s^c}\parallel^2\\
	&=\frac{1}{n_\nu}\parallel (I_{n_\nu}-Q_{\alpha,s})^{-1}(I_{n_\nu}-Q_{\alpha,s})(y_s-X_{\alpha,s}\hat{\beta}_{\alpha,s^c})\parallel^2
	\end{align*}
	with $Q_{\alpha,s}=X_{\alpha,s}(X_\alpha^\prime X_\alpha)^{-1}X_{\alpha,s}^\prime$.\\ 
	Thus it is enough to show that $(I_{n_\nu}-Q_{\alpha,s})(y_s-X_{\alpha,s}\hat{\beta}_{\alpha,s^c})=y_s-X_{\alpha,s}\hat{\beta}_\alpha$ holds 
	\begin{align*}
	&-Q_{\alpha,s}y_s-X_{\alpha,s}\hat{\beta}_{\alpha,s^c}+Q_{\alpha,s}X_{\alpha,s}\hat{\beta}_{\alpha,s^c}=-X_{\alpha,s}\hat{\beta}_\alpha\\
	\Leftrightarrow&-X_{\alpha,s}(X_{\alpha}^\prime X_{\alpha})^{-1}X_{\alpha,s}^\prime y_s- X_{\alpha,s}\hat{\beta}_{\alpha,s^c}+X_{\alpha,s}(X_{\alpha}^\prime X_{\alpha})^{-1}X_{\alpha,s}^\prime X_{\alpha,s}\hat{\beta}_{\alpha,s^c}=-X_{\alpha,s}\hat{\beta}_{\alpha}\\
	\Leftrightarrow&(X_{\alpha}^\prime X_{\alpha})^{-1}X_{\alpha,s}^\prime y_s+ \hat{\beta}_{\alpha,s^c}-(X_{\alpha}^\prime X_{\alpha})^{-1}X_{\alpha,s}^\prime X_{\alpha,s}\hat{\beta}_{\alpha,s^c}=\hat{\beta}_{\alpha}\\
	\Leftrightarrow&(X_{\alpha}^\prime X_{\alpha})^{-1}(X_{\alpha,s}^\prime X_{\alpha,s})\hat{\beta}_{\alpha,s}+ \hat{\beta}_{\alpha,s^c}-(X_{\alpha}^\prime X_{\alpha})^{-1}(X_{\alpha,s}^\prime X_{\alpha,s})\hat{\beta}_{\alpha,s^c}=\hat{\beta}_{\alpha}\\
	\Leftrightarrow&(X_{\alpha}^\prime X_{\alpha})^{-1}[(X_{\alpha,s}^\prime X_{\alpha,s})(\hat{\beta}_{\alpha,s}-\hat{\beta}_{\alpha,s^c})+(X_{\alpha}^\prime X_{\alpha})\hat{\beta}_{\alpha,s^c}]=\hat{\beta}_{\alpha}\\
	\Leftrightarrow&(X_{\alpha}^\prime X_{\alpha})^{-1}[(X_{\alpha,s}^\prime X_{\alpha,s})(\hat{\beta}_{\alpha,s}-\hat{\beta}_{\alpha,s^c})+[X_{\alpha,s}^\prime X_{\alpha,s}+X_{\alpha,s^c}^\prime X_{\alpha,s^c}]\hat{\beta}_{\alpha,s^c}]=\hat{\beta}_{\alpha}\\
	\Leftrightarrow&(X_\alpha^\prime X_\alpha)^{-1}[(X_{\alpha,s}^\prime X_{\alpha,s}) \hat{\beta}_{\alpha,s}+(X_{\alpha,s^c}^\prime X_{\alpha,s^c})\hat{\beta}_{\alpha,s^c}]=\hat{\beta}_{\alpha}
	\end{align*}
	Which is exactly the decomposition of $\hat{\beta}_{\alpha}$ in   (\ref{beta_alpha_hat_decomposition}).\\
\end{proof}
\addcontentsline{toc}{subsection}{Proof Asymptotic Properties of $CV(1)$}
\subsection*{Proof Asymptotic Properties of $CV(1)$}
\begin{proof}[Proof of Theorem \ref{THM_Consistency of $CV(1)$}]~\\
	\textbf{Proof of $(I)$:} First note that we can rewrite $\hat{\Gamma}_{\alpha,n}^{CV(1)}$  by Lemma \ref{Equation3.1} as 
	\begin{align*}
	\hat{\Gamma}_{\alpha,n}^{CV(1)} &= \frac{1}{n}\sum_{i=1}^n (1- p_{ii\alpha})^{-2}(y_i-x_{i,\alpha}^\prime\hat{\beta}_\alpha)^2.
	\end{align*}
	To linearize this expression in $p_{ii\alpha}$, we perform a Taylor expansion of $(1-p_{ii\alpha})^{-2}$.
	More precisely, a first order Taylor expansion of $(1-x)^{-2}$ in $x$ around $0$ yields 
	\begin{align}
	(1- x)^{-2} = 1+ 2 x + o(x^2) \quad \textrm{as} \quad x\to 0 .\label{proof_CV(1)_1}
	\end{align}
	By condition $(ii)$ and as
	\begin{align*}
	2p_{ii\alpha} + o(p_{ii\alpha}^2) 
	\le 2 \max_{1\le i \le n} p_{ii\alpha} +  o(p_{ii\alpha}^2) 
	= O\bigl(\max _{1\le i \le n} p_{ii\alpha}\bigr)
	\end{align*}
	it holds that 
	\begin{align*}
	(1- p_{ii\alpha})^{-2} = 1+ O(\max _{1\le i \le n} p_{ii\alpha}) 
	= 1 +o(1) \quad \textrm{as} \quad n \to \infty. 
	\end{align*}
	
	%DEN CV(1) estimator auch so gekennzeichnet, eventuell vor das THM 4.1.1 packen, damit er nicht so vom himmel fällt
	Using this expression to rewrite $\hat{\Gamma}_{\alpha,n}^{CV(1)}$ yields
	\begin{align}
	\hat{\Gamma}_{\alpha,n}^{CV(1)} &= \frac{1}{n}\sum_{i=1}^n (1- p_{ii\alpha})^{-2}(y_i-x_{i,\alpha}^\prime\hat{\beta}_\alpha)^2\nonumber\\
	&= (1+o(1))\frac{1}{n}\lVert y-X_{\alpha}\hat{\beta}_\alpha\rVert^2\nonumber\\
	&= (1+o(1))\frac{1}{n}\lVert (I_n-P_\alpha) y\rVert^2\nonumber\\
	&= (1+o(1))\frac{1}{n}\lVert (I_n-P_\alpha) X_\alpha\beta_\alpha +(I_n-P_\alpha)\varepsilon \rVert^2\nonumber\\
	&= (1+o(1))\biggl[\frac{1}{n}\lVert(I_n-P_\alpha)\varepsilon \rVert^2+ \underbrace{\frac{1}{n}\lVert (I_n-P_\alpha) X_\alpha\beta_\alpha\rVert^2}_{=\Delta_{\alpha,n}} +\frac{2}{n}\varepsilon'(I_n-P_\alpha) X_\alpha\beta_\alpha\biggr]\nonumber\\
	&= (1+o(1)) \biggl[\frac{\varepsilon'\varepsilon}{n} + \Delta_{\alpha,n} - \frac{\varepsilon'P_\alpha\varepsilon}{n} +\frac{2}{n}\varepsilon'(I_n-P_\alpha) X_\alpha\beta_\alpha\biggr]. \label{proof_CV(1)_2}
	\end{align}
	
	Since the $\varepsilon_i$ are iid with finite second moments it follows by the law of large numbers that
	$\frac{\varepsilon'\varepsilon}{n} = \sigma^2 + o_P(1)$.
	Furthermore for any positive $\delta$
	\begin{align*}
	P\biggl(\biggl|\frac{1}{n}\varepsilon'P_\alpha\varepsilon\biggr|>\delta\biggr) \le \frac{\mathrm{E}\bigl[|\varepsilon'P_\alpha\varepsilon|\bigr]}{n\delta}=\frac{\mathrm{E}\bigl[\lVert P_\alpha\varepsilon\rVert^2\bigr]}{n\delta}=\frac{d_\alpha \sigma^2}{n\delta} \to 0 \quad \textrm{as} \quad n\to \infty
	\end{align*}
	by the Markov inequality and hence $\frac{1}{n}\varepsilon'P_\alpha\varepsilon = o_P(1)$.\\
	Finally, by the Cebysev inequality it holds for any positve $\delta$ that
	%HIER die Notation in Zeile zwei erklären, am besten mit Fußnote,
	%LETZTe Zeile hier Erinnerung an Submultiplikativität und Verträglichkeite
	\begin{align*}
	P\biggl(&\biggl|\frac{2}{n}\varepsilon'(I_n-P_\alpha)X_\alpha\beta_\alpha\biggr|>\delta\biggr) \le \frac{4\mathrm{E}\bigl[(\varepsilon'(I_n-P_\alpha)X_\alpha\beta_\alpha)^2\bigr]}{n^2\delta^2}\\
	&=\frac{4}{n^2\delta^2}\sum_{i,j}\mathrm{E}\bigl[\varepsilon_i[(I_n-P_\alpha)X_\alpha\beta_\alpha]_i\varepsilon_j[(I_n-P_\alpha)X_\alpha\beta_\alpha]_j\bigr]
	\end{align*}
	where $[(I_n-P_\alpha)X_\alpha\beta_\alpha]_i$ denotes the $i$th element of the vector $(I_n-P_\alpha)X_\alpha\beta_\alpha$. By using the independence of th $\varepsilon_i$, this can be further simplified to
	\begin{align}
	&\frac{4}{n^2\delta^2}\sum_{i,j}\mathrm{E}\bigl[\varepsilon_i[(I_n-P_\alpha)X_\alpha\beta_\alpha]_i\varepsilon_j[(I_n-P_\alpha)X_\alpha\beta_\alpha]_j\bigr]\nonumber\\
	= &\frac{4}{n^2\delta^2}\sum_{i=1}^n\mathrm{E}\bigl[\varepsilon_i^2\bigr][(I_n-P_\alpha)X_\alpha\beta_\alpha]_i^2 \nonumber\\
	= &\frac{4\sigma^2}{n^2\delta^2}\lVert(I_n-P_\alpha)X_\alpha\beta_\alpha\rVert^2. \label{proof_CV_1}
	\end{align}
	Next, we use that the spectral norm is sub-multiplicative and subordinate to the Euclidean norm to achieve the bound
	\begin{align}
	\lVert(I_n-P_\alpha)X_\alpha\beta_\alpha\rVert^2
	\le \underbrace{\lVert I_n-P_\alpha \rVert^2}_{=1} \underbrace{\max_{z\in \mathbb{R}^{d_\alpha}\setminus 0}\frac{z'X'_\alpha X_\alpha z}{z'z}}_{=O(n)\textrm{, by condition $(i)$}} \lVert \beta_\alpha \rVert^2,\label{proof_CV_2}
	\end{align}	 
	where we used that $(I_n-P_\alpha)$ is a projection matrix and hence $\lVert I_n-P_\alpha \rVert^2 = 1$.
	Inserting (\ref{proof_CV_2}) into (\ref{proof_CV_1}) yields
	\begin{align*}
	P\biggl(&\biggl|\frac{2}{n}\varepsilon'(I_n-P_\alpha)X_\alpha\beta_\alpha\biggr|>\delta\biggr) = O(n^{-1}),
	\end{align*}
	implying $\frac{2}{n}\varepsilon'(I_n-P_\alpha)X_\alpha\beta_\alpha=o_P(1)$. 
	%In the last inequality we used condition $(i)$ and that $(I_n-P_\alpha)$ is a projection matrix and hence $\lVert I_n-P_\alpha \rVert^2 = 1$.\\
	
	Finally, combining 
	\begin{itemize}
	\item $\frac{\varepsilon'\varepsilon}{n} = \sigma^2 + o_P(1)$
	\item $\frac{1}{n}\varepsilon'P_\alpha\varepsilon = o_P(1)$
	\item $\frac{2}{n}\varepsilon'(I_n-P_\alpha)X_\alpha\beta_\alpha=o_P(1)$
	\end{itemize}
	with (\ref{proof_CV(1)_2}) yields $\hat{\Gamma}_{\alpha,n}^{CV} = \sigma^2 + \Delta_{\alpha,n} + o_P(1)$ which is asymptotically equivalent to $\Gamma_{\alpha,n}$.This proves the first assertion.\\
	
	\textbf{Proof of $(II)$:} To show the second statement, assume that $\mathcal{M}_\alpha$ is in Category II.
	Using the linearization in (\ref{proof_CV(1)_1}) we get
	%ZEIGEN wie der zweite term zustande kommt
	\begin{align*}
	\hat{\Gamma}_{\alpha,n}^{CV(1)} &= \frac{1}{n}\sum_{i=1}^n (1- p_{ii\alpha})^{-2}(y_i-x_{i,\alpha}^\prime\hat{\beta}_\alpha)^2\\
	&= \frac{1}{n}\sum_{i=1}^n (1+2p_{ii\alpha} + o(p_{ii\alpha}^2))(y_i-x_{i,\alpha}^\prime\hat{\beta}_\alpha)^2\\
	&=\frac{1}{n}\sum_{i=1}^n (1 + \underbrace{o(p_{ii\alpha}^2)}_{=o(1)})(y_i-x_{i,\alpha}^\prime\hat{\beta}_\alpha)^2+\frac{2}{n}\sum_{i=1}^n p_{ii\alpha}(y_i-x_{i,\alpha}^\prime\hat{\beta}_\alpha)^2\\
	&= (1+o(1))\frac{1}{n}\lVert y-X_{\alpha}\hat{\beta}_\alpha\rVert^2 + \frac{2}{n}\sum_{i=1}^n p_{ii\alpha}(y_i-x_{i,\alpha}^\prime\hat{\beta}_\alpha)^2. 
	\end{align*}
	Repeating the same calculations as in (\ref{proof_CV(1)_2}) and using that for all $\alpha$ in Category II holds $\Delta_{\alpha,n}=0$ and $\varepsilon'(I_n-P_\alpha)X_\alpha\beta_\alpha =0$, yields
	\begin{align*}
	\hat{\Gamma}_{\alpha,n}^{CV} &=\frac{\varepsilon'\varepsilon}{n} + \Delta_{\alpha,n} - \frac{\varepsilon'P_\alpha\varepsilon}{n} +\frac{2}{n}\varepsilon'(I_n-P_\alpha) X_\alpha\beta_\alpha+ \frac{2}{n}\sum_{i=1}^n p_{ii\alpha}(y_i-x_{i,\alpha}^\prime\hat{\beta}_\alpha)^2 +o(1)\\
	&= \frac{\varepsilon'\varepsilon}{n}- \frac{\varepsilon'P_\alpha\varepsilon}{n} + \frac{2}{n}\sum_{i=1}^n p_{ii\alpha}(y_i-x_{i,\alpha}^\prime\hat{\beta}_\alpha)^2 +o(1).
	\end{align*}
	Thus it remains to show that $\sum_{i=1}^n p_{ii\alpha}(y_i-x_{i,\alpha}^\prime\hat{\beta}_\alpha)^2= d_\alpha \sigma^2 + o_P(1)$.
	To establish this result we first show that $\sum_{i=1}^n p_{ii\alpha}(y_i-x_{i,\alpha}^\prime\hat{\beta}_\alpha)^2= \sum_{i=1}^n p_{ii\alpha}\varepsilon_i^2 + o_P(1)$ and then we argue that $\sum_{i=1}^n p_{ii\alpha}\varepsilon_i^2=d_\alpha\sigma^2+o_P(1)$.\\
	
	(1.) One can rewrite 
	\begin{align}
	\sum_{i=1}^n p_{ii\alpha}(y_i-x_{i,\alpha}^\prime\hat{\beta}_\alpha)^2 &= (y-X_\alpha\hat{\beta}_\alpha)'\mathrm{diag}(P_\alpha)(y-X_\alpha\hat{\beta}_\alpha) 
	= \varepsilon'(I_n-P_\alpha)\mathrm{diag}(P_\alpha)(I_n-P_\alpha)\varepsilon\nonumber\\
	&= \varepsilon'\mathrm{diag}(P_\alpha)\varepsilon-2\varepsilon'\mathrm{diag}(P_\alpha)P_\alpha\varepsilon+\varepsilon'P_\alpha\mathrm{diag}(P_\alpha)P_\alpha \varepsilon. \label{schawarma_king}
	\end{align}
	$\varepsilon'\mathrm{diag}(P_\alpha)P_\alpha\varepsilon$ vanishes in probability as
	\begin{align*}
	\varepsilon'\mathrm{diag}(P_\alpha)P_\alpha\varepsilon= \sum_{i=1}^n\sum_{j=1}^np_{ii\alpha}p_{ij\alpha}\varepsilon_i\varepsilon_j
	=\sum_{i}p_{ii\alpha}^2\varepsilon_i^2 + \sum_{i}\sum_{j\neq i}p_{ii\alpha}p_{ij\alpha}\varepsilon_i\varepsilon_j=o_P(1).
	\end{align*}
	The first part converges to zero by Markov inequality 
	\begin{align*}
	P\biggl(\biggl|\sum_{i}p_{ii\alpha}^2\varepsilon_i^2\biggr|>\delta\biggr)
	\le \frac{1}{\delta}\sum_{i}p_{ii\alpha}^2\mathrm{E}\bigl[\varepsilon_i^2\bigr]
	\le \frac{\sigma^2}{\delta}\biggl(\max_{1\le i\le n}p_{ii\alpha}\biggr) \sum_{i}p_{ii\alpha} 
	= \frac{d_\alpha\sigma^2}{\delta}\max_{1\le i\le n}p_{ii\alpha} \to 0 
	\end{align*}
	and the second part similarly 
	\begin{align*}
	P\biggl(\biggl|\sum_{i}&\sum_{j\neq i}p_{ii\alpha}p_{ij\alpha}\varepsilon_i\varepsilon_j\biggr|>\delta\biggr)\le\frac{1}{\delta^2}\sum_{i}\sum_{j\neq i}p_{ii\alpha}^2p_{ij\alpha}^2\mathrm{E}\bigl[\varepsilon_i^2\varepsilon_j^2\bigr]
	= \frac{\sigma^4}{\delta^2}\sum_{i}p_{ii\alpha}^2\sum_{j\neq i}p_{ij\alpha}^2\\
	&\le \frac{\sigma^4}{\delta^2}\sum_{i}p_{ii\alpha}^3 \le \frac{d_\alpha\sigma^4}{\delta^2}\biggl(\max_{1\le i\le n}p_{ii\alpha}\biggr)^2 \to 0.
	\end{align*}
	Where we used that for the projection matrix $P_\alpha$ holds that
	%ANGeben warum das untere hält, wegen idempotenz und symmetrie von P_\alpha
	$\sum_{j}p_{ij\alpha}^2=p_{ii\alpha}$, due to idempotency and symmetry.\\
	
	The last part in (\ref{schawarma_king}) is asymptotically negligible as well since
	\begin{align*}
	\varepsilon'P_\alpha\mathrm{diag}(P_\alpha)P_\alpha \varepsilon = \sum_i\sum_j\sum_k p_{ii\alpha}p_{ij\alpha}p_{ik\alpha}\varepsilon_j\varepsilon_k\\
	=\sum_i p_{ii\alpha}^3 \varepsilon_i^2 +2\sum_i\sum_{j\neq i}p_{ii\alpha}^2p_{ij\alpha}\varepsilon_i\varepsilon_j + 
	\sum_i\sum_{j\neq i}\sum_{k\neq i}p_{ii\alpha}p_{ij\alpha}p_{ik\alpha}\varepsilon_j\varepsilon_k=o_P(1).
	\end{align*}
	Its first term vanishes as $0\le \sum_i p_{ii\alpha}^3 \varepsilon_i^2 \le \sum_i p_{ii\alpha}^2 \varepsilon_i^2 = o_P(1)$.
	For the second term holds
	\begin{align*}
	P\biggl(\biggl|\sum_{i}&\sum_{j\neq i}p_{ii\alpha}^2p_{ij\alpha}\varepsilon_i\varepsilon_j\biggr|>\delta\biggr)\le\frac{1}{\delta^2}\sum_{i}\sum_{j\neq i}p_{ii\alpha}^4p_{ij\alpha}^2\mathrm{E}\bigl[\varepsilon_i^2\varepsilon_j^2\bigr]
	= \frac{\sigma^4}{\delta^2}\sum_{i}p_{ii\alpha}^4\sum_{j\neq i}p_{ij\alpha}^2\\
	&\le \frac{\sigma^4}{\delta^2}\sum_{i}p_{ii\alpha}^5 \le \frac{d_\alpha\sigma^4}{\delta^2}\biggl(\max_{1\le i\le n}p_{ii\alpha}\biggr)^4 \to 0.
	\end{align*}
	Third term
	\begin{align*}
	P\biggl(\biggl|\sum_i&\sum_{j\neq i}\sum_{k\neq i}p_{ii\alpha}p_{ij\alpha}p_{ik\alpha}\varepsilon_j\varepsilon_k\biggr|>\delta\biggr)\le\frac{1}{\delta^2}\sum_i\sum_{j\neq i}\sum_{k\neq i}p_{ii\alpha}^2p_{ij\alpha}^2p_{ik\alpha}^2\mathrm{E}\bigl[\varepsilon_j^2\varepsilon_k^2\bigr]\\
	&\le \frac{\sigma^4}{\delta^2} \sum_i p_{ii\alpha}^4 \le \frac{d_\alpha\sigma^4}{\delta^2}\biggl(\max_{1\le i\le n}p_{ii\alpha}\biggr)^3 \to 0.
	\end{align*}
	Hence we have shown that (\ref{schawarma_king}) is 
	\begin{align*}
	\sum_{i=1}^n p_{ii\alpha}(y_i-x_{i,\alpha}^\prime\hat{\beta}_\alpha)^2 = \varepsilon'\mathrm{diag}(P_\alpha)\varepsilon + o_P(1).
	\end{align*}
	
	(2.)To show that $\varepsilon'\mathrm{diag}(P_\alpha)\varepsilon=d_\alpha \sigma^2+o_P(1)$ we use the following truncation technique:\\
	Let $\delta>0$ and $C_n=O((\max p_{ii\alpha})^{-0.5})$, then 
	\begin{align*}
	P\biggl(\biggl|\sum_{i=1}^n p_{ii\alpha} (\varepsilon_i^2-\sigma^2)\biggr|>\delta\biggr)&\le
	P\biggl(\biggl|\sum_{i=1}^n p_{ii\alpha}(\varepsilon_i^2-\sigma^2)\mathbbm{1}(|(\varepsilon_i^2-\sigma^2)|\le C_n)\biggr|>\frac{\delta}{2}\biggr)\\
	&+  P\biggl(\biggl|\sum_{i=1}^n p_{ii\alpha}(\varepsilon_i^2-\sigma^2)\mathbbm{1}(|(\varepsilon_i^2-\sigma^2)|> C_n)\biggr|>\frac{\delta}{2}\biggr).
	\end{align*}
	By Cebysev's inequality and using that the $\varepsilon_i$ are iid, we can bound the first probability on the right hand side by
	%CEBY mit g(x)=x^2
	\begin{align*}
	P\biggl(\biggl|\sum_{i=1}^n &p_{ii\alpha}(\varepsilon_i^2-\sigma^2)\mathbbm{1}\bigl(|(\varepsilon_i^2-\sigma^2)|\le C_n\bigr)\biggr|>\frac{\delta}{2}\biggr)\le \frac{4}{\delta^2}\sum_{i=1}^n p_{ii\alpha}^2\mathrm{E}\bigl[|\varepsilon_i^2-\sigma^2|^2\mathbbm{1}\bigl(|(\varepsilon_i^2-\sigma^2)|\le C_n\bigr)\bigr]\\
	&\le \frac{4}{\delta^2}\bigl(\max_{1\le i\le n}p_{ii\alpha}\bigr)\sum_{i=1}^n p_{ii\alpha}\mathrm{E}\bigl[|\varepsilon_i^2-\sigma^2|^2\mathbbm{1}\bigl(|(\varepsilon_i^2-\sigma^2)|\le C_n\bigr)\bigr]\\
	&= \frac{4}{\delta^2}\bigl(\max_{1\le i\le n}p_{ii\alpha}\bigr)\underbrace{\mathrm{E}\bigl[|\varepsilon_1^2-\sigma^2|^2\mathbbm{1}\bigl(|(\varepsilon_1^2-\sigma^2)|\le C_n\bigr)\bigr]}_{\le C_n\mathrm{E}[|\varepsilon_1^2-\sigma^2|]}\sum_{i=1}^n p_{ii\alpha}\\
	&\le \frac{4}{\delta^2}\bigl(\max_{1\le i\le n}p_{ii\alpha}\bigr)C_n\mathrm{E}\bigl[|\varepsilon_1^2-\sigma^2|\bigr]d_\alpha\\
	&\le \frac{8d_\alpha\sigma^2}{\delta^2}\bigl(\max_{1\le i\le n}p_{ii\alpha}\bigr)C_n = O\bigl(\sqrt{\max_{1\le i\le n}p_{ii\alpha}}\bigr).
	\end{align*}
	To see that the second probability on the right hand side vanishes, note that $|\varepsilon_1^2-\sigma^2|\mathbbm{1}\bigl(|(\varepsilon_1^2-\sigma^2)|> C_n\bigr)\to 0$ and $|\varepsilon_1^2-\sigma^2|\mathbbm{1}\bigl(|(\varepsilon_1^2-\sigma^2)|> C_n\bigr)\le |\varepsilon_1^2-\sigma^2|$. Hence by the dominated convergence theorem it holds that $\mathrm{E}\bigl[|\varepsilon_1^2-\sigma^2|\mathbbm{1}\bigl(|(\varepsilon_1^2-\sigma^2)|> C_n\bigr)\bigr] \to 0$. Applying the Markov inequality at the second probability yields
	\begin{align*}
	P\biggl(\biggl|\sum_{i=1}^n &p_{ii\alpha}(\varepsilon_i^2-\sigma^2)\mathbbm{1}(|(\varepsilon_i^2-\sigma^2)|> C_n)\biggr|>\frac{\delta}{2}\biggr) \le \frac{2}{\delta}\sum_{i=1}^n p_{ii\alpha}\mathrm{E}\bigl[|\varepsilon_i^2-\sigma^2|\mathbbm{1}\bigl(|(\varepsilon_i^2-\sigma^2)|\le C_n\bigr)\bigr]\\
	&\le  \frac{2}{\delta}\mathrm{E}\bigl[|\varepsilon_1^2-\sigma^2|\mathbbm{1}\bigl(|(\varepsilon_1^2-\sigma^2)|\le C_n\bigr)\bigr]\sum_{i=1}^n p_{ii\alpha} \\
	&=  \frac{2d_\alpha}{\delta}\mathrm{E}\bigl[|\varepsilon_1^2-\sigma^2|\mathbbm{1}\bigl(|(\varepsilon_1^2-\sigma^2)|\le C_n\bigr)\bigr] \to 0
	\end{align*}
	establishing that $\sum_{i=1}^n p_{ii\alpha}\varepsilon_i^2= d_\alpha \sigma^2 + o_P(1)$.
	Combining the results from (1.) and (2.) we obtain $\hat{\Gamma}_{\alpha,n}^{CV} =\frac{1}{n}\varepsilon'\varepsilon- \frac{1}{n}\varepsilon'P_\alpha\varepsilon + \frac{2}{n}d_\alpha\sigma^2 +o_P(n^{-1})$
	proving the second claim.\\

\textbf{Proof of $(III)$:}
Since $(I)$ and $(II)$ imply that $\hat{\Gamma}_{\alpha,n}^{CV}$ consistently estimates $\Gamma_{\alpha,n}$, it suffices to show that $CV(1)$ asymptotically wouldn't choose any model in Category $I$, if we compare the $\Gamma_{\alpha,n}$. 
Note that for any model in Category $II$ holds
\begin{align*}
\lim_{n\to \infty} \Gamma_{\alpha,n} = \sigma^2 
\end{align*}
and for any model in Category $I$
\begin{align*}
\liminf_{n\to \infty} \Gamma_{\alpha,n} = \sigma^2 + \liminf_{n \to \infty} \Delta_{\alpha,n} >  \sigma^2,
\end{align*}
by condition (\ref{liminf_condition}). Hence, the expected squared prediction error for any model in Category $I$ is larger than the expected squared prediction error for models in category $II$. Or put differently, $CV(1)$ won't choose any model in Category $I$ asymptotically with probability approaching one.\\

\textbf{Proof of $(IV)$:}
Suppose that $\mathcal{M}_\ast$ is not of size $p$, i.e. $d_\ast < p$.
By $(II)$ 
\begin{align*}
\hat{\Gamma}_{\alpha,n}^{CV} = \frac{1}{n}\varepsilon'\varepsilon + \frac{2}{n}d_\alpha\sigma^2 - \frac{1}{n}\varepsilon'P_\alpha\varepsilon + o_P\biggl(\frac{1}{n}\biggr)
\end{align*}
By $(III)$ it suffices to look at models in Category $II$.
Furthermore 
\begin{align*}
&\{ \exists\alpha\in \textrm{Cat. $II$} \textrm{ s.t. $\mathcal{M}_\alpha$ is preferable to $\mathcal{M}_\ast$ w.r.t. $CV(1)$ at step $n$ }\} \\
= &\{ \exists\alpha\in \textrm{Cat. $II$}: \hat{\Gamma}_{\alpha,n}^{CV} < \hat{\Gamma}_{\ast,n}^{CV} \} \\
= &\biggl\{ \exists\alpha\in \textrm{Cat. $II$}: \frac{2}{n}(d_\alpha-d_\ast)\sigma^2 < \frac{1}{n}\varepsilon'(P_\alpha-P_\ast)\varepsilon + o_P\biggl(\frac{1}{n}\biggr) \biggr\}\\
= &\{ \exists\alpha\in \textrm{Cat. $II$}: 2(d_\alpha-d_\ast)\sigma^2 <\varepsilon'(P_\alpha-P_\ast)\varepsilon + o_P(1) \}
\end{align*}
And hence, 
\begin{align*}
&P(\mathcal{M}_{CV}\neq \mathcal{M}_\ast) \\
= &P\bigl(\exists\alpha\in \textrm{Cat. $II$} \textrm{ s.t. $\mathcal{M}_\alpha$ is preferable to $\mathcal{M}_\ast$ w.r.t. $CV(1)$ at step $n$ }\bigr)\\
= &P\bigl(\exists\alpha\in \textrm{Cat. $II$}: 2(d_\alpha-d_\ast)\sigma^2 <\varepsilon'(P_\alpha-P_\ast)\varepsilon\bigr) + o(1)\\
\ge &P\bigl(2(d_\alpha-d_\ast)\sigma^2 <\varepsilon'(P_\alpha-P_\ast)\varepsilon\bigr) + o(1), 
\end{align*}
for some arbitrary but fixed $\alpha\in \textrm{Cat. $II$}$.

To see that this probability doesn't converge to zero, assume for a moment that $\varepsilon\sim \mathcal{N}(0,\sigma^2\mathrm{I}_n)$. In equations (\ref{projection_start})-(\ref{projection_end}) we will show that $P_\alpha-P_\ast$ is an orthogonal projection onto $\mathrm{span}(X_\alpha)\cap \mathrm{span}(X_\ast)^\bot$. Hence, for any $n\in \mathbb{N}$, $\sigma^{-2}\varepsilon'(P_\alpha-P_\ast)\varepsilon$ is chi-squared distributed with $d_\alpha-d_\ast$ degrees of freedom and expected value $d_\alpha-d_\ast$. Therefore this probability will be strictly positive for any $n$.

To show that the same holds in our setting, we will show that $\sigma^{-2}\varepsilon'(P_\alpha-P_\ast)\varepsilon$ converges in distribution to a chi-square distributed random variable. To simplify our notation, we assume that all limits of the used matrix sequences exist.\footnote{Otherwise, because of condition $(i)$ of Theorem \ref{THM_Consistency of $CV(1)$}, we could argue for arbitrary clusterpoints along converging subsequences.}

Because of the conditions $(i)$ and $(ii)$, we can establish that
\begin{align*}
\underbrace{\left(
\begin{array}{c}
(X_\alpha'X_\alpha)^{-0.5} X_\alpha'\\
(X_\ast'X_\ast)^{-0.5}X_\ast'\\
\end{array}
\right)}_{\equiv B}
\varepsilon \overset{d}{\longrightarrow} Y \sim \mathcal{N}\left(0,\sigma^2 \lim_{n\to\infty}BB'\right)
\end{align*}
by using the Lindeberg-Feller central limit theorem.\footnote{cf. \cite{vandervaart}, Example 2.28, p.21}

By the continuous mapping theorem for the function
\begin{align*}
g(x) = x'\mathrm{diag}(\underbrace{1,\dots,1}_{\textrm{$d_\alpha$ times}},\underbrace{-1,\dots,-1}_{\textrm{$d_\ast$ times}})x,
\end{align*}
and by using that $B'\mathrm{diag}(1,\dots,1,-1,\dots,-1)B=P_\alpha-P_\ast$, we achieve 
\begin{align*}
\varepsilon'(P_\alpha-P_\ast)\varepsilon = g(B\varepsilon) \overset{d}{\longrightarrow} g(Y).
\end{align*}
Hence, it remains to show, that $g(Y)$ is chi-square distributed. 
Since we can obtain $Y$ as the limit of $Y_n=\sigma BZ_n$, where $Z_n\sim \mathcal{N}(0,\mathrm{I}_{2n})$, we will derive the distribution of $g(Y)$ as the limit distribution of 
\begin{align*}
g(Y_n) = g(\sigma BZ_n) = \sigma ^2 Z_n'B'\mathrm{diag}(1,\dots,1,-1,\dots,-1)BZ_n %\\
%=\sigma^2 Z'
%\left(
%\begin{array}{cc}
%X_\alpha(X_\alpha'X_\alpha)^{-0.5}  &
%X_\ast'(X_\ast'X_\ast)^{-0.5}
%\end{array}
%\right)
%\mathrm{diag}(1,\dots,1,-1,\dots,-1)
%\left(
%\begin{array}{c}
%(X_\alpha'X_\alpha)^{-0.5} X_\alpha'\\
%(X_\ast'X_\ast)^{-0.5}X_\ast'\\
%\end{array}
%\right)
%Z\\
= \sigma^2 Z_n'(P_\alpha-P_\ast)Z_n.
\end{align*}
If we can show that $P_\alpha-P_\ast$ is an orthogonal projection onto a linear subspace, whose dimension does not depend on $n$, then it would follow that\footnote{cf. \cite{vandervaart}, Lemma 17.1, p.242}
\begin{align}
\sigma^2\chi_{\mathrm{dim}}^2 \sim g(Y_n) \overset{d}{\longrightarrow}g(Y) \sim \sigma^2\chi_{\mathrm{dim}}^2.\label{lim_dist}
\end{align}
To see that $P_\alpha-P_\ast$ is an orthogonal projection, note that
\begin{align}
(P_\alpha-P_\ast)' = P_\alpha'-P_\ast'=P_\alpha-P_\ast \label{projection_start}
\end{align}
and 
\begin{align}
(P_\alpha-P_\ast)(P_\alpha-P_\ast) 
&= P_\alpha P_\alpha-P_\alpha P_\ast-P_\ast P_\alpha+P_\ast P_\ast \nonumber\\
&=P_\alpha - \underbrace{P_\alpha X_\ast}_{=X_\ast}(X_\ast'X_\ast)^{-1}X_\ast' - X_\ast (X_\ast'X_\ast)^{-1}\underbrace{X_\ast'P_\alpha}_{=(P_\alpha X_\ast=X_\ast} +P_\ast\\
&=P_\alpha-P_\ast,\nonumber
\end{align}
where we exploited that $\mathrm{span}(X_\ast)\subseteq \mathrm{span} (X_\alpha)$ and hence $P_\alpha X_\ast=X_\ast$.

Furthermore, $P_\alpha-P_\ast$ projects onto $\mathrm{span}(X_\alpha)\cap \mathrm{span}(X_\ast)^\bot$, since $\mathbb{R}^n$ can be partitioned into
\begin{align}
\mathbb{R}^n&=\mathrm{span}(X_\alpha)^\bot \cup\mathrm{span}(X_\alpha)\nonumber\\
&=\mathrm{span}(X_\alpha)^\bot \cup\bigl[\mathrm{span}(X_\alpha)\cap \bigl(\mathrm{span}(X_\ast)\cap \mathrm{span}(X_\ast)^\bot\bigr)\bigr]\\
&=\mathrm{span}(X_\alpha)^\bot \cup \mathrm{span}(X_\ast) \cup \bigl[\mathrm{span}(X_\alpha)\cap \mathrm{span}(X_\ast)^\bot\bigr]\nonumber
\end{align} 
and
\begin{align}
\forall y\in \mathrm{span}(X_\alpha)^\bot: && (P_\alpha-P_\ast)y=\underbrace{P_\alpha y}_{=0}-\underbrace{P_\ast y}_{=0}=0\nonumber\\
\forall y\in \mathrm{span}(X_\ast): && (P_\alpha-P_\ast)y=\underbrace{P_\alpha y}_{=y}-\underbrace{P_\ast y}_{=y}=0\label{projection_end}\\
\forall y\in \mathrm{span}(X_\alpha)\cap \mathrm{span}(X_\ast)^\bot: && (P_\alpha-P_\ast)y=\underbrace{P_\alpha y}_{=y}-\underbrace{P_\ast y}_{=0}=y.\nonumber
\end{align}
Moreover $d_\alpha-d_\ast$, the dimension of $\mathrm{span}(X_\alpha)\cap \mathrm{span}(X_\ast)^\bot$, does not depend on $n$.

According to (\ref{lim_dist}), we have established that $\sigma^{-2}g(Y)$ is chi-square distributed with $d_\alpha-d_\ast$ degrees of freedo, and hence
\begin{align*}
P\bigl(2(d_\alpha-d\ast)\sigma^2 < \varepsilon'(P_\alpha-P-\ast)\varepsilon\bigr)
=P\bigl(2(d_\alpha-d_\ast)<\chi_{d_\alpha-d_\ast}^2\bigr)+o(1),
\end{align*}
proving $(IV)$.
\end{proof}

\begin{claim}\label{claim_CV(1)_conditions_2}
	If $X'X=O(n)$ and $(X'X)^{-1}=O(n^{-1})$, then $\liminf_{n\to \infty} \Delta_{\alpha,n} >0$.
\end{claim}
\begin{proof}[Proof of Claim \ref{claim_CV(1)_conditions_2}:]~\\~
Since $(I-P_\alpha)$ is an orthogonal projection onto $\mathrm{span}(X_\alpha)^\bot$, we can bound
\begin{align*}
\Delta_{\alpha,n} 
&= \frac{1}{n}\beta'X'(I-P_\alpha)X\beta\\
&= \frac{1}{n}\beta'X_{\alpha^c}'(I-P_\alpha)X_{\alpha^c}\beta\\
&\ge \frac{1}{n}\min_{z\in Z}\frac{z'X_{\alpha^c}'(I-P_\alpha)X_{\alpha^c}z}{z'z}\lVert\beta \rVert^2\\
&= \min_{z\in Z}\frac{z'(\frac{1}{n}X_{\alpha^c}'X_{\alpha^c})z}{z'z}\lVert\beta \rVert^2,
\end{align*}
where $Z=\{y\in \mathbb{R}^{p-d_\alpha}|X_{\alpha^c}y\in \mathrm{span}(X_\alpha)^\bot\}\setminus \{0\}$. Since $Z\subseteq \mathbb{R}^{p-d_\alpha}\setminus \{0\}$, we can further bound
\begin{align*}
\min_{z\in Z}\frac{z'(\frac{1}{n}X_{\alpha^c}'X_{\alpha^c})z}{z'z} 
\ge \min_{z\in \mathbb{R}^{p-d_\alpha}\setminus \{0\} }\frac{z'(\frac{1}{n}X_{\alpha^c}'X_{\alpha^c})z}{z'z}
= \lambda_{\min}\left(\frac{X_{\alpha^c}'X_{\alpha^c}}{n}\right)
\ge \lambda_{\min}\left(\frac{X'X}{n}\right),
\end{align*}
where $\lambda_{\min}(A)$ denotes the smallest eigenvalue of the matrix $A$.

Next, we will show that $(X'X)^{-1}=O(n^{-1})$ implies that these smallest eigenvalues are asymptotically bounded away from zero. Therefore, by using the spectral norm to rewrite $(X'X)^{-1}=O(n^{-1})$ yields 
\begin{align*}
\limsup_{n\to \infty} \max_{z\in \mathbb{R}^p\setminus \{0\}} \frac{z'(\frac{1}{n}X'X)^{-1}z}{z'z}=C <\infty.
\end{align*}
Since furthermore 
\begin{align*}
\max_{z\in \mathbb{R}^p\setminus \{0\}} \frac{z'(\frac{1}{n}X'X)^{-1}z}{z'z} = \frac{1}{\lambda_{\min}(\frac{1}{n}X'X)},
\end{align*}
we conclude
\begin{align*}
\limsup_{n\to\infty} \frac{1}{\lambda_{\min}(\frac{1}{n}X'X)} &= \frac{1}{\liminf_{n\to\infty}\lambda_{\min}(\frac{1}{n}X'X)}=C\\
\iff \quad \liminf_{n\to\infty}\lambda_{\min}\left(\frac{X'X}{n}\right) &= \frac{1}{C} > 0
\end{align*}
proving the claim.
\end{proof}

\begin{claim}\label{claim_CV(1)_conditions}
	If $X'X=O(n)$ and $(X'X)^{-1}=O(n^{-1})$, then for any $\alpha \in \mathcal{A}$
	\begin{align*}
		\lim_{n\to \infty} \max_{1\le i \le p} p_{ii\alpha}=0  \quad \iff \quad 
		 \lim_{n\to \infty} n^{-1}\max_{1\le i \le n} \lVert x_{\alpha,i}\rVert ^2 = 0.
	\end{align*}
\end{claim}
\begin{proof}[Proof of Claim \ref{claim_CV(1)_conditions}:]~\\~
	For arbitrary $\alpha \in \mathcal{A}$ holds
	\begin{align*}
	\lim_{n\to\infty} \max_{1\le i\le n} p_{ii\alpha} &= \lim_{n\to\infty} \max_{1\le i\le n} x_{i,\alpha}'(X_\alpha'X_\alpha)^{-1} x_{i,\alpha} \\
	&= \lim_{n\to\infty} \max_{1\le i\le n} \frac{1}{n} x_{i,\alpha}'\biggl(\frac{X_\alpha'X_\alpha}{n}\biggr)^{-1} x_{i,\alpha} \\
	&\le \lim_{n\to\infty} \max_{1\le i\le n} \frac{1}{n} \underbrace{\max_{z\neq 0} \frac{z'\bigl(\frac{X_\alpha'X_\alpha}{n}\bigr)^{-1}z}{z'z}}_{\le C^\ast < \infty \textrm{ for $n$ sufficiently large, by $(X'X)^{-1}=O(n^{-1})$} }  \lVert x_{i,\alpha} \rVert ^2 \\
	&\le C^\ast \lim_{n\to \infty} \max_{1\le i \le n} \frac{\lVert x_{i,\alpha}\rVert ^2}{n}
	\end{align*}
	and 
	\begin{align*}
	\lim_{n\to \infty} \max_{1\le i \le n} p_{ii\alpha} &=  \lim_{n\to\infty} \max_{1\le i\le n} \frac{1}{n} x_{i,\alpha}'\biggl(\frac{X_\alpha'X_\alpha}{n}\biggr)^{-1} x_{i,\alpha} \\
	&\ge \lim_{n\to\infty} \max_{1\le i\le n} \frac{1}{n} \underbrace{\min_{z\neq 0} \frac{z'\bigl(\frac{X_\alpha'X_\alpha}{n}\bigr)^{-1}z}{z'z}}_{\ge C_\ast > 0 \textrm{ for $n$ sufficiently large, by $X'X=O(n)$}} \lVert x_{i,\alpha} \rVert ^2\\
	&\ge C_\ast \lim_{n\to\infty} \max_{1\le i\le n} \frac{\lVert x_{i,\alpha}\rVert^2}{n}.
	\end{align*}
	Hence, 
	\begin{align*}
	\lim_{n\to \infty} \max_{1\le i \le n} p_{ii\alpha} = 0 \iff \lim_{n\to\infty} \max_{1\le i\le n} \frac{\lVert x_{i,\alpha}\rVert^2}{n} = 0.
	\end{align*}
\end{proof}
\addcontentsline{toc}{subsection}{Proof Asymptotic Properties of $BICV(n_\nu)$}
\subsection*{Proof Asymptotic Properties of $BICV(n_\nu)$}
For the proof of Theorem \ref{THM_Consistency_BICV}, we need the following Claim
\begin{claim}~
	\label{Claim_BICV}
	\begin{enumerate}[(I)]
		\item $\#\{s\in \mathcal{B}| i\in s\} = \frac{n_v}{n} b$ 
		\item $\#\{s\in\mathcal{B}|(i,j)\in s, j>i\}=n_vb\frac{n_v-1}{n(n-1)}$
	\end{enumerate}
\end{claim}
\begin{proof}[Proof of Claim \ref{Claim_BICV}]~\\
	\textbf{Proof of $(I)$:}\\
	$\mathcal{B}$ can be represented as a $n\times b$ matrix containing only zeros and ones. Then one interprets each column as a subset of $\{ 1,\dots,n\}$. Since these subsets have size $n_v$, it follows that $\sum_i^n \mathcal{B}_{i,j}=n_v$ for any $j= 1, \dots ,b$, where $\mathcal{B}_{i,j}$ denotes the $(i,j)$ element of $\mathcal{B}$. Hence
	\begin{align*}
	\sum_{j=1}^b\underbrace{\sum_{i=1}^n \mathcal{B}_{i,j}}_{=n_v} = bn_v.
	\end{align*}
	Furthermore, by condition $(a)$: $\sum_j^b \mathcal{B}_{i,j} = \#\{s\in \mathcal{B}| i\in s\}$ is independent of $i$. Thus 
	\begin{align*}
	bn_v = \sum_{i=1}^n\underbrace{\sum_{j=1}^b \mathcal{B}_{i,j}}_{=\#\{s\in \mathcal{B}| i\in s\}} \quad
	\iff \quad \#\{s\in \mathcal{B}| i\in s\} = \frac{n_v}{n}b. 
	\end{align*} 
	\textbf{Proof of $(II)$:}\\
	As $\mathcal{B}$ only consists of zeros and ones, one can count the subsets $s$ for which this condition is true by calculating the inner product of the $i$th and $j$th row, i.e.
	\begin{align*}
	\#\{s\in\mathcal{B}|(i,j)\in s, j>i\} = \sum_{k=1}^b\mathcal{B}_{i,k}\mathcal{B}_{j,k}.
	\end{align*}
	As any subset $s \in \mathcal{B}$ contains $n_v$ elements, it holds that
	\begin{align*}
	\sum_{k=1}^b\sum_{i=1}^n\sum_{j=1}^n\mathcal{B}_{i,k}\mathcal{B}_{j,k}= \sum_{k=1}^b\underbrace{\sum_{i=1}^n\mathcal{B}_{i,k}}_{=n_v}\underbrace{\sum_{j=1}^n\mathcal{B}_{j,k}}_{=n_v}=bn_v^2
	\end{align*}
	and as $\mathcal{B}_{i,k}^2=\mathcal{B}_{i,k}$
	\begin{align*}
	\sum_{k=1}^b\sum_{i=1}^n\sum_{j=1}^n\mathcal{B}_{i,k}\mathcal{B}_{j,k} &= \sum_{k=1}^b\underbrace{\sum_{i=1}^n\mathcal{B}_{i,k}^2}_{=n_v} +\sum_{k=1}^b\sum_{i=1}^n\sum_{j\neq i}^n\mathcal{B}_{i,k}\mathcal{B}_{j,k}\\
	\Rightarrow \quad \sum_{k=1}^b\sum_{i=1}^n\sum_{j\neq i}^n\mathcal{B}_{i,k}\mathcal{B}_{j,k} &= bn_v(n_v-1).
	\end{align*}
	Finally, by the balance property, $\#\{s\in\mathcal{B}|(i,j)\in s, j>i\}$ is independent of $i$ and $j$ and therefore it holds that
	\begin{align*}
	bn_v(n_v-1) = \sum_{i=1}^n\sum_{j\neq i}^n\underbrace{\sum_{k=1}^b\mathcal{B}_{i,k}\mathcal{B}_{j,k}}_{\#\{s\in\mathcal{B}|(i,j)\in s, j>i\}}&=\#\{s\in\mathcal{B}|(i,j)\in s, j>i\}\underbrace{\sum_{i=1}^n\sum_{j\neq i}^n}_{n(n-1)}\\
	\iff \#\{s\in\mathcal{B}|(i,j)\in s, j>i\}&=n_vb\frac{n_v-1}{n(n-1)}.
	\end{align*}
\end{proof} 

%THM proof of (III) fehlt noch
\begin{proof}[Proof of Theorem \ref{THM_Consistency_BICV}]~\\
	\textbf{Proof of $(I)$:}\\
	At first, we will bound $\hat{\Gamma}_{\alpha,n}^{BICV}$ from below in the following way
	\begin{align*}
	\hat{\Gamma}_{\alpha,n}^{BICV} 
	= \frac{1}{n_vb}\sum_{s\in \mathcal{B}}\lVert (I_{n_v}-Q_{\alpha,s})^{-1}(y_s-X_{\alpha,s}\hat{\beta}_\alpha)\rVert^2 
	\ge \frac{1}{n_vb}\sum_{s\in \mathcal{B}}\lVert (y_s-X_{\alpha,s}\hat{\beta}_\alpha)\rVert^2 
	%= \frac{1}{n}\lVert y-X_{\alpha}\hat{\beta}_\alpha\rVert^2.
	\end{align*}
	To see that this bound is valid, fix some arbitrary $a\in \mathbb{R}^{d_\alpha}\setminus \{ 0\}$ and note that by definition of the minimum
	\begin{align}
	\lVert (I_{n_v}-Q_{\alpha,s})^{-1} a\rVert ^2 
	&\ge \min_{z\in \mathbb{R}^n : \lVert z\rVert=\lVert a\rVert}z'(I_{n_v}-Q_{\alpha,s})^{-2}z \nonumber\\
	&= \lVert a\rVert^2 \min_{z\in \mathbb{R}^n : \lVert z\rVert=1}z'(I_{n_v}-Q_{\alpha,s})^{-2}z. \label{proof_BICV_7}
	\end{align}
	There is a close relation between the minimum problem on the right and the maximal eigenvalue of $I_{n_v}-Q_{\alpha,s}$, denoted by $\lambda_{\max}(I_{n_v}-Q_{\alpha,s})$, namely \footnote{see e.g. \cite{ryabenkii}, THM 5.2, p.130}
	\begin{align}
	\min_{z\in \mathbb{R}^n : \lVert z\rVert=1}z'(I_{n_v}-Q_{\alpha,s})^{-2}z = \frac{1}{(\lambda_{\max}(I_{n_v}-Q_{\alpha,s}))^2}\label{proof_BICV_8}
	\end{align}
	and 
	\begin{align*}
	\max_{z\in \mathbb{R}^{n_v} : \lVert z\rVert=1}z'(I_{n_v}-Q_{\alpha,s})z=\lambda_{\max}(I_{n_v}-Q_{\alpha,s}).
	\end{align*}
	We can bound $\lambda_{\max}(I_{n_v}-Q_{\alpha,s})$ by rewriting this maximum problem as
	\begin{align*}
	\max_{z\in \mathbb{R}^{n_v} : \lVert z\rVert=1}z'(I_{n_v}-Q_{\alpha,s})z 
	= \max_{z\in \mathbb{R}^{n_v} : \lVert z\rVert=1}z'z - z'(Q_{\alpha,s})z 
	= 1- \min_{z\in \mathbb{R}^{n_v} : \lVert z\rVert=1}z'(Q_{\alpha,s})z 
	\end{align*}
	and using the relation between $Q_{\alpha,s}$ and the projection matrix $P_\alpha$
	\begin{align*}
	\min_{z\in \mathbb{R}^{n_v} : \lVert z\rVert=1}z'Q_{\alpha,s}z
	= \min_{z\in \mathbb{R}^n : \lVert z\rVert=1, z_{s^c}=0}z'P_\alpha z
	\ge \min_{z\in \mathbb{R}^n : \lVert z\rVert=1}z'P_\alpha'z = 0.
	\end{align*}
	Combining these results, yield the upper bound
	\begin{align*}
	\lambda_{\max}(I_{n_v}-Q_{\alpha,s})
	=1- \min_{z\in \mathbb{R}^{n_v} : \lVert z\rVert=1}z'(Q_{\alpha,s})z \le 1.
	\end{align*}
	After inserting this and (\ref{proof_BICV_8}) back into (\ref{proof_BICV_7}), we obtain
	\begin{align*}
	\lVert (I_{n_v}-Q_{\alpha,s})^{-1} a\rVert ^2 
	&\ge \lVert a\rVert^2 \min_{z\in \mathbb{R}^n : \lVert z\rVert=1}z'(I_{n_v}-Q_{\alpha,s})^{-2}z\\
	&=  \lVert a\rVert^2\frac{1}{(\lambda_{\max}(I_{n_v}-Q_{\alpha,s}))^2} 
	\ge \lVert a\rVert^2.
	\end{align*}
	Thus, the claimed bound is valid.
	
	Next, we use the balance property of $\mathcal{B}$ and Claim \ref{Claim_BICV} to rewrite the lower bound of $\hat{\Gamma}_{\alpha,n}^{BICV}$ 
	\begin{align*}
	\frac{1}{n_vb}\sum_{s\in \mathcal{B}}\lVert (y_s-X_{\alpha,s}\hat{\beta}_\alpha)\rVert^2
	&= \frac{1}{n_vb}\sum_{s\in \mathcal{B}}\sum_{i \in s} (y_i-x_{\alpha,i}'\hat{\beta}_\alpha)^2\\
	&= \frac{1}{n_vb}\sum_{i=1}^n \underbrace{\#\{s\in \mathcal{B}| i\in s\}}_{=\frac{n_v}{n}b} (y_i-x_{\alpha,i}'\hat{\beta}_\alpha)^2\\
	&=\frac{1}{n}\sum_{i=1}^n(y_i-x_{\alpha,i}'\hat{\beta}_\alpha)^2\\
	&= \frac{1}{n} \lVert y-X_\alpha\hat{\beta}_\alpha\rVert ^2
	\end{align*}
	and hence
	\begin{align*}
	\hat{\Gamma}_{\alpha,n}^{BICV} \ge \frac{1}{n} \lVert y-X_\alpha\hat{\beta}_\alpha\rVert ^2.
	\end{align*}
	Finally, by the proof of statement $(I)$ in Theorem \ref{THM_Consistency of $CV(1)$} this is equivalent to
	\begin{align*}
	\hat{\Gamma}_{\alpha,n}^{BICV} \ge \frac{1}{n}\varepsilon'\varepsilon + \Delta_{\alpha,n} + o_P(1).
	\end{align*}
	Now define $R_n = \hat{\Gamma}_{\alpha,n}^{BICV} - n^{-1}\lVert y-X_{\alpha}\hat{\beta}_\alpha\rVert^2$. This proves the first assertion.
	\\
	
	%DER proof-teil ist unübersichtlich, eventuelle einigen bereichen eine nummer geben und beschreiben mit ein bisschen text was gerade gemacht wird
	\textbf{Proof of $(II)$:} \\
	To prove the second statement, we use a similar argument as in the proof of Theorem \ref{THM_Consistency of $CV(1)$} $(II)$, i.e. we will show that
	\begin{align}
	\hat{\Gamma}_{\alpha,n}^{BICV} = \frac{\varepsilon'\varepsilon}{n}-\frac{\varepsilon'P_\alpha\varepsilon}{n}+\frac{1}{n-n_v}\sum_{i=1}^np_{ii\alpha}e_i^2+o_P\left(\frac{1}{n-n_v}\right) \quad \forall \alpha\in \textrm{Cat. $II$}. \label{proof_BICV_1}
	\end{align}
	Since we have already shown that $\sum_{i=1}^np_{ii\alpha}e_i^2=d_\alpha\sigma^2 +o_P(1)$ (see the proof of Theorem \ref{THM_Consistency of $CV(1)$} $(II)$), we can further simplify to
	\begin{align*}
	\hat{\Gamma}_{\alpha,n}^{BICV} = \frac{\varepsilon'\varepsilon}{n}-\frac{\varepsilon'P_\alpha\varepsilon}{n}+\frac{d_\alpha\sigma^2}{n-n_v}+o_P\left(\frac{1}{n-n_v}\right) \quad \forall \alpha\in \textrm{Cat. $II$},
	\end{align*}
	which is already quite close to the statement. It only differs in the $\varepsilon'P_\alpha\varepsilon$ term. But this term converges to zero, even if we multiply it with $n-n_v$. This can be seen by using the Markov inequality and the growth rate requirements in condition (\ref{growth_rates_nv_BICV}) as follows
	\begin{align*}
	P\left(\left\lvert \frac{n-n_v}{n}\varepsilon'P_\alpha\varepsilon\right\rvert > \delta\right)\le \frac{n-n_v}{n}\frac{1}{\delta}\mathrm{E}[\varepsilon'P_\alpha\varepsilon]=\frac{n-n_v}{n}\frac{d_\alpha\sigma^2}{\delta}\to 0,\quad\forall \delta >0
	\end{align*}
	i.e. $n^{-1}\varepsilon'P_\alpha\varepsilon=o_P((n-n_v)^{-1})$. This yields the statement
	\begin{align*}
	\hat{\Gamma}_{\alpha,n}^{BICV} = \frac{\varepsilon'\varepsilon}{n}+\frac{d_\alpha\sigma^2}{n-n_v}+o_P\left(\frac{1}{n-n_v}\right). \quad \forall \alpha\in \textrm{Cat. $II$}
	\end{align*}
	Thus it only remains to show that (\ref{proof_BICV_1}) holds true. Since the argument is quite lengthy, we split it into the following parts: 	
	First we will decompose $\hat{\Gamma}_{\alpha,n}^{BICV}$ to 
	\begin{align*}
	\hat{\Gamma}_{\alpha,n}^{BICV} = \frac{\varepsilon'\varepsilon}{n}-\frac{\varepsilon'P_\alpha\varepsilon}{n}+\frac{c_n}{n_v b}\sum_{s\in \mathcal{B}}e_s'P_{\alpha,s}e_s+B_\alpha,
	\end{align*}
	where $B_\alpha$ is the remainder term.
	Second we will show that $\frac{c_n}{n_v b}\sum_{s\in \mathcal{B}}e_s'P_{\alpha,s}e_s$ is close to $\frac{1}{n-n_v}\sum_{i=1}^np_{ii\alpha}e_i^2$ and in the last step we will show that the remainder term $B_\alpha$ is vanishing.
	
	(1.) Denote by $e_s=\varepsilon_s-X_{\alpha,s}(X_\alpha^\prime X_\alpha)^{-1}X_\alpha^\prime\varepsilon$ the OLS residuals on the validation set $s$. Then by using Lemma \ref{Equation3.1} we can rewrite $\hat{\Gamma}_{\alpha,n}^{BICV}$ as
	\begin{align*}
	\hat{\Gamma}_{\alpha,n}^{BICV} = \frac{1}{n_vb}\sum_{s\in \mathcal{B}}e_s'(I_{n_v}-Q_{\alpha,s})^{-1}(I_{n_v}-Q_{\alpha,s})^{-1}e_s.
	\end{align*}
	Adding a zero and rearranging yields
	\begin{align}
	\hat{\Gamma}_{\alpha,n}^{BICV} = &\frac{1}{n_vb}\sum_{s\in \mathcal{B}}e_s'(I_{n_v}-Q_{\alpha,s})^{-1}(U_{\alpha,s} + I_{n_v}-U_{\alpha,s})(I_{n_v}-Q_{\alpha,s})^{-1}e_s\nonumber\\
	= &\frac{1}{n_vb}\sum_{s\in \mathcal{B}}e_s'(I_{n_v}-Q_{\alpha,s})^{-1}U_{\alpha,s}(I_{n_v}-Q_{\alpha,s})^{-1}e_s\label{proof_BICV_2}\\
	&+\underbrace{\frac{1}{n_vb}\sum_{s\in \mathcal{B}}e_s'(I_{n_v}-Q_{\alpha,s})^{-1}(I_{n_v}-U_{\alpha,s})(I_{n_v}-Q_{\alpha,s})^{-1}e_s}_{\equiv B_\alpha},\nonumber
	\end{align}
	where $B_\alpha$ is the aforementioned remainder term, 
	\begin{align*}
	U_{\alpha,s}=(I_{n_v}-Q_{\alpha,s})(I_{n_v}+c_n P_{\alpha,s})(I_{n_v}-Q_{\alpha,s}),\\
	c_n = \frac{n_v(2n-n_v)}{(n-n_v)^2}\quad \textrm{and} \quad
	P_{\alpha,s}= X_{\alpha,s}(X_{\alpha,s}'X_{\alpha,s})^{-1}X_{\alpha,s}'.
	\end{align*}
	Inserting the definition of $U_{\alpha,s}$ into (\ref{proof_BICV_2}), yields
	\begin{align}
	\hat{\Gamma}_{\alpha,n}^{BICV}&=\frac{1}{n_vb}\sum_{s\in \mathcal{B}}e_s'\underbrace{(I_{n_v}-Q_{\alpha,s})^{-1}(I_{n_v}-Q_{\alpha,s})}_{=I_{n_v}}(I_{n_v}+c_n P_{\alpha,s})\underbrace{(I_{n_v}-Q_{\alpha,s})(I_{n_v}-Q_{\alpha,s})^{-1}}_{=I_{n_v}}e_s +B_\alpha\nonumber\\
	&=\frac{1}{n_vb}\sum_{s\in \mathcal{B}}e_s'e_s + \frac{c_n}{n_vb}\sum_{s\in \mathcal{B}}e_s'P_{\alpha,s}e_s+B_\alpha.\label{motivation_APCV_1}
	\end{align}
	Furthermore, by using the balance property of $\mathcal{B}$ and claim \ref{Claim_BICV}, we can simplify the first sum to
	\begin{align}
	\frac{1}{n_vb}\sum_{s\in \mathcal{B}}e_s'e_s
	= \frac{1}{n_vb}\sum_{s\in \mathcal{B}}\sum_{i\in s}e_i^2 
	= \frac{1}{n_vb}\frac{n_v}{n}b \sum_{i=1}^ne_i^2 = \frac{1}{n}\lVert y-X_\alpha\hat{\beta}_\alpha\rVert^2.\label{motivation_APCV_2}
	\end{align}
	Finally, if we use the same calculations as in (\ref{proof_CV(1)_2}), we obtain the desired representation
	\begin{align*}
	\hat{\Gamma}_{\alpha,n}^{BICV} = \frac{\varepsilon'\varepsilon}{n}-\frac{\varepsilon'P_\alpha\varepsilon}{n}+\frac{c_n}{n_v b}\sum_{s\in \mathcal{B}}e_s'P_{\alpha,s}e_s+B_\alpha.
	\end{align*}
	
	(2.) Next, we want to show that
	\begin{align*}
	\frac{c_n}{n_vb}\sum_{s\in\mathcal{B}}e_s'P_{\alpha,s}e_s \approx \frac{1}{n-n_v}\sum_{i=1}^np_{ii\alpha}e_i^2+o_P\left(\frac{1}{n-n_v}\right).
	\end{align*}
	To do so, we will first argue that, due to (\ref{gram_matrix_condition_BICV}), $P_{\alpha,s}$ is approximately $Q_{\alpha,s}$ and hence
	\begin{align*}
	\sum_{s\in\mathcal{B}}e_s'P_{\alpha,s}e_s
	\approx \sum_{s\in\mathcal{B}}e_s'Q_{\alpha,s}e_s
	= \sum_{s\in \mathcal{B}}\sum_{i\in s}\sum_{j\in s}p_{ij\alpha}e_ie_j,
	\end{align*}
	which itself is proportional to $\sum_{i=1}^n p_{ii\alpha}e_i^2$, due to the balance property of $\mathcal{B}$.
	
	To make this argument rigorous, consider the difference
	\begin{align*}
	\frac{1}{n}X_\alpha'X_\alpha - \frac{1}{n_v}X_{\alpha,s}'X_{\alpha,s}
	&=\frac{1}{n}X_{\alpha,s}'X_{\alpha,s} + \frac{1}{n}X_{\alpha,s^c}'X_{\alpha,s^c} - \frac{1}{n_v}X_{\alpha,s}'X_{\alpha,s}\\
	&=\frac{1}{n}X_{\alpha,s^c}'X_{\alpha,s^c} - \frac{n-n_v}{n}\frac{1}{n_v}X_{\alpha,s}'X_{\alpha,s}\\
	&=\frac{n-n_v}{n}\underbrace{\left[\frac{1}{n-n_v}X_{\alpha,s^c}'X_{\alpha,s^c} - \frac{1}{n_v}X_{\alpha,s}'X_{\alpha,s}\right]}_{=o(1)\textrm{, by (\ref{gram_matrix_condition_BICV})}}=o\left(\frac{n-n_v}{n}\right),
	\end{align*}
	where we used that $X_\alpha'X_\alpha =X_{\alpha,s}'X_{\alpha,s}+X_{\alpha,s^c}'X_{\alpha,s^c}$. Left multiplying $(X_{\alpha,s}'X_{\alpha,s})^{-1}$ and right multiplying $n(X_{\alpha}'X_{\alpha})^{-1}$ yields
	\begin{align*}
	(X_{\alpha,s}'X_{\alpha,s})^{-1}-\frac{n}{n_v}(X_{\alpha}'X_{\alpha})^{-1} = o\left(\frac{n-n_v}{n}\right) (X_{\alpha,s}'X_{\alpha,s})^{-1} n (X_{\alpha}'X_{\alpha})^{-1}.
	\end{align*}
	Furthermore, condition $(i)$ of Theorem \ref{THM_Consistency of $CV(1)$} implies $(X_{\alpha}'X_{\alpha})^{-1}=O(n^{-1})$, by which we can simplify the right hand side to
	\begin{align*}
	(X_{\alpha,s}'X_{\alpha,s})^{-1}-\frac{n}{n_v}(X_{\alpha}'X_{\alpha})^{-1} 
	&= o\left(\frac{n-n_v}{n}\right)O(1) (X_{\alpha,s}'X_{\alpha,s})^{-1}\\
	&= o\left(\frac{n-n_v}{n}\right)(X_{\alpha,s}'X_{\alpha,s})^{-1}.
	\end{align*}
	After solving for $(X_{\alpha}'X_{\alpha})^{-1}$, we obtain
	\begin{align*}
	(X_{\alpha}'X_{\alpha})^{-1} = \frac{n_v}{n}\left[1+o\left(\frac{n-n_v}{n}\right)\right](X_{\alpha,s}'X_{\alpha,s})^{-1}.
	\end{align*}
	Next, we left multiply $X_{\alpha,s}$ and right multiply $X_{\alpha,s}'$:
	\begin{align*}
	X_{\alpha,s}(X_{\alpha}'X_{\alpha})^{-1}X_{\alpha,s}' &= \frac{n_v}{n}\left[1+o\left(\frac{n-n_v}{n}\right)\right] X_{\alpha,s} (X_{\alpha,s}'X_{\alpha,s})^{-1}X_{\alpha,s}' ,
	\end{align*}
	which is by definition equivalent to
	\begin{align}
	Q_{\alpha,s} &= \frac{n_v}{n}\left[1+o\left(\frac{n-n_v}{n}\right)\right] P_{\alpha,s}.\label{proof_BICV_3}
	\end{align}
	Now, inserting (\ref{proof_BICV_3}) into $\frac{c_n}{n_vb}\sum_{s\in \mathcal{B}}e_s'P_{\alpha,s}e_s$ yields
	\begin{align}
	\frac{c_n}{n_vb}\sum_{s\in \mathcal{B}}e_s'P_{\alpha,s}e_s
	&=\left[1+o\left(\frac{n-n_v}{n}\right)\right]^{-1}\frac{n}{n_v}\frac{c_n}{n_vb}\sum_{s\in \mathcal{B}}e_s'Q_{\alpha,s}e_s\nonumber\\
	&=\left[1+o\left(\frac{n-n_v}{n}\right)\right]\frac{n}{n_v}\frac{c_n}{n_vb}\sum_{s\in \mathcal{B}}e_s'Q_{\alpha,s}e_s. \label{proof_BICV_4}
	\end{align}
	
	Next, we clarify what we mean by $\sum_{s\in \mathcal{B}}e_s'Q_{\alpha,s}e_s$ being proportional to $\sum_{i=1}^np_{ii\alpha}e_i^2$. To do so, spell out the sum and simplify in the following way:
	\begin{align}
	\frac{1}{n_vb}\sum_{s\in \mathcal{B}}e_s'Q_{\alpha,s}e_s
	&= \frac{1}{n_vb}\sum_{s\in \mathcal{B}}\sum_{i\in s}\sum_{j\in s}p_{ij\alpha}e_ie_j\nonumber\\
	&=\frac{1}{n_vb}\underbrace{\sum_{s\in \mathcal{B}} \sum_{i\in s}}_{\frac{n_v}{n}b\sum_{i=1}^n} p_{ii\alpha}e_i^2 
	+ 2\frac{1}{n_vb}\underbrace{\sum_{s\in \mathcal{B}} \sum_{i\in s}\sum_{j\in s, j>i }}_{\frac{n_v}{n}
	b\frac{n_v-1}{n-1}\sum_{i=1}^n\sum_{j>1}^n} p_{ij\alpha}e_ie_j\nonumber\\
	&=\frac{1}{n}\sum_{i=1}^np_{ii\alpha}e_i^2 + \frac{1}{n}\frac{n_v-1}{n-1}\sum_{i=1}^n\sum_{j\neq i}^np_{ij\alpha}e_ie_j\label{proof_BICV_5}
	\end{align}
	Where we used the balance property of $\mathcal{B}$ and Claim \ref{Claim_BICV}. 
	One can further simplify the second sum in the last line, since
	\begin{align*}
	\sum_{i=1}^n\sum_{j\neq i}^np_{ij\alpha}e_ie_j 
	&= \sum_{i=1}^n\sum_{j=1}^np_{ij\alpha}e_ie_j - \sum_{i=1}^np_{ii\alpha}e_i^2\\
	&= \sum_{i=1}^n\sum_{j=1}^np_{ij\alpha}(y_i-x_{\alpha,i}'\hat{\beta}_\alpha)(y_j-x_{\alpha,j}'\hat{\beta}_\alpha) - \sum_{i=1}^np_{ii\alpha}e_i^2\\
	&=\underbrace{(y-X_\alpha\hat{\beta}_\alpha)'P_\alpha(y-X_\alpha\hat{\beta}_\alpha)}_{=0} - \sum_{i=1}^np_{ii\alpha}e_i^2.
	\end{align*}
	After inserting this back into (\ref{proof_BICV_5}) and using that $c_n=\frac{n(2n-n_v)}{(n-n_v)^2}$, we obtain
	\begin{align}
	\frac{c_n}{n_vb}\sum_{s\in \mathcal{B}}e_s'Q_{\alpha,s}e_s 
	= \frac{c_n}{n}\left(1-\frac{n_v-1}{n-1}\right)\sum_{i=1}^np_{ii\alpha}e_i^2
	= \frac{1}{n-n_v}\frac{n_v}{n}\frac{2n-n_v}{n-1}\sum_{i=1}^np_{ii\alpha}e_i^2.\label{proof_BICV_6}
	\end{align}
	
	Finally, combining (\ref{proof_BICV_6}) with (\ref{proof_BICV_4}), yields the final result
	\begin{align*}
	\frac{c_n}{n_vb}\sum_{s\in \mathcal{B}}e_s'P_{\alpha,s}e_s
	&=\left[1+o\left(\frac{n-n_v}{n}\right)\right]\frac{n}{n_v}\frac{c_n}{n_vb}\sum_{s\in \mathcal{B}}e_s'Q_{\alpha,s}e_s\\
	&=\left[\underbrace{1+o\left(\frac{n-n_v}{n}\right)}_{=1+o(1)\textrm{, by (\ref{growth_rates_nv_BICV})}}\right]\frac{n}{n_v}\frac{1}{n-n_v}\frac{n_v}{n}\underbrace{\frac{2n-n_v}{n-1}}_{=1+o(1)\textrm{, by (\ref{growth_rates_nv_BICV})}}\sum_{i=1}^np_{ii\alpha}e_i^2\\
	&=\frac{1}{n-n_v}\sum_{i=1}^np_{ii\alpha}e_i^2 + o_P\left(\frac{1}{n-n_v}\right),
	\end{align*}
	where we used that $\sum_{i=1}^np_{ii\alpha}e_i^2=d_\alpha\sigma^2+o_P(1)$ and hence $\frac{1}{n-n_v}o(1)\sum_{i=1}^np_{ii\alpha}e_i^2=o_P(\frac{1}{n-n_v})$.\\\\
	
	(3.) It remains to show that  
	\begin{align*}
	B_\alpha
	=\frac{1}{n_vb}\sum_{s\in \mathcal{B}}e_s'(I_{n_v}-Q_{\alpha,s})^{-1}(I_{n_v}-U_{\alpha,s})(I_{n_v}-Q_{\alpha,s})^{-1}e_s
	= o_P\left(\frac{1}{n-n_v}\right),
	\end{align*}
	where $U_{\alpha,s} = (I_{n_v}-Q_{\alpha,s})(I_{n_v}+c_n P_{\alpha,s})(I_{n_v}-Q_{\alpha,s})$.
	
	Unlike in the second part of this proof, the $(I_{n_v}-Q_{\alpha,s})$ terms don't cancel themselves. 
	Therefore we use equation (\ref{proof_BICV_3}), i.e. $Q_{\alpha,s} = \left[\frac{n_v}{n}+o\left(\frac{n-n_v}{n}\right)\right] P_{\alpha,s}$, and expand the product 
	\begin{align*}
	U_{\alpha,s} 
	=& \underbrace{(I_{n_v}-Q_{\alpha,s})}_{=I_{n_v}-[\frac{n_v}{n}+o(\frac{n-n_v}{n})]P_{\alpha,s}}(I_{n_v}+c_n P_{\alpha,s})\underbrace{(I_{n_v}-Q_{\alpha,s})}_{=I_{n_v}-[\frac{n_v}{n}+o(\frac{n-n_v}{n})]P_{\alpha,s}}\\
	%= (I_{n_v}+c_n P_{\alpha,s})\left(I_{n_v}-\left[\frac{n_v}{n}+o(\frac{n-n_v}{n})\right]P_{\alpha,s}\right) \\
	%- \left[\frac{n_v}{n}+o(\frac{n-n_v}{n})\right]P_{\alpha,s}	(I_{n_v}+c_n P_{\alpha,s})\left(I_{n_v}-\left[\frac{n_v}{n}+o(\frac{n-n_v}{n})\right]P_{\alpha,s}\right)\\
	=&\biggl(I_{n_v}-\frac{n_v}{n}P_{\alpha,s}\biggr)(I_{n_v}+c_n P_{\alpha,s})\biggl(I_{n_v}-\frac{n_v}{n}P_{\alpha,s}\biggr)
	+\biggl[o\biggl(\frac{n-n_v}{n}\biggr)\biggr]^2(1+c_n)P_{\alpha,s} \\
	&+ 2o\biggl(\frac{n-n_v}{n}\biggr)\biggl(1-\frac{n_v}{n}\biggr)(1+c_n)P_{\alpha,s}\\
	=&\biggl(I_{n_v}-\frac{n_v}{n}P_{\alpha,s}\biggr)^2+\underbrace{c_n\biggl(1-\frac{n_v}{n}\biggr)^2}_{=\frac{n_v}{n}(2-\frac{n_v}{n})}P_{\alpha,s}+\biggl[o\biggl(\frac{n-n_v}{n}\biggr)\biggr]^2(1+c_n)P_{\alpha,s}\\
	&+ 2o\biggl(\frac{n-n_v}{n}\biggr)\biggl(1-\frac{n_v}{n}\biggr)(1+c_n)P_{\alpha,s}\\
	=&I_{n_v} +o(1)\left(\frac{n-n_v}{n}\right)^2(1+c_n)P_{\alpha,s}.
	\end{align*}
	Inserting this into $B_\alpha$ yields
	\begin{align}
	B_\alpha
	=o(1)\left(\frac{n-n_v}{n}\right)^2(1+c_n)\frac{1}{n_vb}\sum_{s\in \mathcal{B}}e_s'(I_{n_v}-Q_{\alpha,s})^{-1}P_{\alpha,s}(I_{n_v}-Q_{\alpha,s})^{-1}e_s\label{proof_BICV_9}
	\end{align}
	Next, we bound the matrix product in the sum. Therefore we first use again the relation between $Q_{\alpha,s}$ and $P_{\alpha,s}$ to obtain
	\begin{align*}
	(I_{n_v}-Q_{\alpha,s})P_{\alpha,s}(I_{n_v}-Q_{\alpha,s})
	&= \left[1-\frac{n_v}{n}+o\left(\frac{n-n_v}{n}\right)\right] P_{\alpha,s}(I_{n_v}-Q_{\alpha,s})\\
	&=\left[1-\frac{n_v}{n}+o\left(\frac{n-n_v}{n}\right)\right]^2 P_{\alpha,s}\\
	&=\biggl[\frac{n-n_v}{n}+o\biggl(\frac{n-n_v}{n}\biggr)\biggr]^2 P_{\alpha,s}\\
	&=\left(\frac{n-n_v}{n}\right)^2[1+o(1)]^2 P_{\alpha,s}.
	\end{align*}
	Then for any $s\in \mathcal{B}$ and $n$ sufficiently large, we can bound
	\begin{align*}
	\biggl(\frac{n}{n-n_v}\biggr)^2 (I_{n_v}-Q_{\alpha,s})P_{\alpha,s}(I_{n_v}-Q_{\alpha,s})=[1+o(1)]^2P_{\alpha,s}\ge \frac{1}{2}P_{\alpha,s}.
	\end{align*}
	And after rearrangement, we obtain 
	%HIER noch weiter ausformulieren
	\begin{align*}
	(I_{n_v}-Q_{\alpha,s})^{-1}P_{\alpha,s}(I_{n_v}-Q_{\alpha,s})^{-1}\le 2 \biggl(\frac{n}{n-n_v}\biggr)^2P_{\alpha,s}.
	\end{align*}
	Using this bound on (\ref{proof_BICV_9}), yields
	\begin{align*}
	B_\alpha
	&=o(1)\left(\frac{n-n_v}{n}\right)^2(1+c_n)\frac{1}{n_vb}\sum_{s\in \mathcal{B}}e_s'\underbrace{(I_{n_v}-Q_{\alpha,s})^{-1}P_{\alpha,s}(I_{n_v}-Q_{\alpha,s})^{-1}}_{\le 2(\frac{n}{n-n_v})^2P_{\alpha,s}}e_s\\
	&\le 2o(1)(1+c_n)\frac{1}{n_vb}\sum_{s\in \mathcal{B}}e_s'P_{\alpha,s}e_s.
	\end{align*}
	From the previous proof we know that $(1+c_n)\frac{1}{n_vb}\sum_{s\in \mathcal{B}}e_s'P_{\alpha,s}e_s=O_P((n-n_v)^{-1})$ and thus we achieve the desired result
	\begin{align*}
	B_\alpha\le 2o(1)O_P\left(\frac{1}{n-n_v}\right)=o_P\left(\frac{1}{n-n_v}\right).
	\end{align*}
	
	\textbf{Proof of $(III)$:} \\
	We will prove this statement in the same way as in the case of $CV(1)$. 
	First note that $\hat{\Gamma}_{\alpha,n}^{BICV} \ge \Gamma_{\alpha,n}+o_P(1)$ for any $\mathcal{M}_\alpha \in $ Category $I$ and that $\hat{\Gamma}_{\alpha,n}^{BICV} = \sigma^2+o_P(1)$ for any $\mathcal{M}_\alpha \in $ Category $II$ . 
	Hence, by the same argument as in Theorem \ref{THM_Consistency of $CV(1)$} $(III)$ one can show that 
	\begin{align*}
	\lim_{n\to\infty} P(\mathcal{M}_{BICV}\textrm{ is in Category I})=0
	\end{align*}
	Therefore, without loss of generality, we only consider models in Category $II$ for the remainder of the proof.
	Next, the event that BICV doesn't choose the true model at step $n$, i.e. $\{ \mathcal{M}_{BICV}\neq \mathcal{M}_\alpha\}$, can be represented by
	\begin{align*}
	&\{ \exists\alpha\in \textrm{Cat. $II$} \textrm{ s.t. $\mathcal{M}_\alpha$ is preferable to $\mathcal{M}_\ast$ w.r.t. $BICV$ at step $n$ }\} \\
	= &\{ \exists\alpha\in \textrm{Cat. $II$}: \hat{\Gamma}_{\alpha,n}^{BICV} < \hat{\Gamma}_{\ast,n}^{BICV} \} \\
	= &\underset{\alpha\in \textrm{Cat. $II$}}{\bigcup}\{\hat{\Gamma}_{\alpha,n}^{BICV} < \hat{\Gamma}_{\ast,n}^{BICV} \}.
\end{align*}
	By using a union bound, we obtain
	\begin{align*}
	P(\mathcal{M}_{BICV}\neq \mathcal{M}_\alpha) \le \sum_{\alpha\in \textrm{Cat. $II$}}P\left(\hat{\Gamma}_{\alpha,n}^{BICV} < \hat{\Gamma}_{\ast,n}^{BICV}\right)
	\end{align*}
	Next, we want to evaluate the event $\hat{\Gamma}_{\alpha,n}^{BICV} < \hat{\Gamma}_{\ast,n}^{BICV}$ by using Theorem \ref{THM_Consistency_BICV} $(II)$. To clarify our notation, define the remainder terms
	\begin{align*}
	R_\alpha := \hat{\Gamma}_{\alpha,n}^{BICV} - \frac{\varepsilon'\varepsilon}{n}+\frac{d_\alpha\sigma^2}{n-n_v}
	\quad \textrm{ and } \quad
	R_\ast := \hat{\Gamma}_{\ast,n}^{BICV} - \frac{\varepsilon'\varepsilon}{n}+\frac{d_\ast\sigma^2}{n-n_v}.
	\end{align*}
	By Theorem \ref{THM_Consistency_BICV} $(II)$ both terms satisfy $R_\alpha=o_P((n-n_v)^{-1})$ and $R_\ast=o_P((n-n_v)^{-1})$. Now the event $\hat{\Gamma}_{\alpha,n}^{BICV} < \hat{\Gamma}_{\ast,n}^{BICV}$ can be rephrased as
	\begin{align*}
	\frac{\varepsilon'\varepsilon}{n}+\frac{d_\alpha\sigma^2}{n-n_v} +R_\alpha
	&< \frac{\varepsilon'\varepsilon}{n}+\frac{d_\ast\sigma^2}{n-n_v} +R_\ast\\
	\iff \quad \frac{(d_\alpha-d_\ast)\sigma^2}{n-n_v} &< R_\ast -R_\alpha\\
	\iff \quad(d_\alpha-d_\ast)\sigma^2 &< (n-n_v)(R_\ast -R_\alpha).
	\end{align*}
	Since $(n-n_v)(R_\ast -R_\alpha)=o_P(1)$, we conclude that 
	\begin{align*}
	\lim_{n\to\infty} P\left(\hat{\Gamma}_{\alpha,n}^{BICV} < \hat{\Gamma}_{\ast,n}^{BICV}\right) 
	=\lim_{n\to\infty}P\left((d_\alpha-d_\ast)\sigma^2 < (n-n_v)(R_\ast -R_\alpha)\right) = 0 
	\end{align*}
	and therefore $\lim_{n\to\infty}P(\mathcal{M}_{BICV}\neq \mathcal{M}_\alpha) = 0$.
\end{proof}
\subfile{ProofClaimJan}
For the proof of the asymptotic properties of $MCCV(n_\nu)$ we need the following Lemma:  
\addcontentsline{toc}{subsection}{Proof Asymptotic Properties of $MCCV(n_\nu)$}
\subsection*{Proof Asymptotic Properties of $MCCV(n_\nu)$}
\begin{lemma}
	\label{Lemma_MCCV}
	Denote by $\mathcal{R}^\ast:= \{s\subseteq\{1,\dots,n\}|\# s=n_v\}$ and let $\mathrm{E}_\mathcal{R}$ and $V_\mathcal{R}$ denote the expectation and variance with respect to the random selection of $\mathcal{R}\subseteq\mathcal{R}^\ast$. Then for any functions $a,c:\mathcal{R}^\ast\to \mathbb{R}$ it holds that
	\begin{enumerate}
		\item $\mathrm{E}_\mathcal{R} \bigl[ b^{-1}\sum_{s\in \mathcal{R}}a(s)\bigr] = \binom{n}{n_v}^{-1}\sum_{s\in\mathcal{R}^\ast}a(s)$
		\item $V_\mathcal{R} \bigl( b^{-1}\sum_{s\in \mathcal{R}}a(s)\bigr) \le b^{-1} \mathrm{E}_\mathcal{R} \bigl[a(s)^2\bigr]$
		\item $V_\mathcal{R}\bigl(a(s)+c(s)\bigr) \le 2\bigl[V_\mathcal{R}\bigl(a(s)\bigr)+V_\mathcal{R}\bigl(c(s)\bigr)\bigr]$
		\item $\mathcal{R}^\ast$ fulfills the \textit{balance conditions} of definition \ref{balance_property}.
	\end{enumerate}
\end{lemma}

\begin{proof}[Proof of Lemma \ref{Lemma_MCCV}]~\\
	(1.) Denote by $s_i$ the $i^{th}$ selected subset in $\mathcal{R}$. Then the probability that subset $s \in \mathcal{R}^\ast$ is selected at the $i$th selection step, i.e. $s_i=s$, is given by
	\begin{align*}
	P(s_i=s)= \frac{1}{\# \mathcal{R}^\ast} = \binom{n}{n_v}^{-1}.
	\end{align*}
	Furthermore the selected subsets at step $i$ and $j\neq i$ are independent and identically distributed, i.e for any $s,s'\in \mathcal{R}^\ast$ it holds
	\begin{align*}
	P(s_i=s,s_j=s')=P(s_i=s)P(s_j=s')=\binom{n}{n_v}^{-2}.
	\end{align*}
	As the choice of sets $s\in \mathcal{R}$ is iid it follows that the random variables $(a(s_i))_{i=1}^b$ are iid as well. Hence, 
	\begin{align*}
	\mathrm{E}_\mathcal{R} \biggl[ \frac{1}{b}\sum_{s\in \mathcal{R}}a(s)\biggr] 
	= \frac{1}{b}\underbrace{\sum_{s\in \mathcal{R}}\mathrm{E}_\mathcal{R} [a(s)]}_{=b\mathrm{E}_\mathcal{R}[a(s)]}
	= \sum_{s\in \mathcal{R}^\ast}a(s)P(s_i = s)
	=\binom{n}{n_v}^{-1}\sum_{s\in\mathcal{R}^\ast}a(s).
	\end{align*}
	(2.) Since the choice of sets $s\in \mathcal{R}$ and the random variables $(a(s_i))_{i=1}^b$ are iid
	\begin{align*}
	V_\mathcal{R} \biggl(\frac{1}{b}\sum_{s\in \mathcal{R}}a(s)\biggr)
	= \frac{1}{b^2}\underbrace{\sum_{s\in \mathcal{R}}V_\mathcal{R} [a(s)]}_{=bV_\mathcal{R} [a(s)]}
	\le \frac{1}{b}\mathrm{E}_\mathcal{R}\bigl[a(s)^2\bigr].
	\end{align*}
	(3.) Using that for any two real numbers $a,c$ it holds that $(a+c)^2\le 2(a^2+c^2)$ yields
	\begin{align*}
	V_\mathcal{R}\bigl(a(s)+c(s)\bigr) 
	&= \mathrm{E}_\mathcal{R}\bigl[\bigl(a(s)-\mathrm{E}_\mathcal{R}[a(s)] + c(s) - \mathrm{E}_\mathcal{R}[c(s)]\bigr)^2\bigr]\\
	&\le 2 \mathrm{E}_\mathcal{R}\bigl[\bigl(a(s)-\mathrm{E}_\mathcal{R}[a(s)] \bigr)^2+\bigl(c(s) - \mathrm{E}_\mathcal{R}[c(s)]\bigr)^2\bigr]\\
	&= 2\bigl[V_\mathcal{R}\bigl(a(s)\bigr)+V_\mathcal{R}\bigl(c(s)\bigr)\bigr]. 
	\end{align*}
	(4.) At first note that $\mathcal{R}^\ast$ has $b=\binom{n}{n_v}$ elements. Fix some $i$ and $j$ in $\{1,\dots, n\}$ such that $j>i$. To count the number of sets which contain $i$, we can count the number of subsets of $\{1,\dots, n\}\backslash\{i\}$ that have $n_v-1$ elements, i.e.
	\begin{align*}
	\#\{s\in \mathcal{R}^\ast|i\in s\} = \#\{s \subseteq \{1,\dots, n\}\backslash\{i\} | \# s =n_v-1\} = \binom{n-1}{n_v-1} =\frac{n_v}{n}\binom{n}{n_v}.
	\end{align*}
	Similarly, the number of sets which contain both $i$ and $j$ can be computed as
	\begin{align*}
	\#\{s\in \mathcal{R}^\ast|i,j\in s , j>i\} &= \#\{s \subseteq \{1,\dots, n\}\backslash\{i,j\} | \# s =n_v-2\} \\
	&= \binom{n-2}{n_v-2} = \frac{n_v(n_v-1)}{n(n-1)}\binom{n}{n_v}.
	\end{align*}
	Hence, $\mathcal{R}^\ast$ fulfills the conditions of definition \ref{balance_property}. \\
\end{proof}


\begin{proof}[Proof of Theorem \ref{THM_Consistency_MCCV}]~\\
	\textbf{Proof of $(I)$:} \\
	\cite{shao} doesn't proof this assertion in his paper but gives a guideline which we follow.
	
	Analogously as in the proof of Theorem \ref{THM_Consistency of $CV(1)$}, we can bound $\hat{\Gamma}_{\alpha,n}^{MCCV}$ by
	\begin{align*}
	\hat{\Gamma}_{\alpha,n}^{MCCV}&= \frac{1}{n_vb}\sum_{s\in \mathcal{R}}\lVert y_s-\hat{y}_{\alpha,s^c}\rVert^2
	= \frac{1}{n_vb}\sum_{s\in \mathcal{R}}\lVert (I_{n_v}-Q_{\alpha,s})^{-1}(y_s-X_{\alpha,s}\hat{\beta}_\alpha)\rVert^2\\
	&\ge \frac{1}{n_vb}\sum_{s\in \mathcal{R}}\lVert y_s-X_{\alpha,s}\hat{\beta}_\alpha\rVert^2\\
	\end{align*}
	As $\mathcal{R}^\ast$ is a balanced incomplete block design by Lemma \ref{Lemma_MCCV}, it holds that
	\begin{align*}
	\mathrm{E}_\mathcal{R} \biggl[\frac{1}{n_vb}\sum_{s\in \mathcal{R}}\lVert y_s-X_{\alpha,s}\hat{\beta}_\alpha\rVert^2 \biggr] 
	= \frac{1}{n_v}\binom{n}{n_v}^{-1}\sum_{s\in \mathcal{R}^\ast}\sum_{i\in s}(y_i-x_{i,\alpha}'\hat{\beta}_\alpha)^2
	= \frac{1}{n}\sum_{i=1}^n(y_i-x_{i,\alpha}'\hat{\beta}_\alpha)^2,
	\end{align*}
	where we only used property $(a)$ in definition \ref{balance_property}.
	Therefore if $\mathcal{R}$ has this property, then $\frac{1}{n_vb}\sum_{s\in \mathcal{R}}\lVert y_s-X_{\alpha,s}\hat{\beta}_\alpha\rVert^2$ and its expectation will coincide. 
	
	To see that $\mathcal{R}$ has property $(a)$ of definition \ref{balance_property} with probability converging to one, denote by $\mathcal{R}\in \{ 0,1\}^{n\times b}$ a matrix representation of the random sampling, where $\mathcal{R}_{i,j}=1$ is interpreted as $i$ is an element of the $j$-th sample.
	Under random sampling with replacement the columns of $\mathcal{R}$, denoted by $\mathcal{R}_j$, are independent and each element $\mathcal{R}_{i,j}$ is Bernoulli distributed with $P(\mathcal{R}_{i,j}=1)=\frac{n_v}{n}$.
	
	By claim \ref{Claim_BICV}, we can express the probability that $\mathcal{R}$ has property $(a)$ of definition \ref{balance_property} as
	\begin{align*}
	&P\biggl( \sum_{j=1}^b \mathcal{R}_{i,j} = \frac{n_v}{n}b \quad\forall 1\le i \le n\biggr)
	= P\biggl( \frac{1}{b}\sum_{j=1}^b \mathcal{R}_{i,j} = \underbrace{\frac{n_v}{n}}_{\mathrm{E}[\mathcal{R}_{i,j}]} \quad\forall 1\le i \le n\biggr)\\
	=&P\biggl(\biggl\lVert\frac{1}{b}\sum_{j=1}^b \mathcal{R}_{i,j}-\mathrm{E}[\mathcal{R}_{i,j}]\biggr\rVert^2=0\biggr)
	\end{align*}
	Next, by using the Markov and the triangle inequality, we can establish for any $\varepsilon >0$
	\begin{align*}
	P\biggl(\biggl\lVert\frac{1}{b}\sum_{j=1}^b \mathcal{R}_{i,j}&-\mathrm{E}[\mathcal{R}_{i,j}]\biggr\rVert^2>\varepsilon\biggr)
	\le \frac{1}{\varepsilon}\mathrm{E}\biggl[\biggl\lvert \sum_{i=1}^n\biggl(\frac{1}{b}\sum_{j=1}^b \mathcal{R}_{i,j}-\mathrm{E}[\mathcal{R}_{i,j}]\biggr)^2\biggr\rvert\biggr]\\
	\le&\frac{1}{b^2\varepsilon}\sum_{i=1}^n\mathrm{E}\biggl[\biggl(\sum_{j=1}^b \mathcal{R}_{i,j}-\mathrm{E}[\mathcal{R}_{i,j}]\biggr)^2\biggr]\\
	= &\frac{1}{b^2\varepsilon}\sum_{i=1}^n\biggl[\sum_{j=1}^b\underbrace{\mathrm{Var}(\mathcal{R}_{i,j})}_{=\frac{n_v}{n}\frac{n-n_v}{n}}+\sum_{j=1}^b\sum_{k\neq j}^b\underbrace{\mathrm{Cov}(\mathcal{R}_{i,j},\mathcal{R}_{i,k})}_{=0}\biggr]\\
	=&\frac{1}{\varepsilon}\frac{n}{b}\frac{n_v}{n}\frac{n-n_v}{n},
	\end{align*}
	which converges to zero, because of (\ref{growth_rates_nv_MCCV}) and since by assumption $b\ge n$.
	Hence, we have shown that $\frac{1}{n_vb}\sum_{s\in \mathcal{R}}\lVert y_s-X_{\alpha,s}\hat{\beta}_\alpha\rVert^2 = \frac{1}{n}\lVert y-X_{\alpha}\hat{\beta}_\alpha\rVert^2 + o_P(1)$.
	
	Finally, let
	\begin{align*}
	R_n= \frac{1}{n_vb}\sum_{s\in \mathcal{R}}\lVert y_s-\hat{y}_{\alpha,s^c}\rVert^2- \frac{1}{n_vb}\sum_{s\in \mathcal{R}}\lVert y_s-X_{\alpha,s}\hat{\beta}_\alpha\rVert^2 \ge 0
	\end{align*} 
	and the remaining proof of statement $(I)$ is the same as in the case of $BICV(n_v)$.\\\\
	\textbf{Proof of $(II)$:}\\
	To proof the second statement, \cite{shao} uses a more refined decomposition as in the proof of Theorem \ref{THM_Consistency of $CV(1)$}. 
	\begin{enumerate}
		\item  $\hat{\Gamma}_{n,\alpha}^{MCCV} = \frac{1}{n_vb}\sum_{s\in \mathcal{R}}\varepsilon_s'\varepsilon_s+ A_{\alpha1}-A_{\alpha2}+a_{\alpha3}+B_\alpha$
		\item $A_{\alpha1} = B_{\alpha1}+ B_{\alpha2}$
		\item $ B_{\alpha1}-A_{\alpha2}+A_{\alpha3} = \frac{1}{n-n_v}d_\alpha\sigma^2 + o_P((n-n_v)^{-1})$
		\item $B_{\alpha2} = o_P((n-n_v)^{-1})$
		\item $B_{\alpha} = o_P((n-n_v)^{-1})$
	\end{enumerate}
	(1.)
	First we decompose $\hat{\Gamma}_{n,\alpha}^{MCCV}$ as in the case for $BICV(n_v)$, i.e.
	\begin{align*}
	\hat{\Gamma}_{n,\alpha}^{MCCV} &= \frac{1}{n_vb}\sum_{s\in\mathcal{R}}e_s'(I_{n_v}-Q_{\alpha,s})^{-1}U_{\alpha,s}(I_{n_v}-Q_{\alpha,s})^{-1}e_s\\
	&+\underbrace{\frac{1}{n_vb}\sum_{s\in\mathcal{R}}e_s'(I_{n_v}-Q_{\alpha,s})^{-1}(I_{n_v}-U_{\alpha,s})(I_{n_v}-Q_{\alpha,s})^{-1}e_s}_{=:B_{\alpha}}\\
	&= \frac{1}{n_vb}\sum_{s\in\mathcal{R}}e_s'(I_{n_v}+c_nP_{\alpha,s})e_s + B_\alpha.
	\end{align*}
	If one multiplies $e_s'(I_{n_v}+c_nP_{\alpha,s})e_s$ out, while using that $e_s = \varepsilon_s - X_{\alpha,s}(X_\alpha'X_\alpha)^{-1}X_\alpha'\varepsilon$ one gets
	\begin{align*}
	e_s'&(I_{n_v}+c_nP_{\alpha,s})e_s \\
	&= (\varepsilon_s - X_{\alpha,s}(X_\alpha'X_\alpha)^{-1}X_\alpha'\varepsilon)'(I_{n_v}+c_nP_{\alpha,s})(\varepsilon_s - X_{\alpha,s}(X_\alpha'X_\alpha)^{-1}X_\alpha'\varepsilon)\\
	&= (\varepsilon_s - X_{\alpha,s}(X_\alpha'X_\alpha)^{-1}X_\alpha'\varepsilon)'(\varepsilon_s - X_{\alpha,s}(X_\alpha'X_\alpha)^{-1}X_\alpha'\varepsilon + c_nP_{\alpha,s}\varepsilon_s - c_n\underbrace{P_{\alpha,s}X_{\alpha,s}(X_\alpha'X_\alpha)^{-1}X_\alpha'}_{=X_{\alpha,s}(X_\alpha'X_\alpha)^{-1}X_\alpha'}\varepsilon)\\
	&= (\varepsilon_s - X_{\alpha,s}(X_\alpha'X_\alpha)^{-1}X_\alpha'\varepsilon)'(\varepsilon_s + c_nP_{\alpha,s}\varepsilon_s -(1+c_n) X_{\alpha,s}(X_\alpha'X_\alpha)^{-1}X_\alpha'\varepsilon)  \\
	&= \varepsilon_s'\varepsilon_s + c_n \varepsilon_s'P_{\alpha,s}\varepsilon_s -2(1+c_n)\varepsilon_s' X_{\alpha,s}(\hat{\beta}_\alpha-\beta_\alpha) + (1+c_n)(\hat{\beta}_\alpha-\beta_\alpha)'X_{\alpha,s}'X_{\alpha,s}(\hat{\beta}_\alpha-\beta_\alpha)
	\end{align*}
	where we used in the last line that $\hat{\beta}_\alpha = \beta_\alpha + (X_\alpha'X_\alpha)^{-1}X_\alpha'\varepsilon$. Inserting this in $\hat{\Gamma}_{n,\alpha}^{MCCV}$ yields 
	\begin{align*}
	\hat{\Gamma}_{n,\alpha}^{MCCV} 
	= &\frac{1}{n_vb}\sum_{s\in \mathcal{R}}\varepsilon_s'\varepsilon_s 
	+ \underbrace{\frac{c_n}{n_vb}\sum_{s\in \mathcal{R}}\varepsilon_s'P_{\alpha,s}\varepsilon_s}_{=:A_{\alpha1}}
	- \underbrace{\frac{2(1+c_n)}{n_vb}\sum_{s\in \mathcal{R}}\varepsilon_s'X_{\alpha,s}(\hat{\beta}_\alpha-\beta_\alpha)}_{=:A_{\alpha2}}\\
	&+ \underbrace{\frac{1+c_n}{n_vb}\sum_{s\in \mathcal{R}}(\hat{\beta}_\alpha-\beta_\alpha)'X_{\alpha,s}'X_{\alpha,s}(\hat{\beta}_\alpha-\beta_\alpha)}_{=:A_{\alpha3}} + B_{\alpha}.
	\end{align*}
	
	(2.)
	By the same arguments as in the proof for the $BICV(n_v)$ case and replacing limits with probability limits, one can show that
	\begin{align*}
	P_{\alpha,s}=\frac{n}{n_v}Q_{\alpha,s}+o_P\biggl(\frac{n-n_v}{n}\biggr)\frac{n}{n_v}P_{\alpha,s}
	\end{align*}
	and hence we can further decompose $A_{\alpha1}$ to
	\begin{align*}
	A_{\alpha1}= \underbrace{\frac{c_n}{n_vb}\sum_{s\in\mathcal{R}}\frac{n}{n_v}\varepsilon_s'Q_{\alpha,s}\varepsilon_s}_{=:B_{\alpha1}}
	+ \underbrace{o_P\biggl(\frac{n-n_v}{n}\biggr)\frac{c_n}{n_vb}\sum_{s\in\mathcal{R}}\frac{n}{n_v}\varepsilon_s'P_{\alpha,s}\varepsilon_s}_{=:B_{\alpha2}}.
	\end{align*}
	
	(3.)
	The expectation of $B_{\alpha1}$ with respect to the random sampling satisfies
	\begin{align*}
	\mathrm{E}_\mathcal{R}[B_{\alpha1}]= \mathrm{E}_\mathcal{R}\biggl[ \frac{c_n}{n_vb}\sum_{s\in\mathcal{R}}\frac{n}{n_v}\varepsilon_s'Q_{\alpha,s}\varepsilon_s\biggr]
	=\frac{c_n}{n_v} \frac{n}{n_v} \mathrm{E}_\mathcal{R}\bigl[\varepsilon_s'Q_{\alpha,s}\varepsilon_s\bigr]
	= \frac{c_n}{n_v} \frac{n}{n_v}\binom{n}{n_v}^{-1}\sum_{s\in \mathcal{R}^\ast}\varepsilon_s'Q_{\alpha,s}\varepsilon_s.
	\end{align*}
	Similar calculations as for Theorem \ref{THM_Consistency of $CV(1)$} and using that $\mathcal{R}^\ast$ is a balanced incomplete block design yields 
	\begin{align*}
	\frac{1}{n_v}\binom{n}{n_v}^{-1}\sum_{s\in \mathcal{R}^\ast}\varepsilon_s'Q_{\alpha,s}\varepsilon_s 
	&= \frac{1}{n_v}\binom{n}{n_v}^{-1}\sum_{s\in \mathcal{R}^\ast} \sum_{i\in s} p_{i,i,\alpha}\varepsilon_i^2 
	+ 2\frac{1}{n_v}\binom{n}{n_v}^{-1}\sum_{s\in \mathcal{R}^\ast} \sum_{i\in s} \sum_{j\in s, j>i} p_{i,j,\alpha}\varepsilon_i\varepsilon_j \\
	&= \frac{1}{n}\sum_{i=1}^n p_{i,i,\alpha}\varepsilon_i^2 + \frac{n_v-1}{n(n-1)}\varepsilon'P_\alpha\varepsilon - \frac{n_v-1}{n(n-1)}\sum_{i=1}^n p_{i,i,\alpha}\varepsilon_i^2\\
	&=\frac{n-n_v}{n(n-1)}\sum_{i=1}^n p_{i,i,\alpha}\varepsilon_i^2
	+\frac{n_v-1}{n(n-1)}\varepsilon'P_\alpha\varepsilon
	\end{align*}
	and hence
	\begin{align*}
	\mathrm{E}_\mathcal{R}[B_{\alpha1}]
	&=\frac{c_nn}{n_v}\biggl[\frac{n-n_v}{n(n-1)}\sum_{i=1}^n p_{i,i,\alpha}\varepsilon_i^2
	+\frac{n_v-1}{n(n-1)}\varepsilon'P_\alpha\varepsilon\biggr]\\
	&=\frac{c_nn}{n_v}\biggl[\frac{n-n_v}{n^2}d_\alpha\sigma^2
	+\frac{n_v-1}{n^2}\varepsilon'P_\alpha\varepsilon + o_P\biggl(\frac{n-n_v}{n^2}\biggr)\biggr].
	\end{align*}
	
	For the expectation of $A_{\alpha2}$ with respect to the random sampling it holds that
	\begin{align*}
	\mathrm{E}_\mathcal{R}[A_{\alpha2}] 
	&=\mathrm{E}_\mathcal{R}\biggl[ \frac{2(1+c_n)}{n_vb}\sum_{s\in \mathcal{R}}\varepsilon_s'\underbrace{X_{\alpha,s}(\hat{\beta}_\alpha-\beta_\alpha)}_{=X_{\alpha,s}(X_\alpha'X_\alpha)^{-1}X_\alpha'\varepsilon}\biggr]\\
	&= \frac{2(1+c_n)}{n_v}\binom{n}{n_v}^{-1}\sum_{s\in\mathcal{R}^\ast}\sum_{i\in s}\varepsilon_i'x_{\alpha,i}'(X_\alpha'X_\alpha)^{-1}X_\alpha'\varepsilon\\
	&= \frac{2(1+c_n)}{n} \sum_{i=1}^n\varepsilon_ix_{\alpha,i}'(X_\alpha'X_\alpha)^{-1}X_\alpha'\varepsilon
	= \frac{2(1+c_n)}{n} \varepsilon'P_\alpha\varepsilon.
	\end{align*}
	
	At last, the expectation of $A_{\alpha3}$ with respect to the random sampling satisfies
	\begin{align*}
	\mathrm{E}_\mathcal{R}[A_{\alpha3}] 
	&=\mathrm{E}_\mathcal{R}\biggl[ \frac{1+c_n}{n_vb}\sum_{s\in \mathcal{R}}(\hat{\beta}_\alpha-\beta_\alpha)'X_{\alpha,s}'X_{\alpha,s}(\hat{\beta}_\alpha-\beta_\alpha)\biggr]\\
	&= \frac{1+c_n}{n_v}\binom{n}{n_v}^{-1}\sum_{s\in\mathcal{R}^\ast}\sum_{i\in s}\varepsilon'X_{\alpha}(X_{\alpha}'X_{\alpha})^{-1}x_{\alpha,i}x_{\alpha,i}'(X_{\alpha}'X_{\alpha})^{-1}X_{\alpha}'\varepsilon\\
	&= \frac{1+c_n}{n}\sum_{i=1}^n\varepsilon'X_{\alpha}(X_{\alpha}'X_{\alpha})^{-1}x_{\alpha,i}x_{\alpha,i}'(X_{\alpha}'X_{\alpha})^{-1}X_{\alpha}'\varepsilon
	= \frac{1+c_n}{n}\varepsilon'P_\alpha\varepsilon.
	\end{align*}
	
	Putting these results together, we obtain
	\begin{align*}
	\mathrm{E}_\mathcal{R}&[B_{\alpha1}-A_{\alpha2}+A_{\alpha3}]\\
	&= \frac{c_nn}{n_v}\biggl[\frac{n-n_v}{n^2}d_\alpha\sigma^2
	+\frac{n_v-1}{n^2}\varepsilon'P_\alpha\varepsilon + o_P\biggl(\frac{n-n_v}{n^2}\biggr)\biggr]
	-\frac{2(1+c_n)}{n} \varepsilon'P_\alpha\varepsilon
	+\frac{1+c_n}{n}\varepsilon'P_\alpha\varepsilon\\
	&= \frac{c_nn}{n_v}\biggl[\frac{n-n_v}{n^2}d_\alpha\sigma^2
	+\frac{n_v-1}{n^2}\varepsilon'P_\alpha\varepsilon + o_P\biggl(\frac{n-n_v}{n^2}\biggr)\biggr]
	-\frac{1+c_n}{n} \varepsilon'P_\alpha\varepsilon\\
	&=\frac{d_\alpha\sigma^2}{n-n_v}+ o_P\biggl(\frac{1}{n-n_v}\biggr).
	\end{align*}
	
	Letting $t_n=O[n^2/((n-n_v)^2b)]$, then corresponding variances satisfy
	\begin{align*}
	V_\mathcal{R}\bigl[(n-n_v)B_{\alpha1}\bigr]\le \underbrace{\frac{c_n^2(n-n_v)^2}{n_v^2b}\frac{n^2}{n_v^2}}_{=t_n} \mathrm{E}_\mathcal{R}\biggl[ (\varepsilon_s'Q_{\alpha,s}\varepsilon_s)^2\biggr]
	\end{align*}
	using that for any two real numbers $a$ and $b$ it holds $(a+b)^2\le 2(a^2+b^2)$ yields
	\begin{align*}
	V_\mathcal{R}\bigl[(n-n_v)B_{\alpha1}\bigr]
	&\le 2t_n\mathrm{E}_\mathcal{R}\biggl[\biggl(\sum_{i\in s} p_{i,i,\alpha}\varepsilon_i^2\biggr)^2 + \biggl(\sum_{i\in s}\sum_{j\in s,j\neq i} p_{i,j,\alpha}\varepsilon_i\varepsilon_j\biggr)^2\biggr]\\
	&\le 2t_n \biggl[\biggl(\sum_{i=1}^n p_{i,i,\alpha}\varepsilon_i^2\biggr)^2 + \binom{n}{n_v}\sum_{s\in \mathcal{R}^\ast}\biggl(\sum_{i\in s}\sum_{j\in s,j\neq i} p_{i,j,\alpha}\varepsilon_i\varepsilon_j\biggr)^2\biggr]\\
	&= 2t_n [O_P(1)+O_P(1)]=O_P(t_n).
	\end{align*}
	And 
	\begin{align*}
	V_\mathcal{R}\bigl[(n-n_v)A_{\alpha2}\bigr]
	&\le \underbrace{\frac{4(1+c_n)^2(n-n_v)^2}{n_v^2b}}_{=t_n}\mathrm{E}_\mathcal{R}\bigl[ \bigl(\varepsilon_s'X_{\alpha,s}(\hat{\beta}_\alpha-\beta_\alpha)\bigr)^2\bigr]\\
	&= t_n (\hat{\beta}_\alpha-\beta_\alpha)'\mathrm{E}_\mathcal{R}\bigl[X_{\alpha,s}'\varepsilon_s \varepsilon_s'X_{\alpha,s}\bigr](\hat{\beta}_\alpha-\beta_\alpha) = O_P(t_n)
	\end{align*}
	where in the last equality we used that $\mathrm{E}[\mathrm{E}_\mathcal{R}[X_{\alpha,s}'\varepsilon_s \varepsilon_s'X_{\alpha,s}]]\le \sigma^2 X_\alpha'X_\alpha=O(n)$ and $\hat{\beta}_\alpha-\beta_\alpha=O_P(n^{-0.5})$.\
	
	Furthermore
	\begin{align*}
	V_\mathcal{R}\bigl[(n-n_v)A_{\alpha3}\bigr]
	&\le \underbrace{\frac{(1+c_n)^2(n-n_v)^2}{n_v^2b}}_{=t_n}\mathrm{E}_\mathcal{R}\bigl[(\hat{\beta}_\alpha-\beta_\alpha)'X_{\alpha,s}'X_{\alpha,s}(\hat{\beta}_\alpha-\beta_\alpha)\bigr]^2\\
	&\le t_n \bigl[(\hat{\beta}_\alpha-\beta_\alpha)'X_{\alpha,s}'X_{\alpha,s}(\hat{\beta}_\alpha-\beta_\alpha)\bigr]^2\\
	&= t_n \bigl[O_P(n^{-0.5})O(n)O_P(n^{-0.5})\bigr]^2=O_P(t_n).
	\end{align*}
	
	Finally by using the third result in Lemma \ref{Lemma_MCCV}, we obtain
	\begin{align*}
	V_\mathcal{R}\bigl[B_{\alpha1}-A_{\alpha2}+A_{\alpha3}\bigr] \le 2\bigl(V_\mathcal{R}\bigl[B_{\alpha1}\bigr]+V_\mathcal{R}\bigl[A_{\alpha2}\bigr]+V_\mathcal{R}\bigl[A_{\alpha3}\bigr]\bigr) = O_P(t_n)=o_P(1)
	\end{align*}
	and hence,
	\begin{align*}
	B_{\alpha1}-A_{\alpha2}+A_{\alpha3} = \frac{d_\alpha\sigma^2 }{n-n_v}+ o_P\biggl(\frac{1}{n-n_v}\biggr).
	\end{align*}
	
	(4.) Next we show that $B_{\alpha2} = o_P((n-n_v)^{-1})$.
	\begin{align*}
	B_{\alpha2} = o_P\biggl(\frac{n-n_v}{n}\biggr)\frac{c_n}{n_vb}\sum_{s\in\mathcal{R}}\frac{n}{n_v}\varepsilon_s'P_{\alpha,s}\varepsilon_s
	\end{align*}
	For any $s\in \mathcal{R}^\ast$
	\begin{align*}
	\varepsilon_s'P_{\alpha,s}\varepsilon_s = \sum_{i\in s} p_{ii,\alpha,s} \varepsilon_i^2 + \sum_{i \in s}\sum_{j\in s, j\neq i}  p_{ij,\alpha,s} \varepsilon_i\varepsilon_j.
	\end{align*}
	By the same truncation technique as in the proof of Theorem \ref{THM_Consistency of $CV(1)$}, one can show that $\sum_{i\in s} p_{ii,\alpha,s} \varepsilon_i^2 = d_\alpha\sigma^2 + o_P(1)$. Since the second term on the right hand side has mean zero and variance
	\begin{align*}
	V\biggl( \sum_{i \in s}\sum_{j\in s, j\neq i}  p_{ij,\alpha,s} \varepsilon_i\varepsilon_j\biggr) =  \sum_{i \in s}
	\underbrace{\sum_{j\in s, j\neq i}  p_{ij,\alpha,s}^2}_{\le p_{ii,\alpha,s}} \sigma^4 \le d_\alpha \sigma^4,
	\end{align*}
	it is bounded in probability.\
	To bound the variance, we used that by idempotency and symmetry of $P_{\alpha,s}$ it holds that
	\begin{align*}
	(P_{\alpha,s}P_{\alpha,s})_{ii} = &\sum_{j \in s} p_{ij,\alpha,s} p_{ji,\alpha,s} = \sum_{j \in s} p_{ij,\alpha,s}^2 = p_{ii,\alpha,s} \\
	\Rightarrow &\sum_{j \in s, j\neq i}p_{ij,\alpha,s}^2 \le p_{ii,\alpha,s}
	\end{align*}
	as any $p_{ii,\alpha,s} \ge 0$. Hence, 
	\begin{align*}
	\varepsilon_s'P_{\alpha,s}\varepsilon_s = d_\alpha\sigma^2 + o_P(1) + O_P(1) = O_P(1)
	\end{align*}
	and therefore $B_{\alpha2}$ satisfies
	\begin{align*}
	B_{\alpha2} &= o_P\biggl(\frac{n-n_v}{n}\biggr)\frac{c_n}{n_vb}\sum_{s\in\mathcal{R}}\underbrace{\frac{n}{n_v}}_{=O(1)}O_P(1)\\
	&= o_P\biggl(\frac{n-n_v}{n}\biggr)\frac{c_n}{n_v} = o_P\biggl(\frac{1}{n-n_v}\biggr)\frac{2n-n_v}{n} = o_P\biggl(\frac{1}{n-n_v}\biggr).
	\end{align*}
	
	(5.)
	$B_{\alpha} = o_P((n-n_v)^{-1})$ can be established by the same arguments as in the corresponding proof for $BICV(n_v)$.\\\\
	\textbf{Proof of $(III)$:} \\
	The proof is essentially the same as that of the corresponding statement for $BICV(n_v)$. One just has to replace $\frac{1}{n}\varepsilon'\varepsilon$ with $\frac{1}{n_vb}\sum_{s\in \mathcal{R}}\varepsilon_s'\varepsilon_s$. Anyway these two terms are asymptocially closely related as
	\begin{align*}
	\mathrm{E}_\mathcal{R} \biggl[ \frac{1}{n_vb}\sum_{s\in \mathcal{R}}\varepsilon_s'\varepsilon_s\biggr] 
	=\frac{1}{n_v}\binom{n}{n_v}^{-1}\sum_{s\in \mathcal{R}^\ast}\varepsilon_s'\varepsilon_s
	\end{align*}
	by the Lemma \ref{Lemma_MCCV}. As furthermore $\mathcal{R}^\ast$ is a balanced incomplete block design by Lemma \ref{Lemma_MCCV} 
	\begin{align*}
	\frac{1}{n_v}\binom{n}{n_v}^{-1}\sum_{s\in \mathcal{R}^\ast}\varepsilon_s'\varepsilon_s
	= \frac{1}{n_v}\binom{n}{n_v}^{-1}\sum_{i=1}^n \varepsilon_i^2 \underbrace{\#\{s\in \mathcal{R}^\ast | i\in s\}}_{=\binom{n}{n_v}\frac{n_v}{n}}
	= \frac{\varepsilon'\varepsilon}{n}
	\end{align*}
	In addition,
	\begin{align*}
	V_\mathcal{R} \biggl[ \frac{1}{n_vb}\sum_{s\in \mathcal{R}}\varepsilon_s'\varepsilon_s\biggr]
	&\le \frac{1}{n_v^2b}\mathrm{E}_\mathcal{R} \bigl[(\varepsilon_s'\varepsilon_s)^2\bigr]
	= \frac{1}{n_v^2b}\binom{n}{n_v}^{-1}\sum_{s\in \mathcal{R}^\ast}\sum_{i\in s} \varepsilon_i^2\\
	&= \frac{1}{n_vn} \sum_{i=1}^n \varepsilon_i^2 
	= \frac{\sigma^2}{n_v} + o_P\biggl(\frac{1}{n_v}\biggr)
	= o_P\biggl(\frac{1}{n_v}\biggr).
	\end{align*}
	Hence, 
	\begin{align*}
	\frac{1}{n_vb}\sum_{s\in \mathcal{R}}\varepsilon_s'\varepsilon_s =\frac{\varepsilon'\varepsilon}{n} +o_P\biggl(\frac{1}{n_v}\biggr). 
	\end{align*}
\end{proof}
\addcontentsline{toc}{subsection}{Proof Asymptotic Properties of $APCV(n_\nu)$}
\subsection*{Proof Asymptotic Properties of $APCV(n_\nu)$}
\begin{proof}[Proof of Corollary \ref{Consistency_APCV}]~\\
	The first statement follows by letting 
	\begin{align*}
	R_n= \frac{2n-n_v}{(n-n_v)(n-1)}\sum_{i=1}^np_{i,i,\alpha}e_i^2\ge 0.
	\end{align*}
	To see that the second statement remains true, note that
	\begin{align*}
	\hat{\Gamma}_{\alpha,n}^{APCV} &= \frac{1}{n}\lVert y-X_\alpha\hat{\beta}_\alpha\rVert^2 +\frac{2n-n_v}{(n-n_v)(n-1)}\sum_{i=1}^np_{i,i,\alpha}e_i^2\\
	&=\frac{1}{n} \varepsilon'(I_n-P_\alpha)\varepsilon+ \underbrace{\frac{2n-n_v}{(n-n_v)(n-1)}}_{=\frac{1}{n-n_v}+o(1)}[d_\alpha\sigma^2+o_P(1)]\\
	&=\frac{\varepsilon'\varepsilon}{n} + \frac{d_\alpha\sigma^2}{n-n_v}+o_P\biggl(\frac{1}{n-n_v}\biggr).
	\end{align*}
	The proof of the third statement remains unchanged. \\
\end{proof}

\addcontentsline{toc}{section}{Appendix \RM{2}}
\section*{Appendix \RM{2}}
\addcontentsline{toc}{subsection}{Simulation Chapter 4.2}
\subsection*{Simulation Chapter 4.2}
\begin{lstlisting}[title={Simulation Graphic Ilustration ESPE CV1 and CV($n_\nu$)}]
library(ggplot2)

n <- 100
p <- 5
p.True <- 2
beta <- c(4,7,0,0,0)
DataGen_X <- function(n){
x_1 <- rep(1,n)         
x_2 <- rnorm(n,0,1)
x_3 <- rnorm(n,0,1)
x_4 <- rnorm(n,0,1)
x_5 <- rnorm(n,4,1)
return(X<- cbind(x_1,x_2,x_3,x_4,x_5))
}
X <- DataGen_X(n)
sigma <- 1
y <- X%*%beta+rnorm(n,0,sigma)

##--------------------------------------------------------------------
##ESPE for CV(1) and CV(n_v) for Models in Category II

ESPE_CV1 <- function(d,n){
return((1+d/n)*sigma^2)
}
ESPE_CVn.v <- function(d,n){
return(1+d/n^(3/4))
}

##--------------------------------------------------------------------
##ESPE for CV(1) and CV(n_v) for Models in Category II

CV <- function(n,y,X,Method = "CV1"){
if(Method =="CV1"){
n_v <- 1
b <- n
train <- combn(seq(1,n,1),n-n_v)
}else{
b <- Method
n_v <- n-floor(n^{3/4})
train <- matrix(ncol = b, nrow = n-n_v)
for (i in 1:b) {
train[,i] <- sample(seq(1,n,1),n-n_v,replace = FALSE)
}
}

Pred.Error <- c()
for (i in 1:length(train[1,])) {
#To make the Code a bit shorter
train.i <- train[,i]                 
X.train <- X[train.i,]    
#Prediction Error for a given alpha and a given subset
Error <- y[-train.i]-X[-train.i,]%*%solve(t(X.train)%*%X.train)%*%t(X.train)%*%y[train.i]
Pred.Error[i] <- t(Error)%*%Error
}
MeanPred.Error <- 1/(b*n_v)*sum(Pred.Error)
return(MeanPred.Error)
}


N.grid <-seq(20,100,1)

CV1_d2 <- c()
for (i in 1:length(N.grid)) {
CV1_d2[i] <- CV(N.grid[i],y[1:N.grid[i]],X[1:N.grid[i],1:2])
}

CV1_d3 <- c()
for (i in 1:length(N.grid)) {
CV1_d3[i] <- CV(N.grid[i],y[1:N.grid[i]],X[1:N.grid[i],1:3])
}

MCCV1_d2 <- c()
for (i in 1:length(N.grid)) {
MCCV1_d2[i] <- CV(N.grid[i],y[1:N.grid[i]],X[1:N.grid[i],1:2],Method = N.grid[i]*5)
}

MCCV1_d3 <- c()
for (i in 1:length(N.grid)) {
MCCV1_d3[i] <- CV(N.grid[i],y[1:N.grid[i]],X[1:N.grid[i],1:3],Method = N.grid[i]*5)
}

##--------------------------------------------------------------------
##Plot for CV1

d <- data.frame("Samplesize"=N.grid,"ESPE1_2"=ESPE_CV1(2,N.grid),"ESPE1_3"=ESPE_CV1(3,N.grid),"ASPE1_2"=CV1_d2,"ASPE1_3"=CV1_d3)

ggplot(d,aes(x=Samplesize)) +
geom_line(aes(y=ESPE1_2,color = "M*"),size =1,linetype=5) +
geom_line(aes(y=ASPE1_2,color = "M*"),size =1.2) +
geom_line(aes(y=ESPE1_3,color = "M*+1"),size =1,linetype=5) +
geom_line(aes(y=ASPE1_3,color = "M*+1"),size =1.2) +
ylab("Expected Squared Prediction Error") + ggtitle("Asymptotic Properties CV(1)") +
theme_bw() +
theme(text = element_text(size=15),
plot.title = element_text(size = 20,hjust = 0.5,face="bold")) +
theme(axis.title.x = element_text(size = 15)) +
theme(axis.title.y = element_text(size = 15)) +
theme(axis.title.y = element_text(margin = margin(t = 0, r = 20, b = 0, l = 0))) +
theme(axis.title.x = element_text(margin = margin(t = 20, r = 0, b = 0, l = 0))) +
theme(plot.title = element_text(margin = margin(t = 0, r = 0 , b = 20, l = 0))) +
theme(legend.title=element_blank(),legend.text = element_text(size = 13)) +
theme(legend.position = c(0.85,0.8))

##--------------------------------------------------------------------
##Plot CV(n_v)


d <- data.frame("Samplesize"=N.grid,"ESPE1_2"=ESPE_CVn.v(2,N.grid),"ESPE1_3"=ESPE_CVn.v(3,N.grid),"ASPE1_2"=MCCV1_d2,"ASPE1_3"=MCCV1_d3)

ggplot(d,aes(x=Samplesize)) +
geom_line(aes(y=ESPE1_2,color = "M*"),size =1,linetype=5) +
geom_line(aes(y=ASPE1_2,color = "M*"),size =1.2) +
geom_line(aes(y=ESPE1_3,color = "M*+1"),size =1,linetype=5) +
geom_line(aes(y=ASPE1_3,color = "M*+1"),size =1.2) +
ylab("Expected Squared Prediction Error") + ggtitle(expression(bold(paste("Asymptotic Properties ",CV(n[v]))))) +
theme_bw() +
theme(text = element_text(size=15),
plot.title = element_text(size = 20,hjust = 0.5,face="bold")) +
theme(axis.title.x = element_text(size = 15)) +
theme(axis.title.y = element_text(size = 15)) +
theme(axis.title.y = element_text(margin = margin(t = 0, r = 20, b = 0, l = 0))) +
theme(axis.title.x = element_text(margin = margin(t = 20, r = 0, b = 0, l = 0))) +
theme(plot.title = element_text(margin = margin(t = 0, r = 0 , b = 20, l = 0))) +
theme(legend.title=element_blank(),legend.text = element_text(size = 13)) +
theme(legend.position = c(0.85,0.8))
\end{lstlisting}
\addcontentsline{toc}{subsection}{Simulation}
\subsection*{Simulation}
%Programmcode-Umgebung im Anhang%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%Überschrift ohne Nummer 
\begin{lstlisting}[title={Function inorder to calculate the set of category I and II models }]
Partion <- function(p.True,p){
	##p denotes the total number of possible regressors 
	##p.True denotes the number of true regressors
	##For the CV we need to compute the set A of all possible models 
	
	##Creats the set {1,...,p}, from which we want to generate the powerset
	Index <- seq(1,p,1)                        

	##Denotes the number of possible models out of {1,...,p} Regressors 
	col.A <- 2^p-1                                           

	##Denote A as powerset of {1,...,p}
	A <- matrix(0L,nrow = p, ncol = col.A)      
	k <- choose(p,1)
	l <- 1
	for (i in 1:p) {
		##combn calculates all combinations of i elements in Index
		A[1:i,l:k] <- combn(Index,i)             
		k <-k + choose(p,i+1)
		l <- l + choose(p,i)
	}

	##We want to split A into two dijoint subsets, ie, the set of category I models and category II models. To compute the set of cat. I and cat. II we assume a certain strurcture of the Data. We need X to be s.t all "TRUE" regrossers are in the first column of the X matrix.
	
	##We may need this sets later for simulation study's
	coln <- c()
	for (i in p.True:col.A) {
		if(all(seq(1,p.True,1) %in% A[,i])){    
		coln <- c(coln,i)
		}
	}
	C2 <- A[,coln]           #Set of all Category II Models
	C1 <-A[,-coln]           #Set of all Category I Models
	return(C2)
}
\end{lstlisting}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{lstlisting}[title={Cross-Validation Function}]
CV <- function(n_v, y, X, Alpha = NULL, MonteCarlo = NULL, Replacement = FALSE, BICV = NULL ){
	##n_v		Number of leaved out data points
	##y,X		Data for regression
	##Alpha	  	Set of possible modelvariations for which the CV 	     should be calculated, by default use all possible 		    models
	#MonteCarlo   	Number of subsets of {1,...,n} which is randomly 	     drawn for a Monte Carlo CV, by befault do K-Fold CV
	#Replacement  	Replacement for Monte Carlo (default FALSE)
	##BICV          Incidence Matrix for a BICV

	##Change some variable names to keep the code shorter
	A <- Alpha
	b <- MonteCarlo

	##Number of possible regressors
	p <- length(X[1,])                          

	##Set of possible models
	if(is.null(A)){

		##Creats the set {1,...,p} from which we want to generate the Powerset
		Index <- seq(1,p,1)    

		##Denotes the number of possible models out of {1,...,p}
		col.A <- 2^p-1
		
		##Denote A as powerset of {1,...,p}
		A <- matrix(0L,nrow = p, ncol = col.A)      
		k <- choose(p,1)
		l <- 1
		for (i in 1:p) {
			#combn spits out all combinations of i elements in Index 
			A[1:i,l:k] <- combn(Index,i)             
			k <-k + choose(p,i+1)
			l <- l + choose(p,i)
			}
		}


	##Number of observations
	n <- length(y)


	##Combinations of sample partions for fitting the model

	##For the Mone Carlo CV with b subsets of {1,...,n}
	if(!is.null(b)){
		train <- matrix(ncol = b, nrow = n-n_v)
		for (i in 1:b) {
			train[,i] <- sample(seq(1,n,1),n-n_v,replace = Replacement)
		}
	}else if(!is.null(BICV)){
		##Convert the Incedence matrix of BIBD in terms of our notation
		train <- BICV * seq(1,n,1)
	}else{
		##For the general case with all subsets of {1,...,n} 
		train <- combn(seq(1,n,1),n-n_v)
	}


	##Compute prediction errors for all sets in A
	MeanPred.Error <- c()
	for (i in 1:length(A[1,])) {
		Pred.Error <- c()
		for (j in 1:length(train[1,])) {
		
			##To make the Code a bit shorter
			train.j <- train[,j]                 
			X.train <- X[train.j,A[,i]] 
			   
			##Prediction Error for a given alpha and a given subset
			Pred.Error[j] <- norm(as.matrix(y[-train.j]-X[-train.j,A[,i]]%*%solve(t(X.train)%*%X.train)%*%t(X.train)%*%y[train.j]),"2")^2

		}
		MeanPred.Error[i] <- 1/length(train[1,])*sum(Pred.Error)
	}
	TheChosenOne <- which.min(MeanPred.Error)
	return(A[,TheChosenOne])
}
\end{lstlisting}

\begin{lstlisting}[title={Akaike and Schwarz Information Criterion}]
InfoCrit <- function(y,X,Alpha = NULL, Criterion = "AIC"){
	#Alpha		Set of possible modelversions for which the CV should 		 be calculated, by default use all possible models
	#Criterion    	Decides if we want to use the Akaike or Bayesian 	     information criterion

	##Change some variable names to keep the code shorter
	A <- Alpha

	##Number of observations
	n <- length(y)

	##Set of possible Models
	if(is.null(A)){

		##Number of Possible Regressors
		p <- length(X[1,])

		##Creats the set {1,...,p} from which we want to generate the powerset
		Index <- seq(1,p,1)    

		##Denotes the number of possible models out of {1,...,p} 
		col.A <- 2^p-1     

		##Denote A as powerset of {1,...,p}
		A <- matrix(0L,nrow = p, ncol = col.A)      
		k <- choose(p,1)
		l <- 1
		for (i in 1:p) {
			##combn calculates all combinations of i elements in Index
			A[1:i,l:k] <- combn(Index,i)               
			k <-k + choose(p,i+1)
		l <- l + choose(p,i)
		}
	}

	##Vector of likelihood values for different Models
	InfoCriterion <- c()
	for (i in 1:length(A[1,])) {

		##Number of regressors
		k <- sum(A[,i] != 0)

		##Data for the choosen model
		X.model <- X[,A[,i]] 

		##Expectation of observation i
		mu <- X.model%*%solve(t(X.model)%*%X.model)%*%t(X.model)%*%y

		error <- sum((y-mu)^2)

		##Variance
		var <- 1/n*error

		##Calculate log likelihood value
		LogL <- -n/2*log(2*pi)-n/2*log(var)-1/(2*var)*error

		##Calculate Information Criterion

		##For AIC
		if(Criterion == "AIC"){
			InfoCriterion[i] <- 2*k-2*LogL
		}

		##For BIC
		if(Criterion == "BIC"){
			InfoCriterion[i] <- log(n)*k - 2*LogL
		}

	}

	#Choose the model which minimizes the Information Criterion
	TheChosenOne <- which.min(InfoCriterion)
	return(A[,TheChosenOne])
}
\end{lstlisting}

\begin{lstlisting}[title={Data Generating Process}]
##Function that Generates Data
##The only purpose of this function is to save some lines of code. Hence that the function is not very general

DataGen <- function(n,p,p.True){
	##Data for true regression model
	#Intercept
	x_1 <- rep(1,n)         
	x_2 <- rnorm(n,2,1)
	x_3 <- rnorm(n,0,1)
	x_4 <- rnorm(n,5,2)
	x_5 <- rnorm(n,4,1)
	
	##Errorterms
	eps <- rnorm(n,0,1)     

	##Possible values for beta and X
	beta.pos <- c(1.5,3,2,5,3)
	X.pos <- cbind(x_1,x_2,x_3,x_4,x_5)

	beta.vec <- c(beta.pos[1:p.True],rep(0,p-p.True)) 
	X<- X.pos[,1:p]

	##Genereta dependent variables according to the above stateted model
	y <- X%*%beta.vec + eps    #True Model
	return(cbind(y,X))
}

\end{lstlisting}

\begin{lstlisting}[title={Simulation 1}]
##Simulation I
##Simulation of P(M_CV is in Cat II) 

##Number of true regressors 
p.True<- 2

##Total number of regressors
p <- 5

##Vector of diffrent sample sizes
N <- seq(15,300,2)

##Number of repetition of the experiment
m <- 500

##Vector of probabilities of choosing a Cat II model for given sample size by m repetations 
BIC <- rep(0,length(N))
AIC <- rep(0,length(N))
CV1 <- rep(0,length(N))
MCV <- rep(0,length(N))

for (i in 1:length(N)) {
	n <- N[i]
	n_v <- floor(n-n^(3/4))

	##Number of samplepartitions
	b <- 5*n

	for (j in 1:m) {
		##Generating data
		Data <- DataGen(n,p,p.True)
		y <- Data[,1]
		X <- Data[,-1]

		#Define some temporary variables, i.e, define for a given method the in iteration i choosed model
		temp.BIC <- InfoCrit(y,X,Criterion = "BIC")
		temp.AIC <- InfoCrit(y,X) 
		temp.CV1 <- CV(1,y,X)
		temp.MCV <- CV(n_v,y,X,MonteCarlo = b)

		##Counting how many times a Cat II model is picked for n= N[i] in m interations
		BIC[i] <- BIC[i] + (sum( temp.BIC < (p.True+1) & temp.BIC >0) == p.True)
		AIC[i] <- AIC[i] + (sum( temp.AIC < (p.True+1) & temp.AIC > 0) == p.True)
		CV1[i] <- CV1[i] + (sum( temp.CV1 < (p.True+1) & temp.CV1 > 0) == p.True)
		MCV[i] <- MCV[i] + (sum( temp.MCV < (p.True+1) & temp.MCV > 0) == p.True)
	}
}

##Plot simulation 1
library(dplyr)
library(tidyr)
library(ggplot2)

MCV <- MCV/m
AIC <- AIC/m
CV1 <- CV/m
BIC <- BIC/m
N <- N

d <- data.frame("Samplesize"=N,"MCCV"=MCV,"AIC"=AIC,"CV1"=CV1,"BIC"=BIC)
d1 <-d %>% gather(key,value,MCCV,AIC,BIC,CV1)

ggplot(d1,aes(x=Samplesize,y=value,colour=key)) +
	geom_line(size=1.2) +
	coord_cartesian(ylim = c(0.5,1), xlim = c(13, 200)) +
	ylab("Prob of choosing a Cat. II Model") + ggtitle("Probability of Cat. II Model") +
	theme_bw() +
	theme(text = element_text(size=15),
	plot.title = element_text(size = 20,hjust = 0.5,face="bold")) +
	theme(axis.title.x = element_text(size = 15)) +
	theme(axis.title.y = element_text(size = 15)) +
	theme(axis.title.y = element_text(margin = margin(t = 0, r = 20, b = 0, l = 0))) +
	theme(axis.title.x = element_text(margin = margin(t = 20, r = 0, b = 0, l = 0))) +
	theme(plot.title = element_text(margin = margin(t = 0, r = 0 , b = 20, l = 0))) +
	theme(legend.title=element_blank(),legend.text = element_text(size = 13)) +
	theme(legend.position = c(0.1,0.8))
\end{lstlisting}

\begin{lstlisting}[title={Simulation II}]
##Simulation II 

##Samplesize
n <- 500

##Number of repetitions of the experiment
m <- 2000

##Number of true regressors 
p.True<- 2

##Total number of regressors
p <- 5

##Calculate the set of Category II models
C2 <- Partion(p.True,p)

##Saves the dimenson of the choosen model for a given iteration
BIC <- c()
AIC <- c()
CV1 <- c()
MCV <- c()
BICV <- c()

##Number of leaved out data points and number of samplepartitions
n_v <- 394
b <- 5*n

for (i in 1:m) {
	##Generating Data
	Data <- DataGen(n,p,p.True)
	y <- Data[,1]
	X <- Data[,-1]

	BIC[i] <- sum( InfoCrit(y,X,C2,Criterion = "BIC") != 0)
	AIC[i] <- sum( InfoCrit(y,X,C2,Criterion = "AIC") != 0)
	CV1[i] <- sum( CV(1,y,X,C2) != 0)
	MCV[i] <- sum( CV(n_v,y,X, MonteCarlo = b) != 0)
}

##Computes the probability for a criterion to choose a Cat II model of a given size.
Probabilities <- matrix(0L,4,p-p.True+1)
for(i in 0:(p-p.True)){
	Probabilities[,i+1] <- c(sum( BIC == (p.True + i)), sum( AIC == (p.True + i)),  sum( CV1 == (p.True + i)) , sum( MCV == (p.True + i)) )/m
}
Probabilities

##Plot Simulation II

library(ggplot2)
Crit <- rep(c("CV(1)","AIC","MCCV","BIC"),4)
Model <- c(rep("M*",4),rep("M*+1",4),rep("M*+2",4),rep("M*+3",4))
Prob <- t(matrix(c(0.5980,0.336,0.0590,0.0070,0.6015,0.3320,0.0590,0.0075,
	0.9510,0.048988,0,0,0.9690,0.305,0.0005,0),4,4))
d <-data.frame("C"=Crit,"M"=Model,
	"P"=c(0.5980,0.6015,0.9510,0.9690,0.3366,0.3320,0.048988
	,0.0305,0.0590,0.0590,0,0.0005,0.007,0.007,0,0),"COL"=rep("blue",16),
	"Prozent"=c("60%","60%","95%","97%","34%","33%","4%","3%","6%","6%","0%","0%","0.7%","0.8%","0%","0%"))

ggplot(d,aes(x=C,y=M,size=P,label=P)) +
	geom_point(alpha=0.8,stat="identity",color ='skyblue3') +
	xlab("Selection Method") + ylab("Model Dimensionality") + ggtitle("Probability of Predicting Cat. II Model") +
	scale_size_continuous(range = c(1, 35)) +
	theme_bw() +
	guides(colour = FALSE,size = FALSE) +
	theme(text = element_text(size=15),
	plot.title = element_text(size = 20,hjust = 0.5,face="bold")) +
	theme(axis.title.x = element_text(size = 15)) +
	theme(axis.title.y = element_text(size = 15)) +
	theme(axis.title.y = element_text(margin = margin(t = 0, r = 20, b = 0, l = 0))) +
	theme(axis.title.x = element_text(margin = margin(t = 20, r = 0, b = 0, l = 0))) +
	theme(plot.title = element_text(margin = margin(t = 0, r = 0 , b = 20, l = 0))) +
	geom_text(size =5,aes(label=ifelse(P>0.04,as.character(Prozent),''))) +
	geom_text(size =5,aes(label=ifelse(P<0.04,as.character(Prozent),'')),hjust=-0.5,vjust=0.5)

\end{lstlisting}

\addcontentsline{toc}{subsection}{Code Efficiency}
\subsection*{Code Efficiency}

\begin{lstlisting}[title={Leave-one-Out Cross-Validation Function}]
##Creats the set {1,...,p} from which we want to generate the powerset
Index <- seq(1,p,1)    

##Denotes the number of possible models out of {1,...,p}
col.A <- 2^p-1
##Denote A as powerset of {1,...,p}
A <- matrix(0L,nrow = p, ncol = col.A)      
k <- choose(p,1)
l <- 1
for (i in 1:p) {
	##combn calculates all combinations of i elements in Index 
	A[1:i,l:k] <- combn(Index,i)             
	k <-k + choose(p,i+1)
	l <- l + choose(p,i)
}
##Generate the diagonal elements of the projection matrices corresponding to model alpha
proj_diag <- matrix(0L,nrow = n, ncol = col.A)
for(j in 1:col.A){
	for(i in 1:n){
		alpha <- A[A[,j]!=0,j]
		proj_diag[i,j] <- t(X[i,alpha])%*%solve(gram[alpha,alpha])%*%X[i,alpha]
	}
}

LOOCV <- function(y,B,C,D, Alpha = A){
	##ttis functions computes the regressors chosen by LOOCV
	##
	##y            Data for dependent variable
	##B            Data for Regressors
	##C            Gram matrix of regressors
	##D            Diagonal elements of the projection matrix
	##
	##Alpha     Set of possible modelvaritaions for which the CV should be calculated 
	##             (By default use all possible models)  

	p <- length(X[1,])#Number of Possible Regressors
	n <- length(y)#Number of Observations
	col.A <- length(A[1,])

	##compute the OLS estimates for model alpha
	b <- matrix(0L, nrow = p, ncol = col.A)
	for(j in 1:col.A){
	b[A[A[,j]!=0,j],j]<- solve(C[A[A[,j]!=0,j],A[A[,j]!=0,j]])%*%t(B[,A[A[,j]!=0,j]])%*%y
	}

	##Compute prediction errors for all sets in A
	MeanPred.Error <- numeric(col.A)
	for (i in 1:col.A) {
		alpha <- A[A[,i]!=0,i]
		if(length(alpha)==1){
			MeanPred.Error[i] <- t((rep(1,n)-D[,i])^(-2))%*%((y - B[,alpha]*b[alpha,i])^2)/n
		}else{
			MeanPred.Error[i] <- t((rep(1,n)-D[,i])^(-2))%*%((y - B[,alpha]%*%b[alpha,i])^2)/n 
		}
	}
	TheChosenOne <- which.min(MeanPred.Error)
	return(A[,TheChosenOne])
}
\end{lstlisting}

\begin{lstlisting}[title={Monte Carlo Cross-Validation Function (Speeded up Version)}]
##Creats the set {1,...,p} from which we want to generate the powerset
Index <- seq(1,p,1)    

##Denotes the number of possible models out of {1,...,p}
col.A <- 2^p-1
##Denote A as powerset of {1,...,p}
A <- matrix(0L,nrow = p, ncol = col.A)      
k <- choose(p,1)
l <- 1
for (i in 1:p) {
	##Combn calculates all combinations of i elements in Index 
	A[1:i,l:k] <- combn(Index,i)             
	k <-k + choose(p,i+1)
	l <- l + choose(p,i)
	}
##Generate the diagonal elements of the projection matrices corresponding to model alpha
proj_diag <- matrix(0L,nrow = n, ncol = col.A)
for(j in 1:col.A){
	for(i in 1:n){
		alpha <- A[A[,j]!=0,j]
		proj_diag[i,j] <- t(X[i,alpha])%*%solve(gram[alpha,alpha])%*%X[i,alpha]
	}
}

##b is the number of samplepartitions

##Combination of Samplesplits
train.MCCV <- matrix(ncol = b, nrow = n-n_v)
for (i in 1:b) {
	train[,i] <- sample(seq(1,n,1),n-n_v,replace = FALSE)
}

##Pmatrix
Pmatrix_MCCV <- list()
k <- 0
for (i in 1:length(A[1,])) {
	A.i <- A[,i]
		for (j in 1:length(train.MCCV[1,])) {
		k <- k+1
		train.j <- train.MCCV[,j]
		X.train <- X[train.j,A.i] 
		Pmatrix_MCCV[[paste0("element",k)]] <- X[-train.j,A.i]%*%(solve(t(X.train)%*%X.train)%*%t(X.train))
	}
}

MCCV.v2 <- function(n,n_v, y, X,I, A, b,train, Replacement = FALSE){  #n_v = #leaved out data points
	##Compute prediction errors for all sets in A
	MeanPred.Error <- c()
	k <- 0
	for (i in 1:length(A[1,])) {
		Pred.Error <- c()
		for (j in 1:length(train[1,])) {
			k <- k+1
			##To make the code a bit shorter
			train.j <- train[,j]                 
			##Prediction error for a given alpha and a given subset
			for(v in I[k]){
				Error.temp <- y[-train.j]-v%*%y[train.j]
				Pred.Error[j] <- t(Error.temp)%*%Error.temp
			}
		}
	MeanPred.Error[i] <- 1/length(n_v)*sum(Pred.Error)
	}
	TheChosenOne <- which.min(MeanPred.Error)
	return(A[,TheChosenOne])
}

\end{lstlisting}

\begin{lstlisting}[title={Schwarz and Akaike Information Criterion (Speeded up Version)}]
##Creats the set {1,...,p} from which we want to generate the powerset
Index <- seq(1,p,1)    

##Denotes the number of possible models out of {1,...,p}
col.A <- 2^p-1
##Denote A as powerset of {1,...,p}
A <- matrix(0L,nrow = p, ncol = col.A)      
k <- choose(p,1)
l <- 1
for (i in 1:p) {
	##combn calculates all combinations of i elements in Index 
	A[1:i,l:k] <- combn(Index,i)             
	k <-k + choose(p,i+1)
	l <- l + choose(p,i)
}
##Generate the diagonal elements of the projection matrices corresponding to model alpha
proj_diag <- matrix(0L,nrow = n, ncol = col.A)
for(j in 1:col.A){
	for(i in 1:n){
		alpha <- A[A[,j]!=0,j]
		proj_diag[i,j] <- t(X[i,alpha])%*%solve(gram[alpha,alpha])%*%X[i,alpha]
	}
}

Pmatrix <- list()
for (i in 1:length(A[1,])) {
	X.train <- X[,A[,i]]
	X.train.transpos <- t(X.train)
	Pmatrix[[paste0("element",i)]] <- X.train%*%(solve(X.train.transpos%*%X.train)%*%X.train.transpos)
}



##AIC/BIC
InfoCrit <- function(y,X,I,A,Criterion = "AIC"){
	##Calculate Information Criterion

	##For AIC
	if(Criterion == "AIC"){

		j <- 0
		InfoCriterion <- c()
		for (i in I) {
			j <- j+1

			##Number of regressors
			k <- sum(A[,j] != 0)

			##Expectation of observation j
			mu <- i%*%y

			##Variance
			var <- 1/n*sum((y-mu)^2)

			##Calculate Log Likelihood value
			LogL <- -n/2*(log(2*pi)+log(var)+1)

			InfoCriterion[j] <- 2*k-2*LogL
		}
	}
	##For BIC
	if(Criterion == "BIC"){
		j <- 0
		InfoCriterion <- c()
		for (i in I) {
			j <- j+1

			##Number of regressors
			k <- sum(A[,j] != 0)

			##Expectation of observation j
			mu <- i%*%y

			##Variance
			var <- 1/n*sum((y-mu)^2)

			##Calculate Log Likelihood value
			LogL <- -n/2*(log(2*pi)+log(var)+1)

			InfoCriterion[j] <- log(n)*k - 2*LogL
		}
	}

	##Choose the model which minimizes the Information Criterion
	TheChosenOne <- which.min(InfoCriterion)
	return(A[,TheChosenOne])
}
\end{lstlisting}

\end{document}