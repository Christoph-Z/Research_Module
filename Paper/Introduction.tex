\documentclass[Research_Module_ES.tex]{subfiles}
\begin{document}
\section{Introduction}	
%NOTIZEN
In this work we consider a model selection method  called \textit{Cross-Validation}, which is used for assesing the prediction ability of models.
In theory \textit{Leave-one-Out Cross-Validation} is a well studied topic, for example in \cite{geisser1975predictive}, \cite{stone1974cross} and \cite{stone1977asymptotic}, but as we will show in the next chapters, this method is asymptotically incorrect. That means the probability of choosing the model with the best prediction ability is not converging to 1.
To rectify this problem, \cite{shao} presents the \textit{Leave-$n_\nu$-out Cross-Validation} that does model selection and is asymptotically correct. But for this computational more complex version, he introduces different variants of this method, for example the \textit{Balanced Incomplete Cross-Validation (BICV)} and the \textit{Monte-Carlo Cross-Validation (MCCV)}, which are computational more effective. In the following chapters we will describe how \textit{Cross-Validation} works and show some theory. Therefore we use mainly the structure of the paper of \cite{shao} and his approaches to show the theoretical part. After that we investigate this method on how good it works, by doing some simulations and compare it to some well known other model selection methods as the \textit{Akaike Information Criterion (AIC)} and the \textit{Bayesian Information Criterion (BIC)}.
We will show in the simulations that \textit{AIC} and \textit{CV(1)} perform nearly the same, which was shown in \cite{stone1977asymptotic}. Also we show that \textit{BIC} and \textit{MCCV} behave similar.
\end{document}